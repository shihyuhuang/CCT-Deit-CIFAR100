{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "check.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUIgPLJxheo-"
      },
      "source": [
        "import argparse\n",
        "from time import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.datasets import ImageFolder, CIFAR100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSIZwSXeh46h",
        "outputId": "a598a248-a630-4795-fc96-db45cdb02368"
      },
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "\n",
        "od.download('https://www.kaggle.com/c/2021-ai-training-final-project/data')\n",
        "data_dir = './2021-ai-training-final-project/CIFAR100'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.7/dist-packages (0.1.20)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.62.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.5.30)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Skipping, found downloaded files in \"./2021-ai-training-final-project\" (use force=True to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIF_4cqbnVp6",
        "outputId": "4ed52f21-5363-4610-ebf9-ab2a8dc810c2"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "device = get_default_device()\n",
        "print(device)\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTAcsVYbhn_8"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Embedder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 word_embedding_dim=300,\n",
        "                 vocab_size=100000,\n",
        "                 padding_idx=1,\n",
        "                 pretrained_weight=None,\n",
        "                 embed_freeze=False,\n",
        "                 *args, **kwargs):\n",
        "        super(Embedder, self).__init__()\n",
        "        self.embeddings = nn.Embedding.from_pretrained(pretrained_weight, freeze=embed_freeze) \\\n",
        "            if pretrained_weight is not None else \\\n",
        "            nn.Embedding(vocab_size, word_embedding_dim, padding_idx=padding_idx)\n",
        "        self.embeddings.weight.requires_grad = not embed_freeze\n",
        "\n",
        "    def forward_mask(self, mask):\n",
        "        bsz, seq_len = mask.shape\n",
        "        new_mask = mask.view(bsz, seq_len, 1)\n",
        "        new_mask = new_mask.sum(-1)\n",
        "        new_mask = (new_mask > 0)\n",
        "        return new_mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        embed = self.embeddings(x)\n",
        "        embed = embed if mask is None else embed * self.forward_mask(mask).unsqueeze(-1).float()\n",
        "        return embed, mask\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        else:\n",
        "            nn.init.normal_(m.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heEtR8gWhtlY"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 n_conv_layers=1,\n",
        "                 n_input_channels=3,\n",
        "                 n_output_channels=64,\n",
        "                 in_planes=64,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 conv_bias=False):\n",
        "        super(Tokenizer, self).__init__()\n",
        "\n",
        "        n_filter_list = [n_input_channels] + \\\n",
        "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
        "                        [n_output_channels]\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
        "                          kernel_size=(kernel_size, kernel_size),\n",
        "                          stride=(stride, stride),\n",
        "                          padding=(padding, padding), bias=conv_bias),\n",
        "                nn.Identity() if activation is None else activation(),\n",
        "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
        "                             stride=pooling_stride,\n",
        "                             padding=pooling_padding) if max_pool else nn.Identity()\n",
        "            )\n",
        "                for i in range(n_conv_layers)\n",
        "            ])\n",
        "\n",
        "        self.flattener = nn.Flatten(2, 3)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
        "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.flattener(self.conv_layers(x)).transpose(-2, -1)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "\n",
        "class TextTokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 embedding_dim=300,\n",
        "                 n_output_channels=128,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 *args, **kwargs):\n",
        "        super(TextTokenizer, self).__init__()\n",
        "\n",
        "        self.max_pool = max_pool\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, n_output_channels,\n",
        "                      kernel_size=(kernel_size, embedding_dim),\n",
        "                      stride=(stride, 1),\n",
        "                      padding=(padding, 0), bias=False),\n",
        "            nn.Identity() if activation is None else activation(),\n",
        "            nn.MaxPool2d(\n",
        "                kernel_size=(pooling_kernel_size, 1),\n",
        "                stride=(pooling_stride, 1),\n",
        "                padding=(pooling_padding, 0)\n",
        "            ) if max_pool else nn.Identity()\n",
        "        )\n",
        "\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def seq_len(self, seq_len=32, embed_dim=300):\n",
        "        return self.forward(torch.zeros((1, seq_len, embed_dim)))[0].shape[1]\n",
        "\n",
        "    def forward_mask(self, mask):\n",
        "        new_mask = mask.unsqueeze(1).float()\n",
        "        cnn_weight = torch.ones(\n",
        "            (1, 1, self.conv_layers[0].kernel_size[0]),\n",
        "            device=mask.device,\n",
        "            dtype=torch.float)\n",
        "        new_mask = F.conv1d(\n",
        "            new_mask, cnn_weight, None,\n",
        "            self.conv_layers[0].stride[0], self.conv_layers[0].padding[0], 1, 1)\n",
        "        if self.max_pool:\n",
        "            new_mask = F.max_pool1d(\n",
        "                new_mask, self.conv_layers[2].kernel_size[0],\n",
        "                self.conv_layers[2].stride[0], self.conv_layers[2].padding[0], 1, False, False)\n",
        "        new_mask = new_mask.squeeze(1)\n",
        "        new_mask = (new_mask > 0)\n",
        "        return new_mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.transpose(1, 3).squeeze(1)\n",
        "        x = x if mask is None else x * self.forward_mask(mask).unsqueeze(-1).float()\n",
        "        return x, mask\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB0vHIwgjmfU"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6SI57hnhyp-"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Module, ModuleList, Linear, Dropout, LayerNorm, Identity, Parameter, init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class Attention(Module):\n",
        "    \"\"\"\n",
        "    Obtained from timm: github.com:rwightman/pytorch-image-models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MaskedAttention(Module):\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask_value = -torch.finfo(attn.dtype).max\n",
        "            assert mask.shape[-1] == attn.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
        "            attn.masked_fill_(~mask, mask_value)\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n",
        "                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "class MaskedTransformerEncoderLayer(Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(MaskedTransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = MaskedAttention(dim=d_model, num_heads=nhead,\n",
        "                                         attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, mask=None, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src), mask))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 sequence_length=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            sequence_length += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                    dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        return pe.unsqueeze(0)\n",
        "\n",
        "\n",
        "class MaskedTransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 seq_len=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert seq_len is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            seq_len += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                seq_len += 1  # padding idx\n",
        "                self.positional_emb = Parameter(torch.zeros(1, seq_len, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(seq_len,\n",
        "                                                                          embedding_dim,\n",
        "                                                                          padding_idx=True),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            MaskedTransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                          dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                          attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        if self.positional_emb is None and x.size(1) < self.seq_len:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "            if mask is not None:\n",
        "                mask = torch.cat([torch.ones(size=(mask.shape[0], 1), device=mask.device), mask.float()], dim=1)\n",
        "                mask = (mask > 0)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, mask=mask)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim, padding_idx=False):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        pe = pe.unsqueeze(0)\n",
        "        if padding_idx:\n",
        "            return torch.cat([torch.zeros((1, 1, dim)), pe], dim=1)\n",
        "        return pe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKXePY45hgYH"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "__all__ = ['cct_2', 'cct_4', 'cct_6', 'cct_7', 'cct_8',\n",
        "           'cct_14', 'cct_16',\n",
        "           'text_cct_2', 'text_cct_4', 'text_cct_6'\n",
        "           ]\n",
        "\n",
        "\n",
        "class CCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 embedding_dim=768,\n",
        "                 n_input_channels=3,\n",
        "                 n_conv_layers=1,\n",
        "                 kernel_size=7,\n",
        "                 stride=2,\n",
        "                 padding=3,\n",
        "                 pooling_kernel_size=3,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(CCT, self).__init__()\n",
        "\n",
        "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
        "                                   n_output_channels=embedding_dim,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   stride=stride,\n",
        "                                   padding=padding,\n",
        "                                   pooling_kernel_size=pooling_kernel_size,\n",
        "                                   pooling_stride=pooling_stride,\n",
        "                                   pooling_padding=pooling_padding,\n",
        "                                   max_pool=True,\n",
        "                                   activation=nn.ReLU,\n",
        "                                   n_conv_layers=n_conv_layers,\n",
        "                                   conv_bias=False)\n",
        "\n",
        "        self.classifier = TransformerClassifier(\n",
        "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
        "                                                           height=img_size,\n",
        "                                                           width=img_size),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout_rate=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenizer(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "class TextCCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 seq_len=64,\n",
        "                 word_embedding_dim=300,\n",
        "                 embedding_dim=256,\n",
        "                 kernel_size=2,\n",
        "                 stride=1,\n",
        "                 padding=1,\n",
        "                 pooling_kernel_size=2,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(TextCCT, self).__init__()\n",
        "\n",
        "        self.embedder = Embedder(word_embedding_dim=word_embedding_dim,\n",
        "                                 *args, **kwargs)\n",
        "\n",
        "        self.tokenizer = TextTokenizer(n_input_channels=word_embedding_dim,\n",
        "                                       n_output_channels=embedding_dim,\n",
        "                                       kernel_size=kernel_size,\n",
        "                                       stride=stride,\n",
        "                                       padding=padding,\n",
        "                                       pooling_kernel_size=pooling_kernel_size,\n",
        "                                       pooling_stride=pooling_stride,\n",
        "                                       pooling_padding=pooling_padding,\n",
        "                                       max_pool=True,\n",
        "                                       activation=nn.ReLU)\n",
        "\n",
        "        self.classifier = MaskedTransformerClassifier(\n",
        "            seq_len=self.tokenizer.seq_len(seq_len=seq_len, embed_dim=word_embedding_dim),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x, mask = self.embedder(x, mask=mask)\n",
        "        x, mask = self.tokenizer(x, mask=mask)\n",
        "        out = self.classifier(x, mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "def _cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "         kernel_size=3, stride=None, padding=None,\n",
        "         *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "    return CCT(num_layers=num_layers,\n",
        "               num_heads=num_heads,\n",
        "               mlp_ratio=mlp_ratio,\n",
        "               embedding_dim=embedding_dim,\n",
        "               kernel_size=kernel_size,\n",
        "               stride=stride,\n",
        "               padding=padding,\n",
        "               *args, **kwargs)\n",
        "\n",
        "\n",
        "def _text_cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "              kernel_size=4, stride=None, padding=None,\n",
        "              *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "\n",
        "    return TextCCT(num_layers=num_layers,\n",
        "                   num_heads=num_heads,\n",
        "                   mlp_ratio=mlp_ratio,\n",
        "                   embedding_dim=embedding_dim,\n",
        "                   kernel_size=kernel_size,\n",
        "                   stride=stride,\n",
        "                   padding=padding,\n",
        "                   *args, **kwargs)\n",
        "\n",
        "\n",
        "def cct_2(*args, **kwargs):\n",
        "    return _cct(num_layers=2, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "def cct_4(*args, **kwargs):\n",
        "    return _cct(num_layers=4, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "def cct_6(*args, **kwargs):\n",
        "    return _cct(num_layers=6, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "def cct_7(*args, **kwargs):\n",
        "    return _cct(num_layers=7, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "def cct_8(*args, **kwargs):\n",
        "    return _cct(num_layers=8, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "def cct_14(*args, **kwargs):\n",
        "    return _cct(num_layers=14, num_heads=6, mlp_ratio=3, embedding_dim=384,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "def cct_16(*args, **kwargs):\n",
        "    return _cct(num_layers=16, num_heads=6, mlp_ratio=3, embedding_dim=384,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "def text_cct_2(*args, **kwargs):\n",
        "    return _text_cct(num_layers=2, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
        "                     *args, **kwargs)\n",
        "\n",
        "\n",
        "def text_cct_4(*args, **kwargs):\n",
        "    return _text_cct(num_layers=4, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
        "                     *args, **kwargs)\n",
        "\n",
        "\n",
        "def text_cct_6(*args, **kwargs):\n",
        "    return _text_cct(num_layers=6, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                     *args, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNzrsyaziNrE"
      },
      "source": [
        "def test(model,data_loader,valid_ds):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        correct = 0\n",
        "        bs = batch_size\n",
        "        result = []\n",
        "        check_names = []\n",
        "        for i, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            arr = pred.data.cpu().numpy()\n",
        "            for j in range(pred.size()[0]):\n",
        "                file_name = valid_ds.samples[i*bs+j][0].split('/')[-1]\n",
        "                result.append((file_name,pred[j].cpu().numpy()[0])) \n",
        "        \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUll9UHFhbVf"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "DATASETS = {\n",
        "    'cifar10': {\n",
        "        'num_classes': 10,\n",
        "        'img_size': 32,\n",
        "        'mean': [0.4914, 0.4822, 0.4465],\n",
        "        'std': [0.2470, 0.2435, 0.2616]\n",
        "    },\n",
        "    'cifar100': {\n",
        "        'num_classes': 100,\n",
        "        'img_size': 32,\n",
        "        'mean': [0.5071, 0.4867, 0.4408],\n",
        "        'std': [0.2675, 0.2565, 0.2761]\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# Data args\n",
        "dataset='cifar100'\n",
        "workers=2\n",
        "print_freq=10\n",
        "checkpoint_path='cct6-3x2_cifar100_best.pth'\n",
        "\n",
        "# Optimization hyperparams\n",
        "batch_size=128\n",
        "model='cct_6'\n",
        "positional_embedding='learnable'\n",
        "conv_layers=2\n",
        "conv_size=3\n",
        "patch_size=4\n",
        "gpu_id=0\n",
        "no_cuda=False\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    img_size = DATASETS[dataset]['img_size']\n",
        "    num_classes = DATASETS[dataset]['num_classes']\n",
        "    img_mean, img_std = DATASETS[dataset]['mean'], DATASETS[dataset]['std']\n",
        "\n",
        "    model = cct_6(img_size=img_size,\n",
        "                                        num_classes=num_classes,\n",
        "                                        positional_embedding=positional_embedding,\n",
        "                                        n_conv_layers=conv_layers,\n",
        "                                        kernel_size=conv_size,\n",
        "                                        patch_size=patch_size)\n",
        "\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
        "    print(\"Loaded checkpoint.\")\n",
        "\n",
        "    normalize = [transforms.Normalize(mean=img_mean, std=img_std)]\n",
        "\n",
        "    if (not no_cuda) and torch.cuda.is_available():\n",
        "        torch.cuda.set_device(gpu_id)\n",
        "        model.cuda(gpu_id)\n",
        "\n",
        "    val_dataset = ImageFolder(\n",
        "        root=data_dir+\"/TEST\",   transform=transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            *normalize,\n",
        "        ]))\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size, shuffle=False,\n",
        "        num_workers=workers)\n",
        "\n",
        "    print(\"Beginning evaluation\")\n",
        "    time_begin = time()\n",
        "    acc1 = cls_validate(val_loader, model,  time_begin=time_begin)\n",
        "\n",
        "    total_mins = (time() - time_begin) / 60\n",
        "    print(f'Script finished in {total_mins:.2f} minutes, '\n",
        "          f'final top-1: {acc1:.2f}')\n",
        "    \n",
        "    result = test(model, val_loader, val_dataset)\n",
        "\n",
        "    with open ('ID_result.csv','w') as f:\n",
        "        f.write('Id,Category\\n')\n",
        "        for data in result:\n",
        "            f.write(data[0]+','+str(data[1])+'\\n')\n",
        "\n",
        "\n",
        "def accuracy(output, target):\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(1, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        correct_k = correct[:1].flatten().float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def cls_validate(val_loader, model,  time_begin=None):\n",
        "    model.eval()\n",
        "    acc1_val = 0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            if (not no_cuda) and torch.cuda.is_available():\n",
        "                images = images.cuda(gpu_id, non_blocking=True)\n",
        "                target = target.cuda(gpu_id, non_blocking=True)\n",
        "\n",
        "            output = model(images)\n",
        "\n",
        "            acc1 = accuracy(output, target)\n",
        "            n += images.size(0)\n",
        "            acc1_val += float(acc1[0] * images.size(0))\n",
        "\n",
        "            if print_freq >= 0 and i % print_freq == 0:\n",
        "                avg_acc1 = (acc1_val / n)\n",
        "                print(f'[Eval][{i}] \\t Top-1 {avg_acc1:6.2f}')\n",
        "\n",
        "    avg_acc1 = (acc1_val / n)\n",
        "    total_mins = -1 if time_begin is None else (time() - time_begin) / 60\n",
        "    print(f'[Final]\\t \\t Top-1 {avg_acc1:6.2f} \\t \\t Time: {total_mins:.2f}')\n",
        "\n",
        "    return avg_acc1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz6T6iNDhb3g",
        "outputId": "98910bc8-8016-4ea8-e334-3c5983dced78"
      },
      "source": [
        " main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded checkpoint.\n",
            "Beginning evaluation\n",
            "[Eval][0] \t Top-1  93.75\n",
            "[Eval][10] \t Top-1  72.73\n",
            "[Eval][20] \t Top-1  74.44\n",
            "[Eval][30] \t Top-1  73.59\n",
            "[Eval][40] \t Top-1  73.59\n",
            "[Eval][50] \t Top-1  74.02\n",
            "[Eval][60] \t Top-1  74.12\n",
            "[Eval][70] \t Top-1  74.64\n",
            "[Final]\t \t Top-1  74.48 \t \t Time: 1.92\n",
            "Script finished in 1.92 minutes, final top-1: 74.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGqOOdaXiK0s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}