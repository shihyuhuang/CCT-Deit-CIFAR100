{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cct6_3x2_check0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhSQ4zfq0CfM"
      },
      "source": [
        "from time import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.datasets import ImageFolder, CIFAR100\n",
        "import random\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57692838La4I",
        "outputId": "42679714-38f7-4596-de0e-7ea84591d216"
      },
      "source": [
        "!pip install torchtoolbox"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtoolbox\n",
            "  Downloading torchtoolbox-0.1.5-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.62.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.19.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (3.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.15.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.99)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torchtoolbox) (1.0.1)\n",
            "Installing collected packages: torchtoolbox\n",
            "Successfully installed torchtoolbox-0.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV0WT80KVVms",
        "outputId": "ae22d5e0-5fc5-4216-d8d9-b18ae993aca0"
      },
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "\n",
        "\n",
        "od.download('https://www.kaggle.com/c/2021-ai-training-final-project/data')\n",
        "data_dir = './2021-ai-training-final-project/CIFAR100'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.20-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.62.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (5.0.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.5.30)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.20\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: shihyuhuang\n",
            "Your Kaggle Key: ··········\n",
            "Downloading 2021-ai-training-final-project.zip to ./2021-ai-training-final-project\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 140M/140M [00:01<00:00, 109MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting archive ./2021-ai-training-final-project/2021-ai-training-final-project.zip to ./2021-ai-training-final-project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxFa3TYR3oA8",
        "outputId": "1142f3d7-983e-4509-9305-b06d520f6e4d"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "device = get_default_device()\n",
        "print(device)\n",
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMDYedpmsyjY"
      },
      "source": [
        "\n",
        "# Data args\n",
        "dataset='cifar100'\n",
        "workers=4\n",
        "print_freq=10\n",
        "pretrain_path='cct6-3x2_cifar100_best.pth'\n",
        "checkpoint_path = 'cct6-3x2_cifar100_training.pth'\n",
        "\n",
        "# Optimization hyperparams\n",
        "epochs=1\n",
        "warmup=5\n",
        "batch_size=128\n",
        "lr=0.0005\n",
        "weight_decay=3e-2\n",
        "clip_grad_norm=0.\n",
        "model='cct_6'\n",
        "positional_embedding='learnable'\n",
        "conv_layers=2\n",
        "conv_size=3\n",
        "patch_size=4\n",
        "gpu_id=0\n",
        "no_cuda=False\n",
        "disable_aug=False\n",
        "disable_cos=False\n",
        "\n",
        "best_acc1=0\n",
        "\n",
        "DATASETS = {\n",
        "    'cifar100': {\n",
        "        'num_classes': 100,\n",
        "        'img_size': 32,\n",
        "        'mean': [0.5071, 0.4867, 0.4408],\n",
        "        'std': [0.2675, 0.2565, 0.2761]\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcGLeXU7te0B"
      },
      "source": [
        "#src/cct.py \n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "__all__ = ['cct_2', 'cct_4', 'cct_6', 'cct_7', 'cct_8',\n",
        "           'cct_14', 'cct_16',\n",
        "           'text_cct_2', 'text_cct_4', 'text_cct_6'\n",
        "           ]\n",
        "\n",
        "\n",
        "class CCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 embedding_dim=768,\n",
        "                 n_input_channels=3,\n",
        "                 n_conv_layers=1,\n",
        "                 kernel_size=7,\n",
        "                 stride=2,\n",
        "                 padding=3,\n",
        "                 pooling_kernel_size=3,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(CCT, self).__init__()\n",
        "\n",
        "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
        "                                   n_output_channels=embedding_dim,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   stride=stride,\n",
        "                                   padding=padding,\n",
        "                                   pooling_kernel_size=pooling_kernel_size,\n",
        "                                   pooling_stride=pooling_stride,\n",
        "                                   pooling_padding=pooling_padding,\n",
        "                                   max_pool=True,\n",
        "                                   activation=nn.ReLU,\n",
        "                                   n_conv_layers=n_conv_layers,\n",
        "                                   conv_bias=False)\n",
        "\n",
        "        self.classifier = TransformerClassifier(\n",
        "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
        "                                                           height=img_size,\n",
        "                                                           width=img_size),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout_rate=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenizer(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "class TextCCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 seq_len=64,\n",
        "                 word_embedding_dim=300,\n",
        "                 embedding_dim=256,\n",
        "                 kernel_size=2,\n",
        "                 stride=1,\n",
        "                 padding=1,\n",
        "                 pooling_kernel_size=2,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(TextCCT, self).__init__()\n",
        "\n",
        "        self.embedder = Embedder(word_embedding_dim=word_embedding_dim,\n",
        "                                 *args, **kwargs)\n",
        "\n",
        "        self.tokenizer = TextTokenizer(n_input_channels=word_embedding_dim,\n",
        "                                       n_output_channels=embedding_dim,\n",
        "                                       kernel_size=kernel_size,\n",
        "                                       stride=stride,\n",
        "                                       padding=padding,\n",
        "                                       pooling_kernel_size=pooling_kernel_size,\n",
        "                                       pooling_stride=pooling_stride,\n",
        "                                       pooling_padding=pooling_padding,\n",
        "                                       max_pool=True,\n",
        "                                       activation=nn.ReLU)\n",
        "\n",
        "        self.classifier = MaskedTransformerClassifier(\n",
        "            seq_len=self.tokenizer.seq_len(seq_len=seq_len, embed_dim=word_embedding_dim),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x, mask = self.embedder(x, mask=mask)\n",
        "        x, mask = self.tokenizer(x, mask=mask)\n",
        "        out = self.classifier(x, mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "def _cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "         kernel_size=3, stride=None, padding=None,\n",
        "         *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "    return CCT(num_layers=num_layers,\n",
        "               num_heads=num_heads,\n",
        "               mlp_ratio=mlp_ratio,\n",
        "               embedding_dim=embedding_dim,\n",
        "               kernel_size=kernel_size,\n",
        "               stride=stride,\n",
        "               padding=padding,\n",
        "               *args, **kwargs)\n",
        "\n",
        "\n",
        "def _text_cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "              kernel_size=4, stride=None, padding=None,\n",
        "              *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "\n",
        "    return TextCCT(num_layers=num_layers,\n",
        "                   num_heads=num_heads,\n",
        "                   mlp_ratio=mlp_ratio,\n",
        "                   embedding_dim=embedding_dim,\n",
        "                   kernel_size=kernel_size,\n",
        "                   stride=stride,\n",
        "                   padding=padding,\n",
        "                   *args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cct_6(*args, **kwargs):\n",
        "    return _cct(num_layers=6, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj6UX-dzvo1l"
      },
      "source": [
        "#/src/utils/embedder.py\n",
        "class Embedder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 word_embedding_dim=300,\n",
        "                 vocab_size=100000,\n",
        "                 padding_idx=1,\n",
        "                 pretrained_weight=None,\n",
        "                 embed_freeze=False,\n",
        "                 *args, **kwargs):\n",
        "        super(Embedder, self).__init__()\n",
        "        self.embeddings = nn.Embedding.from_pretrained(pretrained_weight, freeze=embed_freeze) \\\n",
        "            if pretrained_weight is not None else \\\n",
        "            nn.Embedding(vocab_size, word_embedding_dim, padding_idx=padding_idx)\n",
        "        self.embeddings.weight.requires_grad = not embed_freeze\n",
        "\n",
        "    def forward_mask(self, mask):\n",
        "        bsz, seq_len = mask.shape\n",
        "        new_mask = mask.view(bsz, seq_len, 1)\n",
        "        new_mask = new_mask.sum(-1)\n",
        "        new_mask = (new_mask > 0)\n",
        "        return new_mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        embed = self.embeddings(x)\n",
        "        embed = embed if mask is None else embed * self.forward_mask(mask).unsqueeze(-1).float()\n",
        "        return embed, mask\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        else:\n",
        "            nn.init.normal_(m.weight)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11WxM9ycvfy-"
      },
      "source": [
        "#/src/utils/tokenizer.py \n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 n_conv_layers=1,\n",
        "                 n_input_channels=3,\n",
        "                 n_output_channels=64,\n",
        "                 in_planes=64,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 conv_bias=False):\n",
        "        super(Tokenizer, self).__init__()\n",
        "\n",
        "        n_filter_list = [n_input_channels] + \\\n",
        "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
        "                        [n_output_channels]\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
        "                          kernel_size=(kernel_size, kernel_size),\n",
        "                          stride=(stride, stride),\n",
        "                          padding=(padding, padding), bias=conv_bias),\n",
        "                nn.Identity() if activation is None else activation(),\n",
        "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
        "                             stride=pooling_stride,\n",
        "                             padding=pooling_padding) if max_pool else nn.Identity()\n",
        "            )\n",
        "                for i in range(n_conv_layers)\n",
        "            ])\n",
        "\n",
        "        self.flattener = nn.Flatten(2, 3)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
        "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.flattener(self.conv_layers(x)).transpose(-2, -1)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "\n",
        "class TextTokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 embedding_dim=300,\n",
        "                 n_output_channels=128,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 *args, **kwargs):\n",
        "        super(TextTokenizer, self).__init__()\n",
        "\n",
        "        self.max_pool = max_pool\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, n_output_channels,\n",
        "                      kernel_size=(kernel_size, embedding_dim),\n",
        "                      stride=(stride, 1),\n",
        "                      padding=(padding, 0), bias=False),\n",
        "            nn.Identity() if activation is None else activation(),\n",
        "            nn.MaxPool2d(\n",
        "                kernel_size=(pooling_kernel_size, 1),\n",
        "                stride=(pooling_stride, 1),\n",
        "                padding=(pooling_padding, 0)\n",
        "            ) if max_pool else nn.Identity()\n",
        "        )\n",
        "\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def seq_len(self, seq_len=32, embed_dim=300):\n",
        "        return self.forward(torch.zeros((1, seq_len, embed_dim)))[0].shape[1]\n",
        "\n",
        "    def forward_mask(self, mask):\n",
        "        new_mask = mask.unsqueeze(1).float()\n",
        "        cnn_weight = torch.ones(\n",
        "            (1, 1, self.conv_layers[0].kernel_size[0]),\n",
        "            device=mask.device,\n",
        "            dtype=torch.float)\n",
        "        new_mask = F.conv1d(\n",
        "            new_mask, cnn_weight, None,\n",
        "            self.conv_layers[0].stride[0], self.conv_layers[0].padding[0], 1, 1)\n",
        "        if self.max_pool:\n",
        "            new_mask = F.max_pool1d(\n",
        "                new_mask, self.conv_layers[2].kernel_size[0],\n",
        "                self.conv_layers[2].stride[0], self.conv_layers[2].padding[0], 1, False, False)\n",
        "        new_mask = new_mask.squeeze(1)\n",
        "        new_mask = (new_mask > 0)\n",
        "        return new_mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.transpose(1, 3).squeeze(1)\n",
        "        x = x if mask is None else x * self.forward_mask(mask).unsqueeze(-1).float()\n",
        "        return x, mask\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i67i7ugxvSeM"
      },
      "source": [
        "#src/utils/transformers.py \n",
        "import torch\n",
        "from torch.nn import Module, ModuleList, Linear, Dropout, LayerNorm, Identity, Parameter, init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Attention(Module):\n",
        "    \"\"\"\n",
        "    Obtained from timm: github.com:rwightman/pytorch-image-models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MaskedAttention(Module):\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask_value = -torch.finfo(attn.dtype).max\n",
        "            assert mask.shape[-1] == attn.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
        "            attn.masked_fill_(~mask, mask_value)\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n",
        "                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "class MaskedTransformerEncoderLayer(Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(MaskedTransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = MaskedAttention(dim=d_model, num_heads=nhead,\n",
        "                                         attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, mask=None, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src), mask))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 sequence_length=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            sequence_length += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                    dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        return pe.unsqueeze(0)\n",
        "\n",
        "\n",
        "class MaskedTransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 seq_len=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert seq_len is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            seq_len += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                seq_len += 1  # padding idx\n",
        "                self.positional_emb = Parameter(torch.zeros(1, seq_len, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(seq_len,\n",
        "                                                                          embedding_dim,\n",
        "                                                                          padding_idx=True),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            MaskedTransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                          dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                          attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        if self.positional_emb is None and x.size(1) < self.seq_len:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "            if mask is not None:\n",
        "                mask = torch.cat([torch.ones(size=(mask.shape[0], 1), device=mask.device), mask.float()], dim=1)\n",
        "                mask = (mask > 0)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, mask=mask)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim, padding_idx=False):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        pe = pe.unsqueeze(0)\n",
        "        if padding_idx:\n",
        "            return torch.cat([torch.zeros((1, 1, dim)), pe], dim=1)\n",
        "        return pe"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNOUNvqT7S3g"
      },
      "source": [
        "#utils/transforms.py \n",
        "class ShearX(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n",
        "            Image.BICUBIC, fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class ShearY(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n",
        "            Image.BICUBIC, fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class TranslateX(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, magnitude * x.size[0] * random.choice([-1, 1]), 0, 1, 0),\n",
        "            fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class TranslateY(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * x.size[1] * random.choice([-1, 1])),\n",
        "            fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class Rotate(object):\n",
        "    # from https://stackoverflow.com/questions/\n",
        "    # 5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n",
        "    def __call__(self, x, magnitude):\n",
        "        rot = x.convert(\"RGBA\").rotate(magnitude)\n",
        "        return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(x.mode)\n",
        "\n",
        "\n",
        "class Color(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Color(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Posterize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.posterize(x, magnitude)\n",
        "\n",
        "\n",
        "class Solarize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.solarize(x, magnitude)\n",
        "\n",
        "\n",
        "class Contrast(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Contrast(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Sharpness(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Sharpness(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Brightness(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Brightness(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class AutoContrast(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.autocontrast(x)\n",
        "\n",
        "\n",
        "class Equalize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.equalize(x)\n",
        "\n",
        "\n",
        "class Invert(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.invert(x)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6L2he3-60fH"
      },
      "source": [
        "#utils/autoaug.py \n",
        "class ImageNetPolicy(object):\n",
        "    \"\"\" Randomly choose one of the best 24 Sub-policies on ImageNet.\n",
        "        Example:\n",
        "        >>> policy = ImageNetPolicy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     ImageNetPolicy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.4, \"posterize\", 8, 0.6, \"rotate\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor),\n",
        "            SubPolicy(0.6, \"posterize\", 7, 0.6, \"posterize\", 6, fillcolor),\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 4, 0.8, \"rotate\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 3, 0.6, \"equalize\", 7, fillcolor),\n",
        "            SubPolicy(0.8, \"posterize\", 5, 1.0, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.2, \"rotate\", 3, 0.6, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 8, 0.4, \"posterize\", 6, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 0.4, \"color\", 0, fillcolor),\n",
        "            SubPolicy(0.4, \"rotate\", 9, 0.6, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.0, \"equalize\", 7, 0.8, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 1.0, \"color\", 2, fillcolor),\n",
        "            SubPolicy(0.8, \"color\", 8, 0.8, \"solarize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"sharpness\", 7, 0.6, \"invert\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"shearX\", 5, 1.0, \"equalize\", 9, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 0, 0.6, \"equalize\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment ImageNet Policy\"\n",
        "\n",
        "\n",
        "class CIFAR10Policy(object):\n",
        "    \"\"\" Randomly choose one of the best 25 Sub-policies on CIFAR10.\n",
        "        Example:\n",
        "        >>> policy = CIFAR10Policy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     CIFAR10Policy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n",
        "            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n",
        "            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n",
        "            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n",
        "            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n",
        "            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n",
        "\n",
        "            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n",
        "            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n",
        "            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n",
        "            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n",
        "            SubPolicy(0.2, \"equalize\", 8, 0.6, \"equalize\", 4, fillcolor),\n",
        "            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n",
        "            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n",
        "            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n",
        "            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment CIFAR10 Policy\"\n",
        "\n",
        "\n",
        "class SubPolicy(object):\n",
        "    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n",
        "        ranges = {\n",
        "            \"shearX\": np.linspace(0, 0.3, 10),\n",
        "            \"shearY\": np.linspace(0, 0.3, 10),\n",
        "            \"translateX\": np.linspace(0, 150 / 331, 10),\n",
        "            \"translateY\": np.linspace(0, 150 / 331, 10),\n",
        "            \"rotate\": np.linspace(0, 30, 10),\n",
        "            \"color\": np.linspace(0.0, 0.9, 10),\n",
        "            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n",
        "            \"solarize\": np.linspace(256, 0, 10),\n",
        "            \"contrast\": np.linspace(0.0, 0.9, 10),\n",
        "            \"sharpness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"brightness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"autocontrast\": [0] * 10,\n",
        "            \"equalize\": [0] * 10,\n",
        "            \"invert\": [0] * 10\n",
        "        }\n",
        "\n",
        "        func = {\n",
        "            \"shearX\": ShearX(fillcolor=fillcolor),\n",
        "            \"shearY\": ShearY(fillcolor=fillcolor),\n",
        "            \"translateX\": TranslateX(fillcolor=fillcolor),\n",
        "            \"translateY\": TranslateY(fillcolor=fillcolor),\n",
        "            \"rotate\": Rotate(),\n",
        "            \"color\": Color(),\n",
        "            \"posterize\": Posterize(),\n",
        "            \"solarize\": Solarize(),\n",
        "            \"contrast\": Contrast(),\n",
        "            \"sharpness\": Sharpness(),\n",
        "            \"brightness\": Brightness(),\n",
        "            \"autocontrast\": AutoContrast(),\n",
        "            \"equalize\": Equalize(),\n",
        "            \"invert\": Invert()\n",
        "        }\n",
        "\n",
        "        self.p1 = p1\n",
        "        self.operation1 = func[operation1]\n",
        "        self.magnitude1 = ranges[operation1][magnitude_idx1]\n",
        "        self.p2 = p2\n",
        "        self.operation2 = func[operation2]\n",
        "        self.magnitude2 = ranges[operation2][magnitude_idx2]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p1:\n",
        "            img = self.operation1(img, self.magnitude1)\n",
        "        if random.random() < self.p2:\n",
        "            img = self.operation2(img, self.magnitude2)\n",
        "        return img"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR3mRPNHv1iY"
      },
      "source": [
        "#utils/stochastic_depth.py \n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npCm0MjVWfjU"
      },
      "source": [
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lrr = lr\n",
        "    if epoch < warmup:\n",
        "        lrr = lr / (warmup - epoch)\n",
        "    elif not disable_cos:\n",
        "        lrr *= 0.5 * (1. + math.cos(math.pi * (epoch - warmup) / (epochs - warmup)))\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lrr\n",
        "\n",
        "\n",
        "def accuracy(output, target):\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(1, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        correct_k = correct[:1].flatten().float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def cls_train(loss_train_arr, train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    loss_val, acc1_val = 0, 0\n",
        "    n = 0\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        if (not no_cuda) and torch.cuda.is_available():\n",
        "            images = images.cuda(gpu_id, non_blocking=True)\n",
        "            target = target.cuda(gpu_id, non_blocking=True)\n",
        "        output = model(images)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        acc1 = accuracy(output, target)\n",
        "        n += images.size(0)\n",
        "        loss_val += float(loss.item() * images.size(0))\n",
        "        acc1_val += float(acc1[0] * images.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        if clip_grad_norm > 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm, norm_type=2)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if print_freq >= 0 and i % print_freq == 0:\n",
        "            avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "            print(f'[Epoch {epoch + 1}][Train][{i}] \\t Loss: {avg_loss:.4e} \\t Top-1 {avg_acc1:6.2f}')\n",
        "    \n",
        "    loss_train_arr.append(avg_loss)\n",
        "    \n",
        "    return loss_train_arr\n",
        "\n",
        "\n",
        "def cls_validate(loss_val_arr, val_loader, model, criterion,  epoch=None, time_begin=None):\n",
        "    model.eval()\n",
        "    loss_val, acc1_val = 0, 0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            if (not no_cuda) and torch.cuda.is_available():\n",
        "                images = images.cuda(gpu_id, non_blocking=True)\n",
        "                target = target.cuda(gpu_id, non_blocking=True)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc1 = accuracy(output, target)\n",
        "            n += images.size(0)\n",
        "            loss_val += float(loss.item() * images.size(0))\n",
        "            acc1_val += float(acc1[0] * images.size(0))\n",
        "\n",
        "            if print_freq >= 0 and i % print_freq == 0:\n",
        "                avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "                print(f'[Epoch {epoch + 1}][Eval][{i}] \\t Loss: {avg_loss:.4e} \\t Top-1 {avg_acc1:6.2f}')\n",
        "\n",
        "    avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "    total_mins = -1 if time_begin is None else (time() - time_begin) / 60\n",
        "    print(f'[Epoch {epoch + 1}] \\t \\t Top-1 {avg_acc1:6.2f} \\t \\t Time: {total_mins:.2f}')\n",
        "\n",
        "    loss_val_arr.append(avg_loss)\n",
        "\n",
        "    return loss_val_arr, avg_acc1\n",
        "\n",
        "\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    NLL loss with label smoothing.\n",
        "    \"\"\"\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        \"\"\"\n",
        "        Constructor for the LabelSmoothing module.\n",
        "        :param smoothing: label smoothing factor\n",
        "        \"\"\"\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        assert smoothing < 1.0\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1. - smoothing\n",
        "\n",
        "    def _compute_losses(self, x, target):\n",
        "        log_prob = F.log_softmax(x, dim=-1)\n",
        "        nll_loss = -log_prob.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -log_prob.mean(dim=-1)\n",
        "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        return self._compute_losses(x, target).mean()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cfcCe-_qFgb"
      },
      "source": [
        "def test(model,data_loader,valid_ds):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        correct = 0\n",
        "        bs = batch_size\n",
        "        result = []\n",
        "        check_names = []\n",
        "        for i, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            arr = pred.data.cpu().numpy()\n",
        "            for j in range(pred.size()[0]):\n",
        "                file_name = valid_ds.samples[i*bs+j][0].split('/')[-1]\n",
        "                result.append((file_name,pred[j].cpu().numpy()[0])) \n",
        "        \n",
        "    return result"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ZPaXDFx3w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ea539f-6b17-4748-ecbc-afb7e3048259"
      },
      "source": [
        "#main.py \n",
        "\n",
        "\n",
        "loss_train_arr =[]\n",
        "loss_val_arr =[]\n",
        "\n",
        "img_size = DATASETS[dataset]['img_size']\n",
        "num_classes = DATASETS[dataset]['num_classes']\n",
        "img_mean, img_std = DATASETS[dataset]['mean'], DATASETS[dataset]['std']\n",
        "\n",
        "model = cct_6(img_size=img_size,\n",
        "                                    num_classes=num_classes,\n",
        "                                    positional_embedding=positional_embedding,\n",
        "                                    n_conv_layers=conv_layers,\n",
        "                                    kernel_size=conv_size,\n",
        "                                    patch_size=patch_size)\n",
        "#wrong\n",
        "model.load_state_dict(torch.load(pretrain_path, map_location='cpu'))\n",
        "print(\"Loaded checkpoint.\")\n",
        "\n",
        "criterion = LabelSmoothingCrossEntropy()\n",
        "\n",
        "if (not no_cuda) and torch.cuda.is_available():\n",
        "    torch.cuda.set_device(gpu_id)\n",
        "    model.cuda(gpu_id)\n",
        "    criterion = criterion.cuda(gpu_id)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr,\n",
        "                                weight_decay=weight_decay)\n",
        "\n",
        "normalize = [transforms.Normalize(mean=img_mean, std=img_std)]\n",
        "\n",
        "augmentations = []\n",
        "if not disable_aug:\n",
        "    #from utils.autoaug import CIFAR10Policy\n",
        "    augmentations += [\n",
        "        CIFAR10Policy()\n",
        "    ]\n",
        "augmentations += [\n",
        "    transforms.RandomCrop(img_size, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    *normalize,\n",
        "]\n",
        "\n",
        "augmentations = transforms.Compose(augmentations)\n",
        "train_dataset = ImageFolder(\n",
        "    root=data_dir+\"/TRAIN\",   transform=augmentations)\n",
        "\n",
        "val_dataset = ImageFolder(\n",
        "    root=data_dir+\"/TEST\",   transform=transforms.Compose([\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        *normalize,\n",
        "    ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True,\n",
        "    num_workers=workers)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size, shuffle=False,\n",
        "    num_workers=workers)\n",
        "\n",
        "print(\"Beginning training\")\n",
        "time_begin = time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    loss_train_arr = cls_train(loss_train_arr, train_loader, model, criterion, optimizer, epoch)\n",
        "    loss_val_arr, acc1 = cls_validate(loss_val_arr, val_loader, model, criterion,  epoch=epoch, time_begin=time_begin)\n",
        "    best_acc1 = max(acc1, best_acc1)\n",
        "\n",
        "total_mins = (time() - time_begin) / 60\n",
        "print(f'Script finished in {total_mins:.2f} minutes, '\n",
        "        f'best top-1: {best_acc1:.2f}, '\n",
        "        f'final top-1: {acc1:.2f}')\n",
        "torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "result = test(model, val_loader, val_dataset)\n",
        "\n",
        "with open ('ID_result.csv','w') as f:\n",
        "    f.write('Id,Category\\n')\n",
        "    for data in result:\n",
        "        f.write(data[0]+','+str(data[1])+'\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded checkpoint.\n",
            "Beginning training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1][Train][0] \t Loss: 1.0861e+00 \t Top-1  93.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt6ogGMRWlzY"
      },
      "source": [
        "\n",
        "from torchsummary import summary\n",
        "# Print model\n",
        "print(model)\n",
        "\n",
        "# Print parameter\n",
        "size = summary(model, (3, 32, 32))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaEQExMDWoEV"
      },
      "source": [
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n",
        "\n",
        "\n",
        "plt.plot([x for x in loss_train_arr], \"-bx\")\n",
        "plt.plot([x for x in loss_val_arr],\"-rx\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"train loss\",\"val loss\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}