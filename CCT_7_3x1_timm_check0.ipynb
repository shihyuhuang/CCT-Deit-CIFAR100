{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CCT-7/3x1 timm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhSQ4zfq0CfM"
      },
      "source": [
        "from time import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.datasets import ImageFolder, CIFAR100\n",
        "import random\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "\n"
      ],
      "execution_count": 425,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57692838La4I",
        "outputId": "20751c8a-0be6-43e3-8b72-a4e4a246c1d7"
      },
      "source": [
        "!pip install torchtoolbox"
      ],
      "execution_count": 426,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtoolbox in /usr/local/lib/python3.7/dist-packages (0.1.5)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.99)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.62.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.1.2.30)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torchtoolbox) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV0WT80KVVms",
        "outputId": "bee048ff-9646-4ab0-8704-82905c704f8d"
      },
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "\n",
        "\n",
        "od.download('https://www.kaggle.com/c/2021-ai-training-final-project/data')\n",
        "data_dir = './2021-ai-training-final-project/CIFAR100'"
      ],
      "execution_count": 427,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.7/dist-packages (0.1.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.62.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.5.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (5.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Skipping, found downloaded files in \"./2021-ai-training-final-project\" (use force=True to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxFa3TYR3oA8",
        "outputId": "24b844a4-5fa5-4653-a0a0-702e6287a9f0"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "device = get_default_device()\n",
        "print(device)\n",
        "!nvidia-smi"
      ],
      "execution_count": 428,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Thu Aug 26 15:07:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    61W / 149W |   2143MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMDYedpmsyjY"
      },
      "source": [
        "\n",
        "# Data args\n",
        "dataset='cifar100'\n",
        "workers=4\n",
        "print_freq=10\n",
        "pretrain_path='cct7-3x1_timm_cifar100_1500epochs_82.72.pth'\n",
        "checkpoint_path = 'cct7-3x1_timm_cifar100_1500epochs_82.72_training.pth'\n",
        "\n",
        "# L/PxC \n",
        "# L = transformer layers\n",
        "# P = patch/convolution size\n",
        "# C = convolutional layers\n",
        "\n",
        "# Optimization hyperparams\n",
        "epochs=1\n",
        "warmup=5\n",
        "batch_size=128\n",
        "lr=0.0005\n",
        "weight_decay=3e-2\n",
        "clip_grad_norm=0.\n",
        "model='cct_7'\n",
        "positional_embedding='learnable'\n",
        "conv_layers=1\n",
        "conv_size=3\n",
        "patch_size=4\n",
        "gpu_id=0\n",
        "no_cuda=False\n",
        "disable_aug=False\n",
        "disable_cos=False\n",
        "\n",
        "best_acc1=0\n",
        "\n",
        "DATASETS = {\n",
        "    'cifar100': {\n",
        "        'num_classes': 100,\n",
        "        'img_size': 32,\n",
        "        'mean': [0.5071, 0.4867, 0.4408],\n",
        "        'std': [0.2675, 0.2565, 0.2761]\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 429,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcGLeXU7te0B"
      },
      "source": [
        "#src/cct.py \n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "__all__ = ['cct_2', 'cct_4', 'cct_6', 'cct_7', 'cct_8',\n",
        "           'cct_14', 'cct_16',\n",
        "           'text_cct_2', 'text_cct_4', 'text_cct_6'\n",
        "           ]\n",
        "\n",
        "\n",
        "class CCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 embedding_dim=768,\n",
        "                 n_input_channels=3,\n",
        "                 n_conv_layers=1,\n",
        "                 kernel_size=7,\n",
        "                 stride=2,\n",
        "                 padding=3,\n",
        "                 pooling_kernel_size=3,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(CCT, self).__init__()\n",
        "\n",
        "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
        "                                   n_output_channels=embedding_dim,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   stride=stride,\n",
        "                                   padding=padding,\n",
        "                                   pooling_kernel_size=pooling_kernel_size,\n",
        "                                   pooling_stride=pooling_stride,\n",
        "                                   pooling_padding=pooling_padding,\n",
        "                                   max_pool=True,\n",
        "                                   activation=nn.ReLU,\n",
        "                                   n_conv_layers=n_conv_layers,\n",
        "                                   conv_bias=False)\n",
        "\n",
        "        self.classifier = TransformerClassifier(\n",
        "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
        "                                                           height=img_size,\n",
        "                                                           width=img_size),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout_rate=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenizer(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "class TextCCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 seq_len=64,\n",
        "                 word_embedding_dim=300,\n",
        "                 embedding_dim=256,\n",
        "                 kernel_size=2,\n",
        "                 stride=1,\n",
        "                 padding=1,\n",
        "                 pooling_kernel_size=2,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(TextCCT, self).__init__()\n",
        "\n",
        "        self.embedder = Embedder(word_embedding_dim=word_embedding_dim,\n",
        "                                 *args, **kwargs)\n",
        "\n",
        "        self.tokenizer = TextTokenizer(n_input_channels=word_embedding_dim,\n",
        "                                       n_output_channels=embedding_dim,\n",
        "                                       kernel_size=kernel_size,\n",
        "                                       stride=stride,\n",
        "                                       padding=padding,\n",
        "                                       pooling_kernel_size=pooling_kernel_size,\n",
        "                                       pooling_stride=pooling_stride,\n",
        "                                       pooling_padding=pooling_padding,\n",
        "                                       max_pool=True,\n",
        "                                       activation=nn.ReLU)\n",
        "\n",
        "        self.classifier = MaskedTransformerClassifier(\n",
        "            seq_len=self.tokenizer.seq_len(seq_len=seq_len, embed_dim=word_embedding_dim),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x, mask = self.embedder(x, mask=mask)\n",
        "        x, mask = self.tokenizer(x, mask=mask)\n",
        "        out = self.classifier(x, mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "def _cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "         kernel_size=3, stride=None, padding=None,\n",
        "         *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "    return CCT(num_layers=num_layers,\n",
        "               num_heads=num_heads,\n",
        "               mlp_ratio=mlp_ratio,\n",
        "               embedding_dim=embedding_dim,\n",
        "               kernel_size=kernel_size,\n",
        "               stride=stride,\n",
        "               padding=padding,\n",
        "               *args, **kwargs)\n",
        "\n",
        "\n",
        "def _text_cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "              kernel_size=4, stride=None, padding=None,\n",
        "              *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "\n",
        "    return TextCCT(num_layers=num_layers,\n",
        "                   num_heads=num_heads,\n",
        "                   mlp_ratio=mlp_ratio,\n",
        "                   embedding_dim=embedding_dim,\n",
        "                   kernel_size=kernel_size,\n",
        "                   stride=stride,\n",
        "                   padding=padding,\n",
        "                   *args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cct_6(*args, **kwargs):\n",
        "    return _cct(num_layers=6, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                *args, **kwargs)\n",
        "    \n",
        "    \n",
        "def cct_7(*args, **kwargs):\n",
        "    return _cct(num_layers=7, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 430,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj6UX-dzvo1l"
      },
      "source": [
        "#/src/utils/embedder.py\n",
        "class Embedder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 word_embedding_dim=300,\n",
        "                 vocab_size=100000,\n",
        "                 padding_idx=1,\n",
        "                 pretrained_weight=None,\n",
        "                 embed_freeze=False,\n",
        "                 *args, **kwargs):\n",
        "        super(Embedder, self).__init__()\n",
        "        self.embeddings = nn.Embedding.from_pretrained(pretrained_weight, freeze=embed_freeze) \\\n",
        "            if pretrained_weight is not None else \\\n",
        "            nn.Embedding(vocab_size, word_embedding_dim, padding_idx=padding_idx)\n",
        "        self.embeddings.weight.requires_grad = not embed_freeze\n",
        "\n",
        "    def forward_mask(self, mask):\n",
        "        bsz, seq_len = mask.shape\n",
        "        new_mask = mask.view(bsz, seq_len, 1)\n",
        "        new_mask = new_mask.sum(-1)\n",
        "        new_mask = (new_mask > 0)\n",
        "        return new_mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        embed = self.embeddings(x)\n",
        "        embed = embed if mask is None else embed * self.forward_mask(mask).unsqueeze(-1).float()\n",
        "        return embed, mask\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        else:\n",
        "            nn.init.normal_(m.weight)"
      ],
      "execution_count": 431,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11WxM9ycvfy-"
      },
      "source": [
        "#/src/utils/tokenizer.py \n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 n_conv_layers=1,\n",
        "                 n_input_channels=3,\n",
        "                 n_output_channels=64,\n",
        "                 in_planes=64,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 conv_bias=False):\n",
        "        super(Tokenizer, self).__init__()\n",
        "\n",
        "        n_filter_list = [n_input_channels] + \\\n",
        "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
        "                        [n_output_channels]\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
        "                          kernel_size=(kernel_size, kernel_size),\n",
        "                          stride=(stride, stride),\n",
        "                          padding=(padding, padding), bias=conv_bias),\n",
        "                nn.Identity() if activation is None else activation(),\n",
        "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
        "                             stride=pooling_stride,\n",
        "                             padding=pooling_padding) if max_pool else nn.Identity()\n",
        "            )\n",
        "                for i in range(n_conv_layers)\n",
        "            ])\n",
        "\n",
        "        self.flattener = nn.Flatten(2, 3)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
        "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.flattener(self.conv_layers(x)).transpose(-2, -1)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "\n",
        "class TextTokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 embedding_dim=300,\n",
        "                 n_output_channels=128,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 *args, **kwargs):\n",
        "        super(TextTokenizer, self).__init__()\n",
        "\n",
        "        self.max_pool = max_pool\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, n_output_channels,\n",
        "                      kernel_size=(kernel_size, embedding_dim),\n",
        "                      stride=(stride, 1),\n",
        "                      padding=(padding, 0), bias=False),\n",
        "            nn.Identity() if activation is None else activation(),\n",
        "            nn.MaxPool2d(\n",
        "                kernel_size=(pooling_kernel_size, 1),\n",
        "                stride=(pooling_stride, 1),\n",
        "                padding=(pooling_padding, 0)\n",
        "            ) if max_pool else nn.Identity()\n",
        "        )\n",
        "\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def seq_len(self, seq_len=32, embed_dim=300):\n",
        "        return self.forward(torch.zeros((1, seq_len, embed_dim)))[0].shape[1]\n",
        "\n",
        "    def forward_mask(self, mask):\n",
        "        new_mask = mask.unsqueeze(1).float()\n",
        "        cnn_weight = torch.ones(\n",
        "            (1, 1, self.conv_layers[0].kernel_size[0]),\n",
        "            device=mask.device,\n",
        "            dtype=torch.float)\n",
        "        new_mask = F.conv1d(\n",
        "            new_mask, cnn_weight, None,\n",
        "            self.conv_layers[0].stride[0], self.conv_layers[0].padding[0], 1, 1)\n",
        "        if self.max_pool:\n",
        "            new_mask = F.max_pool1d(\n",
        "                new_mask, self.conv_layers[2].kernel_size[0],\n",
        "                self.conv_layers[2].stride[0], self.conv_layers[2].padding[0], 1, False, False)\n",
        "        new_mask = new_mask.squeeze(1)\n",
        "        new_mask = (new_mask > 0)\n",
        "        return new_mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.transpose(1, 3).squeeze(1)\n",
        "        x = x if mask is None else x * self.forward_mask(mask).unsqueeze(-1).float()\n",
        "        return x, mask\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)"
      ],
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i67i7ugxvSeM"
      },
      "source": [
        "#src/utils/transformers.py \n",
        "import torch\n",
        "from torch.nn import Module, ModuleList, Linear, Dropout, LayerNorm, Identity, Parameter, init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Attention(Module):\n",
        "    \"\"\"\n",
        "    Obtained from timm: github.com:rwightman/pytorch-image-models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MaskedAttention(Module):\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask_value = -torch.finfo(attn.dtype).max\n",
        "            assert mask.shape[-1] == attn.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
        "            attn.masked_fill_(~mask, mask_value)\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n",
        "                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "class MaskedTransformerEncoderLayer(Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(MaskedTransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = MaskedAttention(dim=d_model, num_heads=nhead,\n",
        "                                         attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, mask=None, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src), mask))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 sequence_length=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            sequence_length += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                    dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        return pe.unsqueeze(0)\n",
        "\n",
        "\n",
        "class MaskedTransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 seq_len=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert seq_len is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            seq_len += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                seq_len += 1  # padding idx\n",
        "                self.positional_emb = Parameter(torch.zeros(1, seq_len, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(seq_len,\n",
        "                                                                          embedding_dim,\n",
        "                                                                          padding_idx=True),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            MaskedTransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                          dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                          attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        if self.positional_emb is None and x.size(1) < self.seq_len:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "            if mask is not None:\n",
        "                mask = torch.cat([torch.ones(size=(mask.shape[0], 1), device=mask.device), mask.float()], dim=1)\n",
        "                mask = (mask > 0)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, mask=mask)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim, padding_idx=False):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        pe = pe.unsqueeze(0)\n",
        "        if padding_idx:\n",
        "            return torch.cat([torch.zeros((1, 1, dim)), pe], dim=1)\n",
        "        return pe"
      ],
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNOUNvqT7S3g"
      },
      "source": [
        "#utils/transforms.py \n",
        "class ShearX(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n",
        "            Image.BICUBIC, fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class ShearY(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n",
        "            Image.BICUBIC, fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class TranslateX(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, magnitude * x.size[0] * random.choice([-1, 1]), 0, 1, 0),\n",
        "            fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class TranslateY(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * x.size[1] * random.choice([-1, 1])),\n",
        "            fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class Rotate(object):\n",
        "    # from https://stackoverflow.com/questions/\n",
        "    # 5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n",
        "    def __call__(self, x, magnitude):\n",
        "        rot = x.convert(\"RGBA\").rotate(magnitude)\n",
        "        return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(x.mode)\n",
        "\n",
        "\n",
        "class Color(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Color(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Posterize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.posterize(x, magnitude)\n",
        "\n",
        "\n",
        "class Solarize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.solarize(x, magnitude)\n",
        "\n",
        "\n",
        "class Contrast(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Contrast(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Sharpness(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Sharpness(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Brightness(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Brightness(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class AutoContrast(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.autocontrast(x)\n",
        "\n",
        "\n",
        "class Equalize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.equalize(x)\n",
        "\n",
        "\n",
        "class Invert(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.invert(x)"
      ],
      "execution_count": 434,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6L2he3-60fH"
      },
      "source": [
        "#utils/autoaug.py \n",
        "class ImageNetPolicy(object):\n",
        "    \"\"\" Randomly choose one of the best 24 Sub-policies on ImageNet.\n",
        "        Example:\n",
        "        >>> policy = ImageNetPolicy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     ImageNetPolicy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.4, \"posterize\", 8, 0.6, \"rotate\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor),\n",
        "            SubPolicy(0.6, \"posterize\", 7, 0.6, \"posterize\", 6, fillcolor),\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 4, 0.8, \"rotate\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 3, 0.6, \"equalize\", 7, fillcolor),\n",
        "            SubPolicy(0.8, \"posterize\", 5, 1.0, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.2, \"rotate\", 3, 0.6, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 8, 0.4, \"posterize\", 6, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 0.4, \"color\", 0, fillcolor),\n",
        "            SubPolicy(0.4, \"rotate\", 9, 0.6, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.0, \"equalize\", 7, 0.8, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 1.0, \"color\", 2, fillcolor),\n",
        "            SubPolicy(0.8, \"color\", 8, 0.8, \"solarize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"sharpness\", 7, 0.6, \"invert\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"shearX\", 5, 1.0, \"equalize\", 9, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 0, 0.6, \"equalize\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment ImageNet Policy\"\n",
        "\n",
        "\n",
        "class CIFAR10Policy(object):\n",
        "    \"\"\" Randomly choose one of the best 25 Sub-policies on CIFAR10.\n",
        "        Example:\n",
        "        >>> policy = CIFAR10Policy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     CIFAR10Policy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n",
        "            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n",
        "            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n",
        "            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n",
        "            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n",
        "            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n",
        "\n",
        "            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n",
        "            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n",
        "            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n",
        "            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n",
        "            SubPolicy(0.2, \"equalize\", 8, 0.6, \"equalize\", 4, fillcolor),\n",
        "            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n",
        "            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n",
        "            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n",
        "            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment CIFAR10 Policy\"\n",
        "\n",
        "\n",
        "class SubPolicy(object):\n",
        "    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n",
        "        ranges = {\n",
        "            \"shearX\": np.linspace(0, 0.3, 10),\n",
        "            \"shearY\": np.linspace(0, 0.3, 10),\n",
        "            \"translateX\": np.linspace(0, 150 / 331, 10),\n",
        "            \"translateY\": np.linspace(0, 150 / 331, 10),\n",
        "            \"rotate\": np.linspace(0, 30, 10),\n",
        "            \"color\": np.linspace(0.0, 0.9, 10),\n",
        "            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n",
        "            \"solarize\": np.linspace(256, 0, 10),\n",
        "            \"contrast\": np.linspace(0.0, 0.9, 10),\n",
        "            \"sharpness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"brightness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"autocontrast\": [0] * 10,\n",
        "            \"equalize\": [0] * 10,\n",
        "            \"invert\": [0] * 10\n",
        "        }\n",
        "\n",
        "        func = {\n",
        "            \"shearX\": ShearX(fillcolor=fillcolor),\n",
        "            \"shearY\": ShearY(fillcolor=fillcolor),\n",
        "            \"translateX\": TranslateX(fillcolor=fillcolor),\n",
        "            \"translateY\": TranslateY(fillcolor=fillcolor),\n",
        "            \"rotate\": Rotate(),\n",
        "            \"color\": Color(),\n",
        "            \"posterize\": Posterize(),\n",
        "            \"solarize\": Solarize(),\n",
        "            \"contrast\": Contrast(),\n",
        "            \"sharpness\": Sharpness(),\n",
        "            \"brightness\": Brightness(),\n",
        "            \"autocontrast\": AutoContrast(),\n",
        "            \"equalize\": Equalize(),\n",
        "            \"invert\": Invert()\n",
        "        }\n",
        "\n",
        "        self.p1 = p1\n",
        "        self.operation1 = func[operation1]\n",
        "        self.magnitude1 = ranges[operation1][magnitude_idx1]\n",
        "        self.p2 = p2\n",
        "        self.operation2 = func[operation2]\n",
        "        self.magnitude2 = ranges[operation2][magnitude_idx2]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p1:\n",
        "            img = self.operation1(img, self.magnitude1)\n",
        "        if random.random() < self.p2:\n",
        "            img = self.operation2(img, self.magnitude2)\n",
        "        return img"
      ],
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR3mRPNHv1iY"
      },
      "source": [
        "#utils/stochastic_depth.py \n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)"
      ],
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npCm0MjVWfjU"
      },
      "source": [
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lrr = lr\n",
        "    if epoch < warmup:\n",
        "        lrr = lr / (warmup - epoch)\n",
        "    elif not disable_cos:\n",
        "        lrr *= 0.5 * (1. + math.cos(math.pi * (epoch - warmup) / (epochs - warmup)))\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lrr\n",
        "\n",
        "\n",
        "def accuracy(output, target):\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(1, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        correct_k = correct[:1].flatten().float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def cls_train(loss_train_arr, train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    loss_val, acc1_val = 0, 0\n",
        "    n = 0\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        if (not no_cuda) and torch.cuda.is_available():\n",
        "            images = images.cuda(gpu_id, non_blocking=True)\n",
        "            target = target.cuda(gpu_id, non_blocking=True)\n",
        "        output = model(images)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        acc1 = accuracy(output, target)\n",
        "        n += images.size(0)\n",
        "        loss_val += float(loss.item() * images.size(0))\n",
        "        acc1_val += float(acc1[0] * images.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        if clip_grad_norm > 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm, norm_type=2)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if print_freq >= 0 and i % print_freq == 0:\n",
        "            avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "            print(f'[Epoch {epoch + 1}][Train][{i}] \\t Loss: {avg_loss:.4e} \\t Top-1 {avg_acc1:6.2f}')\n",
        "    \n",
        "    loss_train_arr.append(avg_loss)\n",
        "    \n",
        "    return loss_train_arr\n",
        "\n",
        "\n",
        "def cls_validate(loss_val_arr, val_loader, model, criterion,  epoch=None, time_begin=None):\n",
        "    model.eval()\n",
        "    loss_val, acc1_val = 0, 0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            if (not no_cuda) and torch.cuda.is_available():\n",
        "                images = images.cuda(gpu_id, non_blocking=True)\n",
        "                target = target.cuda(gpu_id, non_blocking=True)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc1 = accuracy(output, target)\n",
        "            n += images.size(0)\n",
        "            loss_val += float(loss.item() * images.size(0))\n",
        "            acc1_val += float(acc1[0] * images.size(0))\n",
        "\n",
        "            if print_freq >= 0 and i % print_freq == 0:\n",
        "                avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "                print(f'[Epoch {epoch + 1}][Eval][{i}] \\t Loss: {avg_loss:.4e} \\t Top-1 {avg_acc1:6.2f}')\n",
        "\n",
        "    avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "    total_mins = -1 if time_begin is None else (time() - time_begin) / 60\n",
        "    print(f'[Epoch {epoch + 1}] \\t \\t Top-1 {avg_acc1:6.2f} \\t \\t Time: {total_mins:.2f}')\n",
        "\n",
        "    loss_val_arr.append(avg_loss)\n",
        "\n",
        "    return loss_val_arr, avg_acc1\n",
        "\n",
        "\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    NLL loss with label smoothing.\n",
        "    \"\"\"\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        \"\"\"\n",
        "        Constructor for the LabelSmoothing module.\n",
        "        :param smoothing: label smoothing factor\n",
        "        \"\"\"\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        assert smoothing < 1.0\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1. - smoothing\n",
        "\n",
        "    def _compute_losses(self, x, target):\n",
        "        log_prob = F.log_softmax(x, dim=-1)\n",
        "        nll_loss = -log_prob.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -log_prob.mean(dim=-1)\n",
        "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        return self._compute_losses(x, target).mean()"
      ],
      "execution_count": 437,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cfcCe-_qFgb"
      },
      "source": [
        "def test(model,data_loader,valid_ds):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        correct = 0\n",
        "        bs = batch_size\n",
        "        result = []\n",
        "        check_names = []\n",
        "        for i, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            arr = pred.data.cpu().numpy()\n",
        "            for j in range(pred.size()[0]):\n",
        "                file_name = valid_ds.samples[i*bs+j][0].split('/')[-1]\n",
        "                result.append((file_name,pred[j].cpu().numpy()[0])) \n",
        "        \n",
        "    return result"
      ],
      "execution_count": 438,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ZPaXDFx3w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a2b4e3-c3f5-40bc-b93b-1e9298b74856"
      },
      "source": [
        "#main.py \n",
        "\n",
        "\n",
        "loss_train_arr =[]\n",
        "loss_val_arr =[]\n",
        "\n",
        "img_size = DATASETS[dataset]['img_size']\n",
        "num_classes = DATASETS[dataset]['num_classes']\n",
        "img_mean, img_std = DATASETS[dataset]['mean'], DATASETS[dataset]['std']\n",
        "\n",
        "model = cct_7(img_size=img_size,\n",
        "            num_classes=num_classes,\n",
        "            positional_embedding=positional_embedding,\n",
        "            n_conv_layers=conv_layers,\n",
        "            kernel_size=conv_size,\n",
        "            patch_size=patch_size)\n",
        "\n",
        "model.load_state_dict(torch.load(pretrain_path, map_location='cpu'))\n",
        "print(\"Loaded checkpoint.\")\n",
        "\n",
        "criterion = LabelSmoothingCrossEntropy()\n",
        "\n",
        "if (not no_cuda) and torch.cuda.is_available():\n",
        "    torch.cuda.set_device(gpu_id)\n",
        "    model.cuda(gpu_id)\n",
        "    criterion = criterion.cuda(gpu_id)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr,\n",
        "                                weight_decay=weight_decay)\n",
        "\n",
        "normalize = [transforms.Normalize(mean=img_mean, std=img_std)]\n",
        "\n",
        "augmentations = []\n",
        "if not disable_aug:\n",
        "    #from utils.autoaug import CIFAR10Policy\n",
        "    augmentations += [\n",
        "        CIFAR10Policy()\n",
        "    ]\n",
        "augmentations += [\n",
        "    transforms.RandomCrop(img_size, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    *normalize,\n",
        "]\n",
        "\n",
        "augmentations = transforms.Compose(augmentations)\n",
        "train_dataset = ImageFolder(\n",
        "    root=data_dir+\"/TRAIN\",   transform=augmentations)\n",
        "\n",
        "val_dataset = ImageFolder(\n",
        "    root=data_dir+\"/TEST\",   transform=transforms.Compose([\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        *normalize,\n",
        "    ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True,\n",
        "    num_workers=workers)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size, shuffle=False,\n",
        "    num_workers=workers)\n",
        "\n",
        "print(\"Beginning training\")\n",
        "time_begin = time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    loss_train_arr = cls_train(loss_train_arr, train_loader, model, criterion, optimizer, epoch)\n",
        "    loss_val_arr, acc1 = cls_validate(loss_val_arr, val_loader, model, criterion,  epoch=epoch, time_begin=time_begin)\n",
        "    best_acc1 = max(acc1, best_acc1)\n",
        "\n",
        "total_mins = (time() - time_begin) / 60\n",
        "print(f'Script finished in {total_mins:.2f} minutes, '\n",
        "        f'best top-1: {best_acc1:.2f}, '\n",
        "        f'final top-1: {acc1:.2f}')\n",
        "torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "result = test(model, val_loader, val_dataset)\n",
        "\n",
        "with open ('ID_result.csv','w') as f:\n",
        "    f.write('Id,Category\\n')\n",
        "    for data in result:\n",
        "        f.write(data[0]+','+str(data[1])+'\\n')\n"
      ],
      "execution_count": 439,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded checkpoint.\n",
            "Beginning training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1][Train][0] \t Loss: 1.2220e+00 \t Top-1  91.41\n",
            "[Epoch 1][Train][10] \t Loss: 1.1638e+00 \t Top-1  89.70\n",
            "[Epoch 1][Train][20] \t Loss: 1.1755e+00 \t Top-1  89.03\n",
            "[Epoch 1][Train][30] \t Loss: 1.1678e+00 \t Top-1  89.26\n",
            "[Epoch 1][Train][40] \t Loss: 1.1695e+00 \t Top-1  89.25\n",
            "[Epoch 1][Train][50] \t Loss: 1.1691e+00 \t Top-1  89.19\n",
            "[Epoch 1][Train][60] \t Loss: 1.1611e+00 \t Top-1  89.40\n",
            "[Epoch 1][Train][70] \t Loss: 1.1566e+00 \t Top-1  89.56\n",
            "[Epoch 1][Train][80] \t Loss: 1.1562e+00 \t Top-1  89.63\n",
            "[Epoch 1][Train][90] \t Loss: 1.1497e+00 \t Top-1  89.80\n",
            "[Epoch 1][Train][100] \t Loss: 1.1485e+00 \t Top-1  89.79\n",
            "[Epoch 1][Train][110] \t Loss: 1.1486e+00 \t Top-1  89.75\n",
            "[Epoch 1][Train][120] \t Loss: 1.1490e+00 \t Top-1  89.72\n",
            "[Epoch 1][Train][130] \t Loss: 1.1484e+00 \t Top-1  89.74\n",
            "[Epoch 1][Train][140] \t Loss: 1.1463e+00 \t Top-1  89.83\n",
            "[Epoch 1][Train][150] \t Loss: 1.1458e+00 \t Top-1  89.86\n",
            "[Epoch 1][Train][160] \t Loss: 1.1431e+00 \t Top-1  89.96\n",
            "[Epoch 1][Train][170] \t Loss: 1.1407e+00 \t Top-1  90.02\n",
            "[Epoch 1][Train][180] \t Loss: 1.1403e+00 \t Top-1  90.08\n",
            "[Epoch 1][Train][190] \t Loss: 1.1397e+00 \t Top-1  90.17\n",
            "[Epoch 1][Train][200] \t Loss: 1.1375e+00 \t Top-1  90.24\n",
            "[Epoch 1][Train][210] \t Loss: 1.1381e+00 \t Top-1  90.21\n",
            "[Epoch 1][Train][220] \t Loss: 1.1350e+00 \t Top-1  90.30\n",
            "[Epoch 1][Train][230] \t Loss: 1.1352e+00 \t Top-1  90.23\n",
            "[Epoch 1][Train][240] \t Loss: 1.1345e+00 \t Top-1  90.28\n",
            "[Epoch 1][Train][250] \t Loss: 1.1326e+00 \t Top-1  90.37\n",
            "[Epoch 1][Train][260] \t Loss: 1.1313e+00 \t Top-1  90.41\n",
            "[Epoch 1][Train][270] \t Loss: 1.1320e+00 \t Top-1  90.37\n",
            "[Epoch 1][Train][280] \t Loss: 1.1310e+00 \t Top-1  90.38\n",
            "[Epoch 1][Train][290] \t Loss: 1.1321e+00 \t Top-1  90.32\n",
            "[Epoch 1][Train][300] \t Loss: 1.1318e+00 \t Top-1  90.31\n",
            "[Epoch 1][Train][310] \t Loss: 1.1306e+00 \t Top-1  90.36\n",
            "[Epoch 1][Train][320] \t Loss: 1.1298e+00 \t Top-1  90.38\n",
            "[Epoch 1][Train][330] \t Loss: 1.1297e+00 \t Top-1  90.36\n",
            "[Epoch 1][Train][340] \t Loss: 1.1293e+00 \t Top-1  90.35\n",
            "[Epoch 1][Train][350] \t Loss: 1.1275e+00 \t Top-1  90.41\n",
            "[Epoch 1][Train][360] \t Loss: 1.1279e+00 \t Top-1  90.40\n",
            "[Epoch 1][Train][370] \t Loss: 1.1286e+00 \t Top-1  90.37\n",
            "[Epoch 1][Train][380] \t Loss: 1.1281e+00 \t Top-1  90.40\n",
            "[Epoch 1][Train][390] \t Loss: 1.1279e+00 \t Top-1  90.39\n",
            "[Epoch 1][Eval][0] \t Loss: 1.0173e+00 \t Top-1  90.62\n",
            "[Epoch 1][Eval][10] \t Loss: 1.3816e+00 \t Top-1  79.05\n",
            "[Epoch 1][Eval][20] \t Loss: 1.3171e+00 \t Top-1  82.37\n",
            "[Epoch 1][Eval][30] \t Loss: 1.3652e+00 \t Top-1  81.02\n",
            "[Epoch 1][Eval][40] \t Loss: 1.3742e+00 \t Top-1  80.95\n",
            "[Epoch 1][Eval][50] \t Loss: 1.3670e+00 \t Top-1  81.00\n",
            "[Epoch 1][Eval][60] \t Loss: 1.3722e+00 \t Top-1  80.99\n",
            "[Epoch 1][Eval][70] \t Loss: 1.3550e+00 \t Top-1  81.64\n",
            "[Epoch 1] \t \t Top-1  81.56 \t \t Time: 6.94\n",
            "Script finished in 6.94 minutes, best top-1: 81.56, final top-1: 81.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt6ogGMRWlzY",
        "outputId": "908a00c8-4ef1-467e-ed8b-d0abb7320f68"
      },
      "source": [
        "\n",
        "from torchsummary import summary\n",
        "# Print model\n",
        "print(model)\n",
        "\n",
        "# Print parameter\n",
        "size = summary(model, (3, 32, 32))\n"
      ],
      "execution_count": 440,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CCT(\n",
            "  (tokenizer): Tokenizer(\n",
            "    (conv_layers): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): ReLU()\n",
            "        (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "    )\n",
            "    (flattener): Flatten(start_dim=2, end_dim=3)\n",
            "  )\n",
            "  (classifier): TransformerClassifier(\n",
            "    (attention_pool): Linear(in_features=256, out_features=1, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "    (blocks): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (6): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (fc): Linear(in_features=256, out_features=100, bias=True)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 256, 32, 32]           6,912\n",
            "              ReLU-2          [-1, 256, 32, 32]               0\n",
            "         MaxPool2d-3          [-1, 256, 16, 16]               0\n",
            "           Flatten-4             [-1, 256, 256]               0\n",
            "         Tokenizer-5             [-1, 256, 256]               0\n",
            "           Dropout-6             [-1, 256, 256]               0\n",
            "         LayerNorm-7             [-1, 256, 256]             512\n",
            "            Linear-8             [-1, 256, 768]         196,608\n",
            "           Dropout-9          [-1, 4, 256, 256]               0\n",
            "           Linear-10             [-1, 256, 256]          65,792\n",
            "          Dropout-11             [-1, 256, 256]               0\n",
            "        Attention-12             [-1, 256, 256]               0\n",
            "         Identity-13             [-1, 256, 256]               0\n",
            "        LayerNorm-14             [-1, 256, 256]             512\n",
            "           Linear-15             [-1, 256, 512]         131,584\n",
            "          Dropout-16             [-1, 256, 512]               0\n",
            "           Linear-17             [-1, 256, 256]         131,328\n",
            "          Dropout-18             [-1, 256, 256]               0\n",
            "         Identity-19             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-20             [-1, 256, 256]               0\n",
            "        LayerNorm-21             [-1, 256, 256]             512\n",
            "           Linear-22             [-1, 256, 768]         196,608\n",
            "          Dropout-23          [-1, 4, 256, 256]               0\n",
            "           Linear-24             [-1, 256, 256]          65,792\n",
            "          Dropout-25             [-1, 256, 256]               0\n",
            "        Attention-26             [-1, 256, 256]               0\n",
            "         DropPath-27             [-1, 256, 256]               0\n",
            "        LayerNorm-28             [-1, 256, 256]             512\n",
            "           Linear-29             [-1, 256, 512]         131,584\n",
            "          Dropout-30             [-1, 256, 512]               0\n",
            "           Linear-31             [-1, 256, 256]         131,328\n",
            "          Dropout-32             [-1, 256, 256]               0\n",
            "         DropPath-33             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-34             [-1, 256, 256]               0\n",
            "        LayerNorm-35             [-1, 256, 256]             512\n",
            "           Linear-36             [-1, 256, 768]         196,608\n",
            "          Dropout-37          [-1, 4, 256, 256]               0\n",
            "           Linear-38             [-1, 256, 256]          65,792\n",
            "          Dropout-39             [-1, 256, 256]               0\n",
            "        Attention-40             [-1, 256, 256]               0\n",
            "         DropPath-41             [-1, 256, 256]               0\n",
            "        LayerNorm-42             [-1, 256, 256]             512\n",
            "           Linear-43             [-1, 256, 512]         131,584\n",
            "          Dropout-44             [-1, 256, 512]               0\n",
            "           Linear-45             [-1, 256, 256]         131,328\n",
            "          Dropout-46             [-1, 256, 256]               0\n",
            "         DropPath-47             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-48             [-1, 256, 256]               0\n",
            "        LayerNorm-49             [-1, 256, 256]             512\n",
            "           Linear-50             [-1, 256, 768]         196,608\n",
            "          Dropout-51          [-1, 4, 256, 256]               0\n",
            "           Linear-52             [-1, 256, 256]          65,792\n",
            "          Dropout-53             [-1, 256, 256]               0\n",
            "        Attention-54             [-1, 256, 256]               0\n",
            "         DropPath-55             [-1, 256, 256]               0\n",
            "        LayerNorm-56             [-1, 256, 256]             512\n",
            "           Linear-57             [-1, 256, 512]         131,584\n",
            "          Dropout-58             [-1, 256, 512]               0\n",
            "           Linear-59             [-1, 256, 256]         131,328\n",
            "          Dropout-60             [-1, 256, 256]               0\n",
            "         DropPath-61             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-62             [-1, 256, 256]               0\n",
            "        LayerNorm-63             [-1, 256, 256]             512\n",
            "           Linear-64             [-1, 256, 768]         196,608\n",
            "          Dropout-65          [-1, 4, 256, 256]               0\n",
            "           Linear-66             [-1, 256, 256]          65,792\n",
            "          Dropout-67             [-1, 256, 256]               0\n",
            "        Attention-68             [-1, 256, 256]               0\n",
            "         DropPath-69             [-1, 256, 256]               0\n",
            "        LayerNorm-70             [-1, 256, 256]             512\n",
            "           Linear-71             [-1, 256, 512]         131,584\n",
            "          Dropout-72             [-1, 256, 512]               0\n",
            "           Linear-73             [-1, 256, 256]         131,328\n",
            "          Dropout-74             [-1, 256, 256]               0\n",
            "         DropPath-75             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-76             [-1, 256, 256]               0\n",
            "        LayerNorm-77             [-1, 256, 256]             512\n",
            "           Linear-78             [-1, 256, 768]         196,608\n",
            "          Dropout-79          [-1, 4, 256, 256]               0\n",
            "           Linear-80             [-1, 256, 256]          65,792\n",
            "          Dropout-81             [-1, 256, 256]               0\n",
            "        Attention-82             [-1, 256, 256]               0\n",
            "         DropPath-83             [-1, 256, 256]               0\n",
            "        LayerNorm-84             [-1, 256, 256]             512\n",
            "           Linear-85             [-1, 256, 512]         131,584\n",
            "          Dropout-86             [-1, 256, 512]               0\n",
            "           Linear-87             [-1, 256, 256]         131,328\n",
            "          Dropout-88             [-1, 256, 256]               0\n",
            "         DropPath-89             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-90             [-1, 256, 256]               0\n",
            "        LayerNorm-91             [-1, 256, 256]             512\n",
            "           Linear-92             [-1, 256, 768]         196,608\n",
            "          Dropout-93          [-1, 4, 256, 256]               0\n",
            "           Linear-94             [-1, 256, 256]          65,792\n",
            "          Dropout-95             [-1, 256, 256]               0\n",
            "        Attention-96             [-1, 256, 256]               0\n",
            "         DropPath-97             [-1, 256, 256]               0\n",
            "        LayerNorm-98             [-1, 256, 256]             512\n",
            "           Linear-99             [-1, 256, 512]         131,584\n",
            "         Dropout-100             [-1, 256, 512]               0\n",
            "          Linear-101             [-1, 256, 256]         131,328\n",
            "         Dropout-102             [-1, 256, 256]               0\n",
            "        DropPath-103             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-104             [-1, 256, 256]               0\n",
            "       LayerNorm-105             [-1, 256, 256]             512\n",
            "          Linear-106               [-1, 256, 1]             257\n",
            "          Linear-107                  [-1, 100]          25,700\n",
            "TransformerClassifier-108                  [-1, 100]               0\n",
            "================================================================\n",
            "Total params: 3,717,733\n",
            "Trainable params: 3,717,733\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 80.00\n",
            "Params size (MB): 14.18\n",
            "Estimated Total Size (MB): 94.20\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "BaEQExMDWoEV",
        "outputId": "e4b89b41-0f28-4363-ee20-c094e499193a"
      },
      "source": [
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n",
        "\n",
        "\n",
        "plt.plot([x for x in loss_train_arr], \"-bx\")\n",
        "plt.plot([x for x in loss_val_arr],\"-rx\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"train loss\",\"val loss\"])\n"
      ],
      "execution_count": 441,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f7fc4a65a10>"
            ]
          },
          "metadata": {},
          "execution_count": 441
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}