{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "CCT-7/3x1 timm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 393,
      "source": [
        "from time import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.datasets import ImageFolder, CIFAR100\n",
        "import random\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "WhSQ4zfq0CfM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "source": [
        "!pip install torchtoolbox"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtoolbox in /usr/local/lib/python3.7/dist-packages (0.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.22.2.post1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.19.5)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.62.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (3.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torchtoolbox) (1.0.1)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57692838La4I",
        "outputId": "b1d82eeb-707d-4d99-ff8e-5833a84ffdd4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "\n",
        "\n",
        "od.download('https://www.kaggle.com/c/2021-ai-training-final-project/data')\n",
        "data_dir = './2021-ai-training-final-project/CIFAR100'"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.7/dist-packages (0.1.20)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.62.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.5.30)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (5.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Skipping, found downloaded files in \"./2021-ai-training-final-project\" (use force=True to force download)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV0WT80KVVms",
        "outputId": "3a52b19d-8c7f-478c-9c70-20062c5796e8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 396,
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "device = get_default_device()\n",
        "print(device)\n",
        "!nvidia-smi"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Thu Aug 26 14:56:57 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    61W / 149W |   2143MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxFa3TYR3oA8",
        "outputId": "612fe480-75a8-40c2-a098-0b432b4e8886"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 397,
      "source": [
        "\n",
        "# Data args\n",
        "dataset='cifar100'\n",
        "workers=4\n",
        "print_freq=10\n",
        "pretrain_path='cct6-3x2_cifar100_best.pth'\n",
        "checkpoint_path = 'cct6-3x2_cifar100_training.pth'\n",
        "\n",
        "# Optimization hyperparams\n",
        "epochs=1\n",
        "warmup=5\n",
        "batch_size=128\n",
        "lr=0.0005\n",
        "weight_decay=3e-2\n",
        "clip_grad_norm=0.\n",
        "model='cct_6'\n",
        "positional_embedding='learnable'\n",
        "conv_layers=2\n",
        "conv_size=3\n",
        "patch_size=4\n",
        "gpu_id=0\n",
        "no_cuda=False\n",
        "disable_aug=False\n",
        "disable_cos=False\n",
        "\n",
        "best_acc1=0\n",
        "\n",
        "DATASETS = {\n",
        "    'cifar100': {\n",
        "        'num_classes': 100,\n",
        "        'img_size': 32,\n",
        "        'mean': [0.5071, 0.4867, 0.4408],\n",
        "        'std': [0.2675, 0.2565, 0.2761]\n",
        "    }\n",
        "}\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "wMDYedpmsyjY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 398,
      "source": [
        "#src/cct.py \n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "__all__ = ['cct_2', 'cct_4', 'cct_6', 'cct_7', 'cct_8',\n",
        "           'cct_14', 'cct_16',\n",
        "           'text_cct_2', 'text_cct_4', 'text_cct_6'\n",
        "           ]\n",
        "\n",
        "\n",
        "class CCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 embedding_dim=768,\n",
        "                 n_input_channels=3,\n",
        "                 n_conv_layers=1,\n",
        "                 kernel_size=7,\n",
        "                 stride=2,\n",
        "                 padding=3,\n",
        "                 pooling_kernel_size=3,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(CCT, self).__init__()\n",
        "\n",
        "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
        "                                   n_output_channels=embedding_dim,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   stride=stride,\n",
        "                                   padding=padding,\n",
        "                                   pooling_kernel_size=pooling_kernel_size,\n",
        "                                   pooling_stride=pooling_stride,\n",
        "                                   pooling_padding=pooling_padding,\n",
        "                                   max_pool=True,\n",
        "                                   activation=nn.ReLU,\n",
        "                                   n_conv_layers=n_conv_layers,\n",
        "                                   conv_bias=False)\n",
        "\n",
        "        self.classifier = TransformerClassifier(\n",
        "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
        "                                                           height=img_size,\n",
        "                                                           width=img_size),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout_rate=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenizer(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "class TextCCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 seq_len=64,\n",
        "                 word_embedding_dim=300,\n",
        "                 embedding_dim=256,\n",
        "                 kernel_size=2,\n",
        "                 stride=1,\n",
        "                 padding=1,\n",
        "                 pooling_kernel_size=2,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(TextCCT, self).__init__()\n",
        "\n",
        "        self.embedder = Embedder(word_embedding_dim=word_embedding_dim,\n",
        "                                 *args, **kwargs)\n",
        "\n",
        "        self.tokenizer = TextTokenizer(n_input_channels=word_embedding_dim,\n",
        "                                       n_output_channels=embedding_dim,\n",
        "                                       kernel_size=kernel_size,\n",
        "                                       stride=stride,\n",
        "                                       padding=padding,\n",
        "                                       pooling_kernel_size=pooling_kernel_size,\n",
        "                                       pooling_stride=pooling_stride,\n",
        "                                       pooling_padding=pooling_padding,\n",
        "                                       max_pool=True,\n",
        "                                       activation=nn.ReLU)\n",
        "\n",
        "        self.classifier = MaskedTransformerClassifier(\n",
        "            seq_len=self.tokenizer.seq_len(seq_len=seq_len, embed_dim=word_embedding_dim),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x, mask = self.embedder(x, mask=mask)\n",
        "        x, mask = self.tokenizer(x, mask=mask)\n",
        "        out = self.classifier(x, mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "def _cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "         kernel_size=3, stride=None, padding=None,\n",
        "         *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "    return CCT(num_layers=num_layers,\n",
        "               num_heads=num_heads,\n",
        "               mlp_ratio=mlp_ratio,\n",
        "               embedding_dim=embedding_dim,\n",
        "               kernel_size=kernel_size,\n",
        "               stride=stride,\n",
        "               padding=padding,\n",
        "               *args, **kwargs)\n",
        "\n",
        "\n",
        "def _text_cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "              kernel_size=4, stride=None, padding=None,\n",
        "              *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "\n",
        "    return TextCCT(num_layers=num_layers,\n",
        "                   num_heads=num_heads,\n",
        "                   mlp_ratio=mlp_ratio,\n",
        "                   embedding_dim=embedding_dim,\n",
        "                   kernel_size=kernel_size,\n",
        "                   stride=stride,\n",
        "                   padding=padding,\n",
        "                   *args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cct_6(*args, **kwargs):\n",
        "    return _cct(num_layers=6, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "xcGLeXU7te0B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 399,
      "source": [
        "#/src/utils/embedder.py\n",
        "class Embedder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 word_embedding_dim=300,\n",
        "                 vocab_size=100000,\n",
        "                 padding_idx=1,\n",
        "                 pretrained_weight=None,\n",
        "                 embed_freeze=False,\n",
        "                 *args, **kwargs):\n",
        "        super(Embedder, self).__init__()\n",
        "        self.embeddings = nn.Embedding.from_pretrained(pretrained_weight, freeze=embed_freeze) \\\n",
        "            if pretrained_weight is not None else \\\n",
        "            nn.Embedding(vocab_size, word_embedding_dim, padding_idx=padding_idx)\n",
        "        self.embeddings.weight.requires_grad = not embed_freeze\n",
        "\n",
        "    def forward_mask(self, mask):\n",
        "        bsz, seq_len = mask.shape\n",
        "        new_mask = mask.view(bsz, seq_len, 1)\n",
        "        new_mask = new_mask.sum(-1)\n",
        "        new_mask = (new_mask > 0)\n",
        "        return new_mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        embed = self.embeddings(x)\n",
        "        embed = embed if mask is None else embed * self.forward_mask(mask).unsqueeze(-1).float()\n",
        "        return embed, mask\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        else:\n",
        "            nn.init.normal_(m.weight)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Sj6UX-dzvo1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 400,
      "source": [
        "#/src/utils/tokenizer.py \n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 n_conv_layers=1,\n",
        "                 n_input_channels=3,\n",
        "                 n_output_channels=64,\n",
        "                 in_planes=64,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 conv_bias=False):\n",
        "        super(Tokenizer, self).__init__()\n",
        "\n",
        "        n_filter_list = [n_input_channels] + \\\n",
        "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
        "                        [n_output_channels]\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
        "                          kernel_size=(kernel_size, kernel_size),\n",
        "                          stride=(stride, stride),\n",
        "                          padding=(padding, padding), bias=conv_bias),\n",
        "                nn.Identity() if activation is None else activation(),\n",
        "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
        "                             stride=pooling_stride,\n",
        "                             padding=pooling_padding) if max_pool else nn.Identity()\n",
        "            )\n",
        "                for i in range(n_conv_layers)\n",
        "            ])\n",
        "\n",
        "        self.flattener = nn.Flatten(2, 3)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
        "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.flattener(self.conv_layers(x)).transpose(-2, -1)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "\n",
        "class TextTokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 embedding_dim=300,\n",
        "                 n_output_channels=128,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 *args, **kwargs):\n",
        "        super(TextTokenizer, self).__init__()\n",
        "\n",
        "        self.max_pool = max_pool\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, n_output_channels,\n",
        "                      kernel_size=(kernel_size, embedding_dim),\n",
        "                      stride=(stride, 1),\n",
        "                      padding=(padding, 0), bias=False),\n",
        "            nn.Identity() if activation is None else activation(),\n",
        "            nn.MaxPool2d(\n",
        "                kernel_size=(pooling_kernel_size, 1),\n",
        "                stride=(pooling_stride, 1),\n",
        "                padding=(pooling_padding, 0)\n",
        "            ) if max_pool else nn.Identity()\n",
        "        )\n",
        "\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def seq_len(self, seq_len=32, embed_dim=300):\n",
        "        return self.forward(torch.zeros((1, seq_len, embed_dim)))[0].shape[1]\n",
        "\n",
        "    def forward_mask(self, mask):\n",
        "        new_mask = mask.unsqueeze(1).float()\n",
        "        cnn_weight = torch.ones(\n",
        "            (1, 1, self.conv_layers[0].kernel_size[0]),\n",
        "            device=mask.device,\n",
        "            dtype=torch.float)\n",
        "        new_mask = F.conv1d(\n",
        "            new_mask, cnn_weight, None,\n",
        "            self.conv_layers[0].stride[0], self.conv_layers[0].padding[0], 1, 1)\n",
        "        if self.max_pool:\n",
        "            new_mask = F.max_pool1d(\n",
        "                new_mask, self.conv_layers[2].kernel_size[0],\n",
        "                self.conv_layers[2].stride[0], self.conv_layers[2].padding[0], 1, False, False)\n",
        "        new_mask = new_mask.squeeze(1)\n",
        "        new_mask = (new_mask > 0)\n",
        "        return new_mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.transpose(1, 3).squeeze(1)\n",
        "        x = x if mask is None else x * self.forward_mask(mask).unsqueeze(-1).float()\n",
        "        return x, mask\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)"
      ],
      "outputs": [],
      "metadata": {
        "id": "11WxM9ycvfy-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 401,
      "source": [
        "#src/utils/transformers.py \n",
        "import torch\n",
        "from torch.nn import Module, ModuleList, Linear, Dropout, LayerNorm, Identity, Parameter, init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Attention(Module):\n",
        "    \"\"\"\n",
        "    Obtained from timm: github.com:rwightman/pytorch-image-models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MaskedAttention(Module):\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask_value = -torch.finfo(attn.dtype).max\n",
        "            assert mask.shape[-1] == attn.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
        "            attn.masked_fill_(~mask, mask_value)\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n",
        "                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "class MaskedTransformerEncoderLayer(Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(MaskedTransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = MaskedAttention(dim=d_model, num_heads=nhead,\n",
        "                                         attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, mask=None, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src), mask))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 sequence_length=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            sequence_length += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                    dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        return pe.unsqueeze(0)\n",
        "\n",
        "\n",
        "class MaskedTransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 seq_len=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert seq_len is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            seq_len += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                seq_len += 1  # padding idx\n",
        "                self.positional_emb = Parameter(torch.zeros(1, seq_len, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(seq_len,\n",
        "                                                                          embedding_dim,\n",
        "                                                                          padding_idx=True),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            MaskedTransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                          dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                          attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        if self.positional_emb is None and x.size(1) < self.seq_len:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "            if mask is not None:\n",
        "                mask = torch.cat([torch.ones(size=(mask.shape[0], 1), device=mask.device), mask.float()], dim=1)\n",
        "                mask = (mask > 0)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, mask=mask)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim, padding_idx=False):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        pe = pe.unsqueeze(0)\n",
        "        if padding_idx:\n",
        "            return torch.cat([torch.zeros((1, 1, dim)), pe], dim=1)\n",
        "        return pe"
      ],
      "outputs": [],
      "metadata": {
        "id": "i67i7ugxvSeM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 402,
      "source": [
        "#utils/transforms.py \n",
        "class ShearX(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n",
        "            Image.BICUBIC, fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class ShearY(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n",
        "            Image.BICUBIC, fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class TranslateX(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, magnitude * x.size[0] * random.choice([-1, 1]), 0, 1, 0),\n",
        "            fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class TranslateY(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * x.size[1] * random.choice([-1, 1])),\n",
        "            fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class Rotate(object):\n",
        "    # from https://stackoverflow.com/questions/\n",
        "    # 5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n",
        "    def __call__(self, x, magnitude):\n",
        "        rot = x.convert(\"RGBA\").rotate(magnitude)\n",
        "        return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(x.mode)\n",
        "\n",
        "\n",
        "class Color(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Color(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Posterize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.posterize(x, magnitude)\n",
        "\n",
        "\n",
        "class Solarize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.solarize(x, magnitude)\n",
        "\n",
        "\n",
        "class Contrast(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Contrast(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Sharpness(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Sharpness(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Brightness(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Brightness(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class AutoContrast(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.autocontrast(x)\n",
        "\n",
        "\n",
        "class Equalize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.equalize(x)\n",
        "\n",
        "\n",
        "class Invert(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.invert(x)"
      ],
      "outputs": [],
      "metadata": {
        "id": "uNOUNvqT7S3g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 403,
      "source": [
        "#utils/autoaug.py \n",
        "class ImageNetPolicy(object):\n",
        "    \"\"\" Randomly choose one of the best 24 Sub-policies on ImageNet.\n",
        "        Example:\n",
        "        >>> policy = ImageNetPolicy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     ImageNetPolicy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.4, \"posterize\", 8, 0.6, \"rotate\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor),\n",
        "            SubPolicy(0.6, \"posterize\", 7, 0.6, \"posterize\", 6, fillcolor),\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 4, 0.8, \"rotate\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 3, 0.6, \"equalize\", 7, fillcolor),\n",
        "            SubPolicy(0.8, \"posterize\", 5, 1.0, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.2, \"rotate\", 3, 0.6, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 8, 0.4, \"posterize\", 6, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 0.4, \"color\", 0, fillcolor),\n",
        "            SubPolicy(0.4, \"rotate\", 9, 0.6, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.0, \"equalize\", 7, 0.8, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 1.0, \"color\", 2, fillcolor),\n",
        "            SubPolicy(0.8, \"color\", 8, 0.8, \"solarize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"sharpness\", 7, 0.6, \"invert\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"shearX\", 5, 1.0, \"equalize\", 9, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 0, 0.6, \"equalize\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment ImageNet Policy\"\n",
        "\n",
        "\n",
        "class CIFAR10Policy(object):\n",
        "    \"\"\" Randomly choose one of the best 25 Sub-policies on CIFAR10.\n",
        "        Example:\n",
        "        >>> policy = CIFAR10Policy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     CIFAR10Policy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n",
        "            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n",
        "            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n",
        "            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n",
        "            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n",
        "            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n",
        "\n",
        "            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n",
        "            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n",
        "            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n",
        "            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n",
        "            SubPolicy(0.2, \"equalize\", 8, 0.6, \"equalize\", 4, fillcolor),\n",
        "            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n",
        "            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n",
        "            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n",
        "            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment CIFAR10 Policy\"\n",
        "\n",
        "\n",
        "class SubPolicy(object):\n",
        "    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n",
        "        ranges = {\n",
        "            \"shearX\": np.linspace(0, 0.3, 10),\n",
        "            \"shearY\": np.linspace(0, 0.3, 10),\n",
        "            \"translateX\": np.linspace(0, 150 / 331, 10),\n",
        "            \"translateY\": np.linspace(0, 150 / 331, 10),\n",
        "            \"rotate\": np.linspace(0, 30, 10),\n",
        "            \"color\": np.linspace(0.0, 0.9, 10),\n",
        "            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n",
        "            \"solarize\": np.linspace(256, 0, 10),\n",
        "            \"contrast\": np.linspace(0.0, 0.9, 10),\n",
        "            \"sharpness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"brightness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"autocontrast\": [0] * 10,\n",
        "            \"equalize\": [0] * 10,\n",
        "            \"invert\": [0] * 10\n",
        "        }\n",
        "\n",
        "        func = {\n",
        "            \"shearX\": ShearX(fillcolor=fillcolor),\n",
        "            \"shearY\": ShearY(fillcolor=fillcolor),\n",
        "            \"translateX\": TranslateX(fillcolor=fillcolor),\n",
        "            \"translateY\": TranslateY(fillcolor=fillcolor),\n",
        "            \"rotate\": Rotate(),\n",
        "            \"color\": Color(),\n",
        "            \"posterize\": Posterize(),\n",
        "            \"solarize\": Solarize(),\n",
        "            \"contrast\": Contrast(),\n",
        "            \"sharpness\": Sharpness(),\n",
        "            \"brightness\": Brightness(),\n",
        "            \"autocontrast\": AutoContrast(),\n",
        "            \"equalize\": Equalize(),\n",
        "            \"invert\": Invert()\n",
        "        }\n",
        "\n",
        "        self.p1 = p1\n",
        "        self.operation1 = func[operation1]\n",
        "        self.magnitude1 = ranges[operation1][magnitude_idx1]\n",
        "        self.p2 = p2\n",
        "        self.operation2 = func[operation2]\n",
        "        self.magnitude2 = ranges[operation2][magnitude_idx2]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p1:\n",
        "            img = self.operation1(img, self.magnitude1)\n",
        "        if random.random() < self.p2:\n",
        "            img = self.operation2(img, self.magnitude2)\n",
        "        return img"
      ],
      "outputs": [],
      "metadata": {
        "id": "H6L2he3-60fH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 404,
      "source": [
        "#utils/stochastic_depth.py \n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)"
      ],
      "outputs": [],
      "metadata": {
        "id": "HR3mRPNHv1iY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 405,
      "source": [
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lrr = lr\n",
        "    if epoch < warmup:\n",
        "        lrr = lr / (warmup - epoch)\n",
        "    elif not disable_cos:\n",
        "        lrr *= 0.5 * (1. + math.cos(math.pi * (epoch - warmup) / (epochs - warmup)))\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lrr\n",
        "\n",
        "\n",
        "def accuracy(output, target):\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(1, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        correct_k = correct[:1].flatten().float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def cls_train(loss_train_arr, train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    loss_val, acc1_val = 0, 0\n",
        "    n = 0\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        if (not no_cuda) and torch.cuda.is_available():\n",
        "            images = images.cuda(gpu_id, non_blocking=True)\n",
        "            target = target.cuda(gpu_id, non_blocking=True)\n",
        "        output = model(images)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        acc1 = accuracy(output, target)\n",
        "        n += images.size(0)\n",
        "        loss_val += float(loss.item() * images.size(0))\n",
        "        acc1_val += float(acc1[0] * images.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        if clip_grad_norm > 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm, norm_type=2)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if print_freq >= 0 and i % print_freq == 0:\n",
        "            avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "            print(f'[Epoch {epoch + 1}][Train][{i}] \\t Loss: {avg_loss:.4e} \\t Top-1 {avg_acc1:6.2f}')\n",
        "    \n",
        "    loss_train_arr.append(avg_loss)\n",
        "    \n",
        "    return loss_train_arr\n",
        "\n",
        "\n",
        "def cls_validate(loss_val_arr, val_loader, model, criterion,  epoch=None, time_begin=None):\n",
        "    model.eval()\n",
        "    loss_val, acc1_val = 0, 0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            if (not no_cuda) and torch.cuda.is_available():\n",
        "                images = images.cuda(gpu_id, non_blocking=True)\n",
        "                target = target.cuda(gpu_id, non_blocking=True)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc1 = accuracy(output, target)\n",
        "            n += images.size(0)\n",
        "            loss_val += float(loss.item() * images.size(0))\n",
        "            acc1_val += float(acc1[0] * images.size(0))\n",
        "\n",
        "            if print_freq >= 0 and i % print_freq == 0:\n",
        "                avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "                print(f'[Epoch {epoch + 1}][Eval][{i}] \\t Loss: {avg_loss:.4e} \\t Top-1 {avg_acc1:6.2f}')\n",
        "\n",
        "    avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "    total_mins = -1 if time_begin is None else (time() - time_begin) / 60\n",
        "    print(f'[Epoch {epoch + 1}] \\t \\t Top-1 {avg_acc1:6.2f} \\t \\t Time: {total_mins:.2f}')\n",
        "\n",
        "    loss_val_arr.append(avg_loss)\n",
        "\n",
        "    return loss_val_arr, avg_acc1\n",
        "\n",
        "\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    NLL loss with label smoothing.\n",
        "    \"\"\"\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        \"\"\"\n",
        "        Constructor for the LabelSmoothing module.\n",
        "        :param smoothing: label smoothing factor\n",
        "        \"\"\"\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        assert smoothing < 1.0\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1. - smoothing\n",
        "\n",
        "    def _compute_losses(self, x, target):\n",
        "        log_prob = F.log_softmax(x, dim=-1)\n",
        "        nll_loss = -log_prob.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -log_prob.mean(dim=-1)\n",
        "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        return self._compute_losses(x, target).mean()"
      ],
      "outputs": [],
      "metadata": {
        "id": "npCm0MjVWfjU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 406,
      "source": [
        "def test(model,data_loader,valid_ds):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        correct = 0\n",
        "        bs = batch_size\n",
        "        result = []\n",
        "        check_names = []\n",
        "        for i, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            arr = pred.data.cpu().numpy()\n",
        "            for j in range(pred.size()[0]):\n",
        "                file_name = valid_ds.samples[i*bs+j][0].split('/')[-1]\n",
        "                result.append((file_name,pred[j].cpu().numpy()[0])) \n",
        "        \n",
        "    return result"
      ],
      "outputs": [],
      "metadata": {
        "id": "_cfcCe-_qFgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 407,
      "source": [
        "#main.py \n",
        "\n",
        "\n",
        "loss_train_arr =[]\n",
        "loss_val_arr =[]\n",
        "\n",
        "img_size = DATASETS[dataset]['img_size']\n",
        "num_classes = DATASETS[dataset]['num_classes']\n",
        "img_mean, img_std = DATASETS[dataset]['mean'], DATASETS[dataset]['std']\n",
        "\n",
        "model = cct_6(img_size=img_size,\n",
        "                                    num_classes=num_classes,\n",
        "                                    positional_embedding=positional_embedding,\n",
        "                                    n_conv_layers=conv_layers,\n",
        "                                    kernel_size=conv_size,\n",
        "                                    patch_size=patch_size)\n",
        "#wrong\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
        "print(\"Loaded checkpoint.\")\n",
        "\n",
        "criterion = LabelSmoothingCrossEntropy()\n",
        "\n",
        "if (not no_cuda) and torch.cuda.is_available():\n",
        "    torch.cuda.set_device(gpu_id)\n",
        "    model.cuda(gpu_id)\n",
        "    criterion = criterion.cuda(gpu_id)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr,\n",
        "                                weight_decay=weight_decay)\n",
        "\n",
        "normalize = [transforms.Normalize(mean=img_mean, std=img_std)]\n",
        "\n",
        "augmentations = []\n",
        "if not disable_aug:\n",
        "    #from utils.autoaug import CIFAR10Policy\n",
        "    augmentations += [\n",
        "        CIFAR10Policy()\n",
        "    ]\n",
        "augmentations += [\n",
        "    transforms.RandomCrop(img_size, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    *normalize,\n",
        "]\n",
        "\n",
        "augmentations = transforms.Compose(augmentations)\n",
        "train_dataset = ImageFolder(\n",
        "    root=data_dir+\"/TRAIN\",   transform=augmentations)\n",
        "\n",
        "val_dataset = ImageFolder(\n",
        "    root=data_dir+\"/TEST\",   transform=transforms.Compose([\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        *normalize,\n",
        "    ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True,\n",
        "    num_workers=workers)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size, shuffle=False,\n",
        "    num_workers=workers)\n",
        "\n",
        "print(\"Beginning training\")\n",
        "time_begin = time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    loss_train_arr = cls_train(loss_train_arr, train_loader, model, criterion, optimizer, epoch)\n",
        "    loss_val_arr, acc1 = cls_validate(loss_val_arr, val_loader, model, criterion,  epoch=epoch, time_begin=time_begin)\n",
        "    best_acc1 = max(acc1, best_acc1)\n",
        "\n",
        "total_mins = (time() - time_begin) / 60\n",
        "print(f'Script finished in {total_mins:.2f} minutes, '\n",
        "        f'best top-1: {best_acc1:.2f}, '\n",
        "        f'final top-1: {acc1:.2f}')\n",
        "torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "result = test(model, val_loader, val_dataset)\n",
        "\n",
        "with open ('ID_result.csv','w') as f:\n",
        "    f.write('Id,Category\\n')\n",
        "    for data in result:\n",
        "        f.write(data[0]+','+str(data[1])+'\\n')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint.\n",
            "Beginning training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1][Train][0] \t Loss: 1.0724e+00 \t Top-1  93.75\n",
            "[Epoch 1][Train][10] \t Loss: 1.1324e+00 \t Top-1  90.48\n",
            "[Epoch 1][Train][20] \t Loss: 1.1260e+00 \t Top-1  91.11\n",
            "[Epoch 1][Train][30] \t Loss: 1.1276e+00 \t Top-1  91.00\n",
            "[Epoch 1][Train][40] \t Loss: 1.1283e+00 \t Top-1  91.12\n",
            "[Epoch 1][Train][50] \t Loss: 1.1264e+00 \t Top-1  91.12\n",
            "[Epoch 1][Train][60] \t Loss: 1.1248e+00 \t Top-1  91.06\n",
            "[Epoch 1][Train][70] \t Loss: 1.1270e+00 \t Top-1  91.01\n",
            "[Epoch 1][Train][80] \t Loss: 1.1300e+00 \t Top-1  90.87\n",
            "[Epoch 1][Train][90] \t Loss: 1.1283e+00 \t Top-1  90.89\n",
            "[Epoch 1][Train][100] \t Loss: 1.1257e+00 \t Top-1  90.91\n",
            "[Epoch 1][Train][110] \t Loss: 1.1265e+00 \t Top-1  90.91\n",
            "[Epoch 1][Train][120] \t Loss: 1.1245e+00 \t Top-1  90.97\n",
            "[Epoch 1][Train][130] \t Loss: 1.1232e+00 \t Top-1  91.02\n",
            "[Epoch 1][Train][140] \t Loss: 1.1217e+00 \t Top-1  91.07\n",
            "[Epoch 1][Train][150] \t Loss: 1.1222e+00 \t Top-1  91.12\n",
            "[Epoch 1][Train][160] \t Loss: 1.1213e+00 \t Top-1  91.17\n",
            "[Epoch 1][Train][170] \t Loss: 1.1222e+00 \t Top-1  91.14\n",
            "[Epoch 1][Train][180] \t Loss: 1.1226e+00 \t Top-1  91.14\n",
            "[Epoch 1][Train][190] \t Loss: 1.1209e+00 \t Top-1  91.21\n",
            "[Epoch 1][Train][200] \t Loss: 1.1190e+00 \t Top-1  91.27\n",
            "[Epoch 1][Train][210] \t Loss: 1.1188e+00 \t Top-1  91.28\n",
            "[Epoch 1][Train][220] \t Loss: 1.1175e+00 \t Top-1  91.32\n",
            "[Epoch 1][Train][230] \t Loss: 1.1170e+00 \t Top-1  91.33\n",
            "[Epoch 1][Train][240] \t Loss: 1.1161e+00 \t Top-1  91.37\n",
            "[Epoch 1][Train][250] \t Loss: 1.1169e+00 \t Top-1  91.32\n",
            "[Epoch 1][Train][260] \t Loss: 1.1169e+00 \t Top-1  91.33\n",
            "[Epoch 1][Train][270] \t Loss: 1.1171e+00 \t Top-1  91.31\n",
            "[Epoch 1][Train][280] \t Loss: 1.1172e+00 \t Top-1  91.32\n",
            "[Epoch 1][Train][290] \t Loss: 1.1169e+00 \t Top-1  91.34\n",
            "[Epoch 1][Train][300] \t Loss: 1.1163e+00 \t Top-1  91.36\n",
            "[Epoch 1][Train][310] \t Loss: 1.1169e+00 \t Top-1  91.33\n",
            "[Epoch 1][Train][320] \t Loss: 1.1180e+00 \t Top-1  91.30\n",
            "[Epoch 1][Train][330] \t Loss: 1.1174e+00 \t Top-1  91.31\n",
            "[Epoch 1][Train][340] \t Loss: 1.1174e+00 \t Top-1  91.31\n",
            "[Epoch 1][Train][350] \t Loss: 1.1168e+00 \t Top-1  91.33\n",
            "[Epoch 1][Train][360] \t Loss: 1.1164e+00 \t Top-1  91.31\n",
            "[Epoch 1][Train][370] \t Loss: 1.1174e+00 \t Top-1  91.28\n",
            "[Epoch 1][Train][380] \t Loss: 1.1171e+00 \t Top-1  91.29\n",
            "[Epoch 1][Train][390] \t Loss: 1.1173e+00 \t Top-1  91.27\n",
            "[Epoch 1][Eval][0] \t Loss: 1.0083e+00 \t Top-1  93.75\n",
            "[Epoch 1][Eval][10] \t Loss: 1.7284e+00 \t Top-1  71.52\n",
            "[Epoch 1][Eval][20] \t Loss: 1.6732e+00 \t Top-1  73.70\n",
            "[Epoch 1][Eval][30] \t Loss: 1.6955e+00 \t Top-1  72.88\n",
            "[Epoch 1][Eval][40] \t Loss: 1.7071e+00 \t Top-1  72.66\n",
            "[Epoch 1][Eval][50] \t Loss: 1.6917e+00 \t Top-1  73.10\n",
            "[Epoch 1][Eval][60] \t Loss: 1.6926e+00 \t Top-1  73.17\n",
            "[Epoch 1][Eval][70] \t Loss: 1.6777e+00 \t Top-1  73.56\n",
            "[Epoch 1] \t \t Top-1  73.41 \t \t Time: 1.53\n",
            "Script finished in 1.53 minutes, best top-1: 73.41, final top-1: 73.41\n"
          ]
        }
      ],
      "metadata": {
        "id": "j4ZPaXDFx3w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98a0225-96e8-40e1-89f8-169f6401280b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 408,
      "source": [
        "\n",
        "from torchsummary import summary\n",
        "# Print model\n",
        "print(model)\n",
        "\n",
        "# Print parameter\n",
        "size = summary(model, (3, 32, 32))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CCT(\n",
            "  (tokenizer): Tokenizer(\n",
            "    (conv_layers): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): ReLU()\n",
            "        (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): ReLU()\n",
            "        (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "    )\n",
            "    (flattener): Flatten(start_dim=2, end_dim=3)\n",
            "  )\n",
            "  (classifier): TransformerClassifier(\n",
            "    (attention_pool): Linear(in_features=256, out_features=1, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "    (blocks): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (fc): Linear(in_features=256, out_features=100, bias=True)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "              ReLU-2           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-3           [-1, 64, 16, 16]               0\n",
            "            Conv2d-4          [-1, 256, 16, 16]         147,456\n",
            "              ReLU-5          [-1, 256, 16, 16]               0\n",
            "         MaxPool2d-6            [-1, 256, 8, 8]               0\n",
            "           Flatten-7              [-1, 256, 64]               0\n",
            "         Tokenizer-8              [-1, 64, 256]               0\n",
            "           Dropout-9              [-1, 64, 256]               0\n",
            "        LayerNorm-10              [-1, 64, 256]             512\n",
            "           Linear-11              [-1, 64, 768]         196,608\n",
            "          Dropout-12            [-1, 4, 64, 64]               0\n",
            "           Linear-13              [-1, 64, 256]          65,792\n",
            "          Dropout-14              [-1, 64, 256]               0\n",
            "        Attention-15              [-1, 64, 256]               0\n",
            "         Identity-16              [-1, 64, 256]               0\n",
            "        LayerNorm-17              [-1, 64, 256]             512\n",
            "           Linear-18              [-1, 64, 512]         131,584\n",
            "          Dropout-19              [-1, 64, 512]               0\n",
            "           Linear-20              [-1, 64, 256]         131,328\n",
            "          Dropout-21              [-1, 64, 256]               0\n",
            "         Identity-22              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-23              [-1, 64, 256]               0\n",
            "        LayerNorm-24              [-1, 64, 256]             512\n",
            "           Linear-25              [-1, 64, 768]         196,608\n",
            "          Dropout-26            [-1, 4, 64, 64]               0\n",
            "           Linear-27              [-1, 64, 256]          65,792\n",
            "          Dropout-28              [-1, 64, 256]               0\n",
            "        Attention-29              [-1, 64, 256]               0\n",
            "         DropPath-30              [-1, 64, 256]               0\n",
            "        LayerNorm-31              [-1, 64, 256]             512\n",
            "           Linear-32              [-1, 64, 512]         131,584\n",
            "          Dropout-33              [-1, 64, 512]               0\n",
            "           Linear-34              [-1, 64, 256]         131,328\n",
            "          Dropout-35              [-1, 64, 256]               0\n",
            "         DropPath-36              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-37              [-1, 64, 256]               0\n",
            "        LayerNorm-38              [-1, 64, 256]             512\n",
            "           Linear-39              [-1, 64, 768]         196,608\n",
            "          Dropout-40            [-1, 4, 64, 64]               0\n",
            "           Linear-41              [-1, 64, 256]          65,792\n",
            "          Dropout-42              [-1, 64, 256]               0\n",
            "        Attention-43              [-1, 64, 256]               0\n",
            "         DropPath-44              [-1, 64, 256]               0\n",
            "        LayerNorm-45              [-1, 64, 256]             512\n",
            "           Linear-46              [-1, 64, 512]         131,584\n",
            "          Dropout-47              [-1, 64, 512]               0\n",
            "           Linear-48              [-1, 64, 256]         131,328\n",
            "          Dropout-49              [-1, 64, 256]               0\n",
            "         DropPath-50              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-51              [-1, 64, 256]               0\n",
            "        LayerNorm-52              [-1, 64, 256]             512\n",
            "           Linear-53              [-1, 64, 768]         196,608\n",
            "          Dropout-54            [-1, 4, 64, 64]               0\n",
            "           Linear-55              [-1, 64, 256]          65,792\n",
            "          Dropout-56              [-1, 64, 256]               0\n",
            "        Attention-57              [-1, 64, 256]               0\n",
            "         DropPath-58              [-1, 64, 256]               0\n",
            "        LayerNorm-59              [-1, 64, 256]             512\n",
            "           Linear-60              [-1, 64, 512]         131,584\n",
            "          Dropout-61              [-1, 64, 512]               0\n",
            "           Linear-62              [-1, 64, 256]         131,328\n",
            "          Dropout-63              [-1, 64, 256]               0\n",
            "         DropPath-64              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-65              [-1, 64, 256]               0\n",
            "        LayerNorm-66              [-1, 64, 256]             512\n",
            "           Linear-67              [-1, 64, 768]         196,608\n",
            "          Dropout-68            [-1, 4, 64, 64]               0\n",
            "           Linear-69              [-1, 64, 256]          65,792\n",
            "          Dropout-70              [-1, 64, 256]               0\n",
            "        Attention-71              [-1, 64, 256]               0\n",
            "         DropPath-72              [-1, 64, 256]               0\n",
            "        LayerNorm-73              [-1, 64, 256]             512\n",
            "           Linear-74              [-1, 64, 512]         131,584\n",
            "          Dropout-75              [-1, 64, 512]               0\n",
            "           Linear-76              [-1, 64, 256]         131,328\n",
            "          Dropout-77              [-1, 64, 256]               0\n",
            "         DropPath-78              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-79              [-1, 64, 256]               0\n",
            "        LayerNorm-80              [-1, 64, 256]             512\n",
            "           Linear-81              [-1, 64, 768]         196,608\n",
            "          Dropout-82            [-1, 4, 64, 64]               0\n",
            "           Linear-83              [-1, 64, 256]          65,792\n",
            "          Dropout-84              [-1, 64, 256]               0\n",
            "        Attention-85              [-1, 64, 256]               0\n",
            "         DropPath-86              [-1, 64, 256]               0\n",
            "        LayerNorm-87              [-1, 64, 256]             512\n",
            "           Linear-88              [-1, 64, 512]         131,584\n",
            "          Dropout-89              [-1, 64, 512]               0\n",
            "           Linear-90              [-1, 64, 256]         131,328\n",
            "          Dropout-91              [-1, 64, 256]               0\n",
            "         DropPath-92              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-93              [-1, 64, 256]               0\n",
            "        LayerNorm-94              [-1, 64, 256]             512\n",
            "           Linear-95                [-1, 64, 1]             257\n",
            "           Linear-96                  [-1, 100]          25,700\n",
            "TransformerClassifier-97                  [-1, 100]               0\n",
            "================================================================\n",
            "Total params: 3,333,669\n",
            "Trainable params: 3,333,669\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 16.25\n",
            "Params size (MB): 12.72\n",
            "Estimated Total Size (MB): 28.98\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt6ogGMRWlzY",
        "outputId": "deb58682-c33d-4d49-efd6-3556679fc985"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 409,
      "source": [
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n",
        "\n",
        "\n",
        "plt.plot([x for x in loss_train_arr], \"-bx\")\n",
        "plt.plot([x for x in loss_val_arr],\"-rx\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"train loss\",\"val loss\"])\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f7fc5636790>"
            ]
          },
          "metadata": {},
          "execution_count": 409
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEHCAYAAACjh0HiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcFUlEQVR4nO3de1BU5/0G8AcBL4got1Xj2gLeXS6rrqmXuEhQ460WvFMzulIlxhiTtvEybYmS2LGKE61pKt0WM2qtl2gVjQ5pE0FjayMbClGMiVFRIWp2UfCGzq6+vz8o/CTCurCcXZb3+cwwy9nznrPfd3XOwznv2Xe9hBACREQkrVbuLoCIiNyLQUBEJDkGARGR5BgERESSYxAQEUmOQUBEJDkfpXacnJyMDz/8ECqVCqdPn35ifXp6OrZv3w4AsNls+PLLL2E2mxEUFGR3vyEhIQgLC1OiZCKiFqu4uBgWi6XOdV5KfY7g2LFj8Pf3x+zZs+sMgscdPHgQ69evx5EjR566X51OB5PJ1FRlEhFJwd6xU7FLQ3q9/ql/3VfbsWMHkpKSlCqFiIjscPsYwb1795CdnY0pU6a4uxQiIikpNkbgqIMHD2L48OF2zx6MRiOMRiMAwGw2u6o0IiIpuD0Idu7c+dTLQikpKUhJSQFQdZ2LiFomq9WKkpIS3L9/392leKy2bdtCrVbD19fX4W3cGgQVFRU4evQo/vrXv7qzDCJqJkpKStChQweEhYXBy8vL3eV4HCEEysrKUFJSgvDwcIe3U2yMICkpCUOHDsVXX30FtVqNzMxMZGRkICMjo6bNvn37MGbMGLRv316pMoiUt3YtkJNT+7mcnKrnqUHu37+P4OBghkAjeXl5ITg4uMFnVIqdEezYseOpbQwGAwwGg1IlELnG4MHA9OnA7t1AXFxVCFQvU4MxBJzTmPfP7XcNEXm8uLiqg/706cCbb9YOBfIo5eXl+OMf/9iobcePH4/y8nKH269cuRLr1q1r1Gs1NQYBUVOIiwNefhl4++2qR4aAR7IXBDabze62hw8fRqdOnZQoS3EMAqKmkJMDbNoEpKZWPX5/zICanBJDM8uXL8f58+eh1WqxZMkS5ObmYsSIEZg0aRL69+8PAEhISMCgQYOg0WhqbmsHgLCwMFgsFhQXF6Nfv36YP38+NBoNxowZg8rKSruvW1BQgCFDhiA6OhqJiYm4efMmAGDjxo3o378/oqOjMXPmTADA0aNHodVqodVqMWDAANy+fbvxHa4mPMygQYPcXQJRbUeOCBESUvVY1zI57MyZMw63VeJtv3jxotBoNDXLOTk5ws/PT1y4cKHmubKyMiGEEPfu3RMajUZYLBYhhBA//OEPhdlsFhcvXhTe3t7iv//9rxBCiGnTpolt27Y98VorVqwQ6enpQgghoqKiRG5urhBCiNTUVPHaa68JIYTo2rWruH//vhBCiJs3bwohhJg4caI4fvy4EEKI27dvC6vV+sS+63of7R073f45AiKPl5dXe0ygeswgL4+XiJzw+utAQYH9Ns88A7zwAtC1K3D1KtCvH5CWVvVTF60W2LChYXU8++yztW7F3LhxI/bt2wcAuHLlCs6dO4fg4OBa24SHh0Or1QIABg0ahOLi4nr3X1FRgfLycsTGxgIA5syZg2nTpgEAoqOjMWvWLCQkJCAhIQEAMHz4cPziF7/ArFmzMHnyZKjV6oZ1qA68NETkrKVLnzzgx8VVPU+KCgysCoHLl6seAwOb/jUev709NzcXH3/8MU6cOIHCwkIMGDCgzls127RpU/O7t7f3U8cX6nPo0CG88soryM/Px+DBg2Gz2bB8+XL85S9/QWVlJYYPH46zZ882at+P4xkBETVLjvzlXn2nbvXQzIoVzp2EdejQwe4194qKCgQGBsLPzw9nz57Ff/7zn8a/2P907NgRgYGB+PTTTzFixAhs27YNsbGxePToEa5cuYK4uDg899xz2LlzJ+7cuYOysjJERUUhKioKeXl5OHv2LPr27etUDQwCIvJIj39cIy6u6sfZO3eDg4MxfPhwREZGYty4cZgwYUKt9WPHjkVGRgb69euHPn36YMiQIU3QE2DLli1YsGAB7t27h4iICLz//vt4+PAhXnzxRVRUVEAIgcWLF6NTp05ITU1FTk4OWrVqBY1Gg3Hjxjn9+op9H4FS+H0ERC3Xl19+iX79+jnUdu3aqs/yPX7Qz8mpGpqR/apcXe+jvWMnzwiIyCPVdbCvPjOghuFgMRGR5BgERESSYxAQEUmOQUBEJDkGARGR5BgERERO8Pf3b9DzzRGDgIhIcgwCIvJMCsxDvXz5crz33ns1y9VfHnPnzh3Ex8dj4MCBiIqKQlZWlsP7FEJgyZIliIyMRFRUFHbt2gUAuHr1KvR6PbRaLSIjI/Hpp5/i4cOHMBgMNW3Xr1/f6L40SL3zkjZTnIaaqOVqyDTUSsxDnZ+fL/R6fc1yv379xOXLl4XVahUVFRVCCCHMZrPo0aOHePTokRBCiPbt29e5r+rn9+zZI0aNGiVsNpu4du2a6N69u/j222/FunXrxKpVq4QQQthsNnHr1i1hMpnEqFGjavZRPfV0Q3EaaiJqGdwwD/WAAQPw3Xff4dtvv4XZbEZgYCC6d+8Oq9WKX/3qVzh27BhatWqF0tJSXL9+HV26dHlqN44fP46kpCR4e3ujc+fOiI2NRV5eHgYPHozk5GRYrVYkJCRAq9UiIiICFy5cwKuvvooJEyZgzJgxT91/U+ClISLyXArMQz1t2jTs2bMHu3btwowZMwAA27dvh9lsxueff46CggJ07ty5zumnG0Kv1+PYsWPo1q0bDAYDtm7disDAQBQWFmLkyJHIyMjAvHnznO6PI3hGQETNkzvmoQYwY8YMzJ8/HxaLBUePHgVQNf20SqWCr68vcnJycOnSJYf3N2LECPzpT3/CnDlzcOPGDRw7dgzp6em4dOkS1Go15s+fjwcPHiA/Px/jx49H69atMWXKFPTp0wcvvviiU31xFIOAiDyTEvNQA9BoNLh9+za6deuGrl27AgBmzZqFH//4x4iKioJOp2vQ/P+JiYk4ceIEYmJi4OXlhbVr16JLly7YsmUL0tPT4evrC39/f2zduhWlpaWYO3cuHj16BABYvXp1o/vREJyGmoiajYZMQ815qOvHaaiJSA6ch7rJcLCYiEhyigVBcnIyVCoVIiMj622Tm5sLrVYLjUaD2NhYpUohIiI7FAsCg8GA7OzseteXl5dj4cKFOHDgAIqKivDBBx8oVQoReRAPG7Zsdhrz/ikWBHq9HkFBQfWu/9vf/obJkyfjBz/4AQBApVIpVQoReYi2bduirKyMYdBIQgiUlZWhbdu2DdrObYPFX3/9NaxWK0aOHInbt2/jtddew+zZs91VDhE1A2q1GiUlJTCbze4uxWO1bdsWarW6Qdu4LQhsNhs+//xzfPLJJ6isrMTQoUMxZMgQ9O7d+4m2RqMRRqMRAPgfhKgF8/X1RXh4uLvLkI7b7hpSq9V44YUX0L59e4SEhECv16OwsLDOtikpKTCZTDCZTAgNDXVxpURELZvbguAnP/kJjh8/DpvNhnv37uGzzz5z/IMkRETUZBS7NJSUlITc3FxYLBao1WqkpaXBarUCABYsWIB+/fph7NixiI6ORqtWrTBv3jy7t5oSEZEyOMUEEZEE7B07+cliIiLJMQiIiCTHICAikhyDgIhIcgwCIiLJMQiIiCTHICAikhyDgIhIcgwCIiLJMQiIiCTHICAikhyDgIhIcgwCIiLJMQiIiCTHICAikhyDgIhIcgwCIiLJMQiIiCTHICAikhyDgIhIcgwCIiLJMQiIiCTHICAikhyDgIhIcgwCIiLJMQiIiCSnWBAkJydDpVIhMjKyzvW5ubno2LEjtFottFot3nrrLaVKISIiO3yU2rHBYMCiRYswe/bsetuMGDECH374oVIlEBGRAxQ7I9Dr9QgKClJq90RE1ETcOkZw4sQJxMTEYNy4cSgqKnJnKURE0lLs0tDTDBw4EJcuXYK/vz8OHz6MhIQEnDt3rs62RqMRRqMRAGA2m11ZJhFRi+e2M4KAgAD4+/sDAMaPHw+r1QqLxVJn25SUFJhMJphMJoSGhrqyTCKiFs9tQXDt2jUIIQAAJ0+exKNHjxAcHOyucoiIpKXYpaGkpCTk5ubCYrFArVYjLS0NVqsVALBgwQLs2bMHmzZtgo+PD9q1a4edO3fCy8tLqXKIiKgeXqL6z3IPodPpYDKZ3F0GEZFHsXfs5CeLiYgkxyAgIpIcg4CISHIMAiIiyTEIiIgkxyAgIpIcg4CISHIMAiIiyTEIiIgkxyAgIpIcg4CISHIMAiIiyTEIiIgkxyAgIpIcg4CISHIMAiIiyTEIiIgkxyAgIpIcg4CISHIMAiIiyTEIiIgk51AQ3L17F48ePQIAfP311zhw4ACsVquihRERkWs4FAR6vR73799HaWkpxowZg23btsFgMChcGhERuYJDQSCEgJ+fH/7+979j4cKF+OCDD1BUVKR0bURE5AIOB8GJEyewfft2TJgwAQDw8OFDRQsjIiLXcCgINmzYgNWrVyMxMREajQYXLlxAXFyc0rUREZELOBQEsbGxOHDgAJYtW4ZHjx4hJCQEGzdutLtNcnIyVCoVIiMj7bbLy8uDj48P9uzZ43jVRETUZBwKgp/+9Ke4desW7t69i8jISPTv3x/p6el2tzEYDMjOzrbb5uHDh1i2bBnGjBnjeMVERNSkHAqCM2fOICAgAPv378e4ceNw8eJFbNu2ze42er0eQUFBdtu8++67mDJlClQqleMVExFRk3IoCKxWK6xWK/bv349JkybB19cXXl5eTr1waWkp9u3bh5dfftmp/RARkXMcCoKXXnoJYWFhuHv3LvR6PS5duoSAgACnXvj111/HmjVr0KrV00swGo3Q6XTQ6XQwm81OvS4REdXmJYQQjdnQZrPBx8fHbpvi4mJMnDgRp0+ffmJdeHg4ql/aYrHAz88PRqMRCQkJdvep0+lgMpkaUzIRkbTsHTvtH8n/p6KiAmlpaTh27BiAqruI3nzzTXTs2LHRRV28eLHmd4PBgIkTJz41BIiIqOk5dGkoOTkZHTp0wO7du7F7924EBARg7ty5drdJSkrC0KFD8dVXX0GtViMzMxMZGRnIyMhoksKJiKhpOHRpSKvVoqCg4KnPuQIvDRERNZy9Y6dDZwTt2rXD8ePHa5b/9a9/oV27dk1THRERuZVDYwQZGRmYPXs2KioqAACBgYHYsmWLooUREZFrOBQEMTExKCwsxK1btwAAAQEB2LBhA6KjoxUtjoiIlNegbygLCAio+fzAO++8o0hBRETkWo3+qspGfvyAiIiamUYHgbNTTBARUfNgd4ygQ4cOdR7whRCorKxUrCgiInIdu0Fw+/ZtV9VBRERu0uhLQ0RE1DIwCIiIJMcgICKSHIOAiEhyDAIiIskxCIiIJMcgICKSHIOAiEhyDAIiIskxCIiIJMcgICKSHIOAiEhyDAIiIskxCIiIJMcgICKSHIOAiEhyDAIiIskxCIiIJKdYECQnJ0OlUiEyMrLO9VlZWYiOjoZWq4VOp8Px48eVKoWIiOxQLAgMBgOys7PrXR8fH4/CwkIUFBRg8+bNmDdvnlKlEBGRHYoFgV6vR1BQUL3r/f394eXlBQC4e/duze9ERORabh0j2LdvH/r27YsJEyZg8+bN7iyFiEhabg2CxMREnD17Fvv370dqamq97YxGI3Q6HXQ6HcxmswsrJCJq+ZrFXUN6vR4XLlyAxWKpc31KSgpMJhNMJhNCQ0NdXB0RUcvmtiD45ptvIIQAAOTn5+PBgwcIDg52VzlERNLyUWrHSUlJyM3NhcVigVqtRlpaGqxWKwBgwYIF2Lt3L7Zu3QpfX1+0a9cOu3bt4oAxEZEbeInqP8s9hE6ng8lkcncZREQexd6xs1mMERARkfswCIiIJMcgICKSHIOAiEhyDAIiIskxCIiIJMcgICKSHIOAiEhyDAIiIskxCIiIJMcgICKSHIOAiEhyDAIiIskxCIiIJMcgICKSHIOAiEhyDAIiIskxCIiIJMcgICKSHIOAiEhyDAIiIskxCIiIJMcgICKSHIOAiEhyDAIiIskxCIiIJKdYECQnJ0OlUiEyMrLO9du3b0d0dDSioqIwbNgwFBYWKlUKERHZoVgQGAwGZGdn17s+PDwcR48exalTp5CamoqUlBSlSiEiIjt8lNqxXq9HcXFxveuHDRtW8/uQIUNQUlKiVClERGRHsxgjyMzMxLhx49xdBhGRlBQ7I3BUTk4OMjMzcfz48XrbGI1GGI1GAIDZbHZVaUREUnDrGcEXX3yBefPmISsrC8HBwfW2S0lJgclkgslkQmhoqAsrJCJq+dwWBJcvX8bkyZOxbds29O7d211lEBFJT7FLQ0lJScjNzYXFYoFarUZaWhqsVisAYMGCBXjrrbdQVlaGhQsXVhXi4wOTyaRUOUREVA8vIYRwdxENodPpGBhERA1k79jZLO4aIiIi92EQEBFJjkFARCQ5BgERkeQYBEREkmMQEBFJjkFARCQ5BgERkeQYBEREkmMQEBFJjkFARCQ5BgERkeQYBEREkmMQEBFJjkFARCQ5BgERkeQYBEREkmMQEBFJjkFARCQ5BgERkeQYBEREkmMQEDlp7VogJ6f2czk5Vc8TeQIGAZGTBg8Gpk///zDIyalaHjzYvXUROcrH3QUQebq4OGD37qqD/8svA5s2VS3Hxbm7MiLH8IyAqAnExVWFwNtvVz0yBMiTMAiImkBOTtWZQGpq1eP3xwyImjPFgiA5ORkqlQqRkZF1rj979iyGDh2KNm3aYN26dUqVQaS46jGB3buBt976/8tEDAPyFIoFgcFgQHZ2dr3rg4KCsHHjRrzxxhtKlUDkEnl5tccEqscM8vLcWxeRoxQbLNbr9SguLq53vUqlgkqlwqFDh5Qqgcglli598rm4OI4TkOfgGAERkeQ84vZRo9EIo9EIADCbzW6uhoioZfGIM4KUlBSYTCaYTCaEhoa6uxwiohbFI4KAiIiUo9iloaSkJOTm5sJisUCtViMtLQ1WqxUAsGDBAly7dg06nQ63bt1Cq1atsGHDBpw5cwYBAQFKlURERHXwEkIIdxfRECEhIQgLC3N3GQ1mNpulu6zFPrd8svUX8Nw+FxcXw2Kx1LnO44LAU+l0OphMJneX4VLsc8snW3+BltlnjhEQEUmOQUBEJDnvlStXrnR3EbIYNGiQu0twOfa55ZOtv0DL6zPHCIiIJMdLQ0REkmMQNKEbN25g9OjR6NWrF0aPHo2bN2/W2W7Lli3o1asXevXqhS1btjyxftKkSfVO393cONPne/fuYcKECejbty80Gg2WL1/uytIbJDs7G3369EHPnj3xu9/97on1Dx48wIwZM9CzZ0/86Ec/qjXh4urVq9GzZ0/06dMHH330kQurdk5j+/zPf/4TgwYNQlRUFAYNGoQjR464uPLGc+bfGQAuX74Mf39/z5taX1CTWbJkiVi9erUQQojVq1eLpUuXPtGmrKxMhIeHi7KyMnHjxg0RHh4ubty4UbN+7969IikpSWg0GpfV7Qxn+nz37l1x5MgRIYQQDx48EM8995w4fPiwS+t3hM1mExEREeL8+fPiwYMHIjo6WhQVFdVq895774mXXnpJCCHEjh07xPTp04UQQhQVFYno6Ghx//59ceHCBRERESFsNpvL+9BQzvQ5Pz9flJaWCiGEOHXqlHjmmWdcW3wjOdPnalOmTBFTp04V6enpLqu7KfCMoAllZWVhzpw5AIA5c+Zg//79T7T56KOPMHr0aAQFBSEwMBCjR4+u+d6GO3fu4J133sFvfvMbl9btDGf67Ofnh7j/zdXcunVrDBw4ECUlJS6t3xEnT55Ez549ERERgdatW2PmzJnIysqq1ebx92Hq1Kn45JNPIIRAVlYWZs6ciTZt2iA8PBw9e/bEyZMn3dGNBnGmzwMGDMAzzzwDANBoNKisrMSDBw9c3oeGcqbPALB//36Eh4dDo9G4vHZnMQia0PXr19G1a1cAQJcuXXD9+vUn2pSWlqJ79+41y2q1GqWlpQCA1NRU/PKXv4Sfn59rCm4Czva5Wnl5OQ4ePIj4+HhlC24ER+p/vI2Pjw86duyIsrIyh7Ztjpzp8+P27t2LgQMHok2bNsoX7SRn+nznzh2sWbMGK1ascGnNTcUjpqFuTkaNGoVr16498fxvf/vbWsteXl7w8vJyeL8FBQU4f/481q9fb/cLfdxBqT5Xs9lsSEpKwuLFixEREdHoOql5KSoqwrJly/CPf/zD3aUobuXKlfj5z38Of39/d5fSKAyCBvr444/rXde5c2dcvXoVXbt2xdWrV6FSqZ5o061bN+Tm5tYsl5SUYOTIkThx4gRMJhPCwsJgs9nw3XffYeTIkbXauotSfa6WkpKCXr164fXXX2/KsptMt27dcOXKlZrlkpISdOvWrc42arUaNpsNFRUVCA4Odmjb5siZPle3T0xMxNatW9GjRw+X1t5YzvT5s88+w549e7B06VKUl5ejVatWaNu2LRYtWuTqbjSOm8coWpQ33nij1sDpkiVLnmhTVlYmwsLCxI0bN8SNGzdEWFiYKCsrq9Xm4sWLHjNY7Gyff/3rX4vJkyeLhw8furTuhrBarSI8PFxcuHChZhDx9OnTtdr84Q9/qDWIOG3aNCGEEKdPn641WBweHu4Rg8XO9PnmzZsiOjpa7N271+V1O8OZPj9uxYoVHjdYzCBoQhaLRTz//POiZ8+eIj4+vuZgl5eXJ372s5/VtMvMzBQ9evQQPXr0EJs3b35iP54UBM70+cqVKwKA6Nu3r4iJiRExMTHiz3/+s1v68TSHDh0SvXr1EhEREWLVqlVCCCFSU1NFVlaWEEKIyspKMXXqVNGjRw8xePBgcf78+ZptV61aJSIiIkTv3r2b5V1R9Wlsn99++23h5+dX828aExMjrl+/7rZ+NIQz/87VPDEI+MliIiLJ8a4hIiLJMQiIiCTHICAikhyDgIhIcgwCIiLJMQiIvsfb2xtarbbmp65ZKBuruLjYY2aWJXnwk8VE39OuXTsUFBS4uwwil+EZAZGDwsLCsHTpUkRFReHZZ5/FN998A6Dqr/znn38e0dHRiI+Px+XLlwFUTciXmJiImJgYxMTE4N///jcA4OHDh5g/fz40Gg3GjBmDyspKt/WJCGAQED2hsrKy1qWhXbt21azr2LEjTp06hUWLFtXMjfTqq69izpw5+OKLLzBr1iwsXrwYALB48WLExsaisLAQ+fn5NdMTnzt3Dq+88gqKiorQqVMn7N271/WdJHoMP1lM9D3+/v64c+fOE8+HhYXhyJEjiIiIgNVqRZcuXVBWVoaQkBBcvXoVvr6+sFqt6Nq1KywWC0JDQ1FSUlJrCubi4mKMHj0a586dAwCsWbMGVqvVo76DgloenhEQNcDj02w3ZsptALWCwdvbGzabzem6iJzBICBqgOrLRLt27cLQoUMBAMOGDcPOnTsBANu3b8eIESMAAPHx8di0aROAqnGBiooKN1RM9HS8a4joe6rHCKqNHTu25hbSmzdvIjo6Gm3atMGOHTsAAO+++y7mzp2L9PR0hIaG4v333wcA/P73v0dKSgoyMzPh7e2NTZs21XybG1FzwjECIgeFhYXBZDIhJCTE3aUQNSleGiIikhzPCIiIJMczAiIiyTEIiIgkxyAgIpIcg4CISHIMAiIiyTEIiIgk93+Skn4sYORgqQAAAABJRU5ErkJggg=="
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "BaEQExMDWoEV",
        "outputId": "7d0ebb8e-9513-4f8a-a3cf-9a792ef5ca67"
      }
    }
  ]
}