{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CCT 6-3x1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57692838La4I",
        "outputId": "5f84ba06-1313-41e5-a461-4fa23983513a"
      },
      "source": [
        "!pip install torchtoolbox"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtoolbox in /usr/local/lib/python3.7/dist-packages (0.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.62.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (3.0.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.99)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torchtoolbox) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhSQ4zfq0CfM"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Module, ModuleList, Linear, Dropout, LayerNorm, Identity, Parameter, init\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.datasets import ImageFolder, CIFAR100\n",
        "import random\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV0WT80KVVms",
        "outputId": "df116621-1b29-4c0d-eb53-3153cb6aeebe"
      },
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "\n",
        "\n",
        "od.download('https://www.kaggle.com/c/2021-ai-training-final-project/data')\n",
        "data_dir = './2021-ai-training-final-project/CIFAR100'"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.7/dist-packages (0.1.20)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.62.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.5.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (5.0.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Skipping, found downloaded files in \"./2021-ai-training-final-project\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxFa3TYR3oA8",
        "outputId": "65cefab0-3e23-48f0-a181-57c2357ef67c"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "device = get_default_device()\n",
        "print(device)\n",
        "!nvidia-smi"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Sun Sep  5 07:34:52 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    32W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Tnuo9vqENF"
      },
      "source": [
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcGLeXU7te0B"
      },
      "source": [
        "\n",
        "class CCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 embedding_dim=768,\n",
        "                 n_input_channels=3,\n",
        "                 n_conv_layers=1,\n",
        "                 kernel_size=7,\n",
        "                 stride=2,\n",
        "                 padding=3,\n",
        "                 pooling_kernel_size=3,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(CCT, self).__init__()\n",
        "\n",
        "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
        "                                   n_output_channels=embedding_dim,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   stride=stride,\n",
        "                                   padding=padding,\n",
        "                                   pooling_kernel_size=pooling_kernel_size,\n",
        "                                   pooling_stride=pooling_stride,\n",
        "                                   pooling_padding=pooling_padding,\n",
        "                                   max_pool=True,\n",
        "                                   activation=nn.ReLU,\n",
        "                                   n_conv_layers=n_conv_layers,\n",
        "                                   conv_bias=False)\n",
        "\n",
        "        self.classifier = TransformerClassifier(\n",
        "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
        "                                                           height=img_size,\n",
        "                                                           width=img_size),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout_rate=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenizer(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "         kernel_size=3, stride=None, padding=None,\n",
        "         *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "    return CCT(num_layers=num_layers,\n",
        "               num_heads=num_heads,\n",
        "               mlp_ratio=mlp_ratio,\n",
        "               embedding_dim=embedding_dim,\n",
        "               kernel_size=kernel_size,\n",
        "               stride=stride,\n",
        "               padding=padding,\n",
        "               *args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def cct_6(*args, **kwargs):\n",
        "    return _cct(num_layers=6, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11WxM9ycvfy-"
      },
      "source": [
        "\n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 n_conv_layers=1,\n",
        "                 n_input_channels=3,\n",
        "                 n_output_channels=64,\n",
        "                 in_planes=64,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 conv_bias=False):\n",
        "        super(Tokenizer, self).__init__()\n",
        "\n",
        "        n_filter_list = [n_input_channels] + \\\n",
        "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
        "                        [n_output_channels]\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
        "                          kernel_size=(kernel_size, kernel_size),\n",
        "                          stride=(stride, stride),\n",
        "                          padding=(padding, padding), bias=conv_bias),\n",
        "                nn.Identity() if activation is None else activation(),\n",
        "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
        "                             stride=pooling_stride,\n",
        "                             padding=pooling_padding) if max_pool else nn.Identity()\n",
        "            )\n",
        "                for i in range(n_conv_layers)\n",
        "            ])\n",
        "\n",
        "        self.flattener = nn.Flatten(2, 3)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
        "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.flattener(self.conv_layers(x)).transpose(-2, -1)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i67i7ugxvSeM"
      },
      "source": [
        "\n",
        "class Attention(Module):\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n",
        "                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 sequence_length=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            sequence_length += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                    dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        return pe.unsqueeze(0)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNOUNvqT7S3g"
      },
      "source": [
        "\n",
        "class ShearX(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n",
        "            Image.BICUBIC, fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class ShearY(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n",
        "            Image.BICUBIC, fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class TranslateX(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, magnitude * x.size[0] * random.choice([-1, 1]), 0, 1, 0),\n",
        "            fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class TranslateY(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * x.size[1] * random.choice([-1, 1])),\n",
        "            fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class Rotate(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        rot = x.convert(\"RGBA\").rotate(magnitude)\n",
        "        return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(x.mode)\n",
        "\n",
        "\n",
        "class Color(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Color(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Posterize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.posterize(x, magnitude)\n",
        "\n",
        "\n",
        "class Solarize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.solarize(x, magnitude)\n",
        "\n",
        "\n",
        "class Contrast(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Contrast(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Sharpness(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Sharpness(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Brightness(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Brightness(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class AutoContrast(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.autocontrast(x)\n",
        "\n",
        "\n",
        "class Equalize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.equalize(x)\n",
        "\n",
        "\n",
        "class Invert(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.invert(x)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6L2he3-60fH"
      },
      "source": [
        "\n",
        "class CIFAR10Policy(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n",
        "            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n",
        "            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n",
        "            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n",
        "            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n",
        "            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n",
        "\n",
        "            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n",
        "            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n",
        "            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n",
        "            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n",
        "            SubPolicy(0.2, \"equalize\", 8, 0.6, \"equalize\", 4, fillcolor),\n",
        "            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n",
        "            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n",
        "            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n",
        "            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment CIFAR10 Policy\"\n",
        "\n",
        "\n",
        "class SubPolicy(object):\n",
        "    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n",
        "        ranges = {\n",
        "            \"shearX\": np.linspace(0, 0.3, 10),\n",
        "            \"shearY\": np.linspace(0, 0.3, 10),\n",
        "            \"translateX\": np.linspace(0, 150 / 331, 10),\n",
        "            \"translateY\": np.linspace(0, 150 / 331, 10),\n",
        "            \"rotate\": np.linspace(0, 30, 10),\n",
        "            \"color\": np.linspace(0.0, 0.9, 10),\n",
        "            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n",
        "            \"solarize\": np.linspace(256, 0, 10),\n",
        "            \"contrast\": np.linspace(0.0, 0.9, 10),\n",
        "            \"sharpness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"brightness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"autocontrast\": [0] * 10,\n",
        "            \"equalize\": [0] * 10,\n",
        "            \"invert\": [0] * 10\n",
        "        }\n",
        "\n",
        "        func = {\n",
        "            \"shearX\": ShearX(fillcolor=fillcolor),\n",
        "            \"shearY\": ShearY(fillcolor=fillcolor),\n",
        "            \"translateX\": TranslateX(fillcolor=fillcolor),\n",
        "            \"translateY\": TranslateY(fillcolor=fillcolor),\n",
        "            \"rotate\": Rotate(),\n",
        "            \"color\": Color(),\n",
        "            \"posterize\": Posterize(),\n",
        "            \"solarize\": Solarize(),\n",
        "            \"contrast\": Contrast(),\n",
        "            \"sharpness\": Sharpness(),\n",
        "            \"brightness\": Brightness(),\n",
        "            \"autocontrast\": AutoContrast(),\n",
        "            \"equalize\": Equalize(),\n",
        "            \"invert\": Invert()\n",
        "        }\n",
        "\n",
        "        self.p1 = p1\n",
        "        self.operation1 = func[operation1]\n",
        "        self.magnitude1 = ranges[operation1][magnitude_idx1]\n",
        "        self.p2 = p2\n",
        "        self.operation2 = func[operation2]\n",
        "        self.magnitude2 = ranges[operation2][magnitude_idx2]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p1:\n",
        "            img = self.operation1(img, self.magnitude1)\n",
        "        if random.random() < self.p2:\n",
        "            img = self.operation2(img, self.magnitude2)\n",
        "        return img"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR3mRPNHv1iY"
      },
      "source": [
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  \n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  \n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npCm0MjVWfjU"
      },
      "source": [
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lrr = lr\n",
        "    if epoch < warmup:\n",
        "        lrr = lr / (warmup - epoch)\n",
        "    else:\n",
        "        lrr *= 0.5 * (1. + math.cos(math.pi * (epoch - warmup) / (total_epochs - warmup)))\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lrr\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "\n",
        "\n",
        "def accuracy(output, target):\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(1, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        correct_k = correct[:1].flatten().float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def train(loss_train_arr, train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    loss_val, acc1_val = 0, 0\n",
        "    n = 0\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda(gpu_id, non_blocking=True)\n",
        "            target = target.cuda(gpu_id, non_blocking=True)\n",
        "        output = model(images)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        acc1 = accuracy(output, target)\n",
        "        n += images.size(0)\n",
        "        loss_val += float(loss.item() * images.size(0))\n",
        "        acc1_val += float(acc1[0] * images.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        if clip_grad_norm > 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm, norm_type=2)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        \n",
        "    avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "\n",
        "    print(f'Train: Epoch [{epoch + 1}], Loss: {avg_loss:.4e}, acc: {avg_acc1:6.2f}')\n",
        "    \n",
        "    loss_train_arr.append(avg_loss)\n",
        "    \n",
        "    return loss_train_arr\n",
        "\n",
        "\n",
        "def validate(loss_val_arr, val_loader, model, criterion,  epoch=None):\n",
        "    model.eval()\n",
        "    loss_val, acc1_val = 0, 0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda(gpu_id, non_blocking=True)\n",
        "                target = target.cuda(gpu_id, non_blocking=True)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc1 = accuracy(output, target)\n",
        "            n += images.size(0)\n",
        "            loss_val += float(loss.item() * images.size(0))\n",
        "            acc1_val += float(acc1[0] * images.size(0))\n",
        "\n",
        "\n",
        "    avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "   \n",
        "    print(f'Eval: Epoch [{epoch + 1}], Loss: {avg_loss:.4e},  acc: {avg_acc1:6.2f} ')\n",
        "\n",
        "    loss_val_arr.append(avg_loss)\n",
        "\n",
        "    return loss_val_arr, avg_acc1\n",
        "\n",
        "\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        assert smoothing < 1.0\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1. - smoothing\n",
        "\n",
        "    def _compute_losses(self, x, target):\n",
        "        log_prob = F.log_softmax(x, dim=-1)\n",
        "        nll_loss = -log_prob.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -log_prob.mean(dim=-1)\n",
        "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        return self._compute_losses(x, target).mean()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cfcCe-_qFgb"
      },
      "source": [
        "def test(model,data_loader,valid_ds):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        correct = 0\n",
        "        bs = batch_size\n",
        "        result = []\n",
        "        check_names = []\n",
        "        for i, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            arr = pred.data.cpu().numpy()\n",
        "            for j in range(pred.size()[0]):\n",
        "                file_name = valid_ds.samples[i*bs+j][0].split('/')[-1]\n",
        "                result.append((file_name,pred[j].cpu().numpy()[0])) \n",
        "        \n",
        "    return result"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMDYedpmsyjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c64df22-83c1-4a68-cda7-53091460b75b"
      },
      "source": [
        "\n",
        "workers=2\n",
        "pretrain_path='cct6-3x1_cifar100_epoch200_training.pth'\n",
        "checkpoint_path = 'cct6-3x1_cifar100_test.pth'\n",
        "\n",
        "# L/PxC \n",
        "# L = transformer layers\n",
        "# P = patch/convolution size\n",
        "# C = convolutional layers\n",
        "\n",
        "\n",
        "total_epochs=200\n",
        "start_epoch=0\n",
        "end_epoch=2\n",
        "\n",
        "warmup=20\n",
        "batch_size=128\n",
        "lr=0.0005\n",
        "weight_decay=3e-2\n",
        "clip_grad_norm=0.\n",
        "model='cct_6'\n",
        "positional_embedding='learnable'\n",
        "conv_layers=1\n",
        "conv_size=3\n",
        "patch_size=4\n",
        "gpu_id=0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#cifar100\n",
        "num_classes = 100\n",
        "img_size = 32\n",
        "mean = [0.5071, 0.4867, 0.4408]\n",
        "std = [0.2675, 0.2565, 0.2761]\n",
        "    \n",
        "\n",
        "model = cct_6(img_size=img_size,\n",
        "              num_classes=num_classes,\n",
        "              positional_embedding=positional_embedding,\n",
        "              n_conv_layers=conv_layers,\n",
        "              kernel_size=conv_size,\n",
        "              patch_size=patch_size).to(device)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ7TTYN62voV"
      },
      "source": [
        "best_acc1=0\n",
        "loss_train_arr =[]\n",
        "loss_val_arr =[]\n",
        "lrs =[]\n",
        "\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(pretrain_path, map_location='cpu'))\n",
        "\n",
        "\n",
        "criterion = LabelSmoothingCrossEntropy()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.set_device(gpu_id)\n",
        "    criterion = criterion.cuda(gpu_id)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr,\n",
        "                                weight_decay=weight_decay)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umM6W8fJ4Imc"
      },
      "source": [
        "normalize = [transforms.Normalize(mean=mean, std=std)]\n",
        "\n",
        "augmentations = transforms.Compose([\n",
        "    transforms.RandomCrop(img_size, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    CIFAR10Policy(),\n",
        "    transforms.ToTensor(),\n",
        "    *normalize,\n",
        "])\n",
        "\n",
        "train_dataset = ImageFolder(\n",
        "    root=data_dir+\"/TRAIN\",   transform=augmentations)\n",
        "\n",
        "val_dataset = ImageFolder(\n",
        "    root=data_dir+\"/TEST\",   transform=transforms.Compose([\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        *normalize,\n",
        "    ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True,\n",
        "    num_workers=workers)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size, shuffle=False,\n",
        "    num_workers=workers)\n",
        "\n",
        "\n",
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "val_loader = DeviceDataLoader(val_loader, device)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4ZPaXDFx3w5",
        "outputId": "5da4957a-f373-4ba1-b2ea-c730e2bd9731"
      },
      "source": [
        "for epoch in range(start_epoch, end_epoch):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    loss_train_arr = train(loss_train_arr, train_loader, model, criterion, optimizer, epoch)\n",
        "    loss_val_arr, acc1 = validate(loss_val_arr, val_loader, model, criterion,  epoch=epoch)\n",
        "    best_acc1 = max(acc1, best_acc1)\n",
        "    lrs.append(get_lr(optimizer))\n",
        "    print(f'lr: {lrs[-1]:.4e}')\n",
        "\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "result = test(model, val_loader, val_dataset)\n",
        "\n",
        "with open ('ID_result.csv','w') as f:\n",
        "    f.write('Id,Category\\n')\n",
        "    for data in result:\n",
        "        f.write(data[0]+','+str(data[1])+'\\n')\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: Epoch [1], Loss: 1.0637e+00, acc:  93.18\n",
            "Eval: Epoch [1], Loss: 1.6225e+00,  acc:  75.37 \n",
            "lr: 2.5000e-05\n",
            "Train: Epoch [2], Loss: 1.0664e+00, acc:  93.13\n",
            "Eval: Epoch [2], Loss: 1.6242e+00,  acc:  75.07 \n",
            "lr: 2.6316e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt6ogGMRWlzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c844584-91d6-4925-f71b-6e17f88d6b8c"
      },
      "source": [
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "# Print parameter\n",
        "size = summary(model, (3, 32, 32))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 256, 32, 32]           6,912\n",
            "              ReLU-2          [-1, 256, 32, 32]               0\n",
            "         MaxPool2d-3          [-1, 256, 16, 16]               0\n",
            "           Flatten-4             [-1, 256, 256]               0\n",
            "         Tokenizer-5             [-1, 256, 256]               0\n",
            "           Dropout-6             [-1, 256, 256]               0\n",
            "         LayerNorm-7             [-1, 256, 256]             512\n",
            "            Linear-8             [-1, 256, 768]         196,608\n",
            "           Dropout-9          [-1, 4, 256, 256]               0\n",
            "           Linear-10             [-1, 256, 256]          65,792\n",
            "          Dropout-11             [-1, 256, 256]               0\n",
            "        Attention-12             [-1, 256, 256]               0\n",
            "         Identity-13             [-1, 256, 256]               0\n",
            "        LayerNorm-14             [-1, 256, 256]             512\n",
            "           Linear-15             [-1, 256, 512]         131,584\n",
            "          Dropout-16             [-1, 256, 512]               0\n",
            "           Linear-17             [-1, 256, 256]         131,328\n",
            "          Dropout-18             [-1, 256, 256]               0\n",
            "         Identity-19             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-20             [-1, 256, 256]               0\n",
            "        LayerNorm-21             [-1, 256, 256]             512\n",
            "           Linear-22             [-1, 256, 768]         196,608\n",
            "          Dropout-23          [-1, 4, 256, 256]               0\n",
            "           Linear-24             [-1, 256, 256]          65,792\n",
            "          Dropout-25             [-1, 256, 256]               0\n",
            "        Attention-26             [-1, 256, 256]               0\n",
            "         DropPath-27             [-1, 256, 256]               0\n",
            "        LayerNorm-28             [-1, 256, 256]             512\n",
            "           Linear-29             [-1, 256, 512]         131,584\n",
            "          Dropout-30             [-1, 256, 512]               0\n",
            "           Linear-31             [-1, 256, 256]         131,328\n",
            "          Dropout-32             [-1, 256, 256]               0\n",
            "         DropPath-33             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-34             [-1, 256, 256]               0\n",
            "        LayerNorm-35             [-1, 256, 256]             512\n",
            "           Linear-36             [-1, 256, 768]         196,608\n",
            "          Dropout-37          [-1, 4, 256, 256]               0\n",
            "           Linear-38             [-1, 256, 256]          65,792\n",
            "          Dropout-39             [-1, 256, 256]               0\n",
            "        Attention-40             [-1, 256, 256]               0\n",
            "         DropPath-41             [-1, 256, 256]               0\n",
            "        LayerNorm-42             [-1, 256, 256]             512\n",
            "           Linear-43             [-1, 256, 512]         131,584\n",
            "          Dropout-44             [-1, 256, 512]               0\n",
            "           Linear-45             [-1, 256, 256]         131,328\n",
            "          Dropout-46             [-1, 256, 256]               0\n",
            "         DropPath-47             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-48             [-1, 256, 256]               0\n",
            "        LayerNorm-49             [-1, 256, 256]             512\n",
            "           Linear-50             [-1, 256, 768]         196,608\n",
            "          Dropout-51          [-1, 4, 256, 256]               0\n",
            "           Linear-52             [-1, 256, 256]          65,792\n",
            "          Dropout-53             [-1, 256, 256]               0\n",
            "        Attention-54             [-1, 256, 256]               0\n",
            "         DropPath-55             [-1, 256, 256]               0\n",
            "        LayerNorm-56             [-1, 256, 256]             512\n",
            "           Linear-57             [-1, 256, 512]         131,584\n",
            "          Dropout-58             [-1, 256, 512]               0\n",
            "           Linear-59             [-1, 256, 256]         131,328\n",
            "          Dropout-60             [-1, 256, 256]               0\n",
            "         DropPath-61             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-62             [-1, 256, 256]               0\n",
            "        LayerNorm-63             [-1, 256, 256]             512\n",
            "           Linear-64             [-1, 256, 768]         196,608\n",
            "          Dropout-65          [-1, 4, 256, 256]               0\n",
            "           Linear-66             [-1, 256, 256]          65,792\n",
            "          Dropout-67             [-1, 256, 256]               0\n",
            "        Attention-68             [-1, 256, 256]               0\n",
            "         DropPath-69             [-1, 256, 256]               0\n",
            "        LayerNorm-70             [-1, 256, 256]             512\n",
            "           Linear-71             [-1, 256, 512]         131,584\n",
            "          Dropout-72             [-1, 256, 512]               0\n",
            "           Linear-73             [-1, 256, 256]         131,328\n",
            "          Dropout-74             [-1, 256, 256]               0\n",
            "         DropPath-75             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-76             [-1, 256, 256]               0\n",
            "        LayerNorm-77             [-1, 256, 256]             512\n",
            "           Linear-78             [-1, 256, 768]         196,608\n",
            "          Dropout-79          [-1, 4, 256, 256]               0\n",
            "           Linear-80             [-1, 256, 256]          65,792\n",
            "          Dropout-81             [-1, 256, 256]               0\n",
            "        Attention-82             [-1, 256, 256]               0\n",
            "         DropPath-83             [-1, 256, 256]               0\n",
            "        LayerNorm-84             [-1, 256, 256]             512\n",
            "           Linear-85             [-1, 256, 512]         131,584\n",
            "          Dropout-86             [-1, 256, 512]               0\n",
            "           Linear-87             [-1, 256, 256]         131,328\n",
            "          Dropout-88             [-1, 256, 256]               0\n",
            "         DropPath-89             [-1, 256, 256]               0\n",
            "TransformerEncoderLayer-90             [-1, 256, 256]               0\n",
            "        LayerNorm-91             [-1, 256, 256]             512\n",
            "           Linear-92               [-1, 256, 1]             257\n",
            "           Linear-93                  [-1, 100]          25,700\n",
            "TransformerClassifier-94                  [-1, 100]               0\n",
            "================================================================\n",
            "Total params: 3,191,397\n",
            "Trainable params: 3,191,397\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 69.50\n",
            "Params size (MB): 12.17\n",
            "Estimated Total Size (MB): 81.69\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaEQExMDWoEV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "d1332fd1-5798-4e2d-bf94-c92278c2a170"
      },
      "source": [
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n",
        "\n",
        "\n",
        "plt.plot([x for x in loss_train_arr], \"-bx\")\n",
        "plt.plot([x for x in loss_val_arr],\"-rx\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"train loss\",\"val loss\"])\n",
        "\n",
        "print(loss_train_arr)\n",
        "print(loss_val_arr)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0637174175834656, 1.0664437671470641]\n",
            "[1.6224999289961906, 1.6242396833897115]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84_-ky406Kby",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "d82eb558-5366-4af3-8e64-b665cf4b0b9e"
      },
      "source": [
        "plt.plot([x for x in lrs], \"-bx\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Learning rate\")\n",
        "\n",
        "print(lrs)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.5e-05, 2.6315789473684212e-05]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}