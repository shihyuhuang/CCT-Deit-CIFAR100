{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cct_train1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhSQ4zfq0CfM"
      },
      "source": [
        "from time import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.datasets import ImageFolder, CIFAR100\n",
        "import random\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57692838La4I",
        "outputId": "bcc9d871-c663-47eb-dd46-6dff00385ad6"
      },
      "source": [
        "!pip install torchtoolbox"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtoolbox in /usr/local/lib/python3.7/dist-packages (0.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.19.5)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.99)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.62.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torchtoolbox) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV0WT80KVVms",
        "outputId": "85d67856-925d-4217-c4d5-64301f447437"
      },
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "\n",
        "\n",
        "od.download('https://www.kaggle.com/c/2021-ai-training-final-project/data')\n",
        "data_dir = './2021-ai-training-final-project/CIFAR100'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.7/dist-packages (0.1.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.62.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.5.30)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Skipping, found downloaded files in \"./2021-ai-training-final-project\" (use force=True to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxFa3TYR3oA8",
        "outputId": "89a5a833-c7bb-4fc0-a8aa-1afd12d86d5e"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "device = get_default_device()\n",
        "print(device)\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Wed Aug 25 15:12:32 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P0    64W / 149W |   2613MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMDYedpmsyjY"
      },
      "source": [
        "\n",
        "# Data args\n",
        "dataset='cifar100'\n",
        "workers=4\n",
        "print_freq=10\n",
        "pretrain_path='cct6-3x2_cifar100_best.pth'\n",
        "checkpoint_path = 'cct6-3x2_cifar100_training.pth'\n",
        "\n",
        "# Optimization hyperparams\n",
        "epochs=50\n",
        "warmup=5\n",
        "batch_size=128\n",
        "lr=0.0005\n",
        "weight_decay=3e-2\n",
        "clip_grad_norm=0.\n",
        "model='cct_6'\n",
        "positional_embedding='learnable'\n",
        "conv_layers=2\n",
        "conv_size=3\n",
        "patch_size=4\n",
        "gpu_id=0\n",
        "no_cuda=False\n",
        "disable_aug=False\n",
        "disable_cos=False\n",
        "\n",
        "\n",
        "\n",
        "DATASETS = {\n",
        "    'cifar100': {\n",
        "        'num_classes': 100,\n",
        "        'img_size': 32,\n",
        "        'mean': [0.5071, 0.4867, 0.4408],\n",
        "        'std': [0.2675, 0.2565, 0.2761]\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcGLeXU7te0B"
      },
      "source": [
        "#src/cct.py \n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "__all__ = ['cct_2', 'cct_4', 'cct_6', 'cct_7', 'cct_8',\n",
        "           'cct_14', 'cct_16',\n",
        "           'text_cct_2', 'text_cct_4', 'text_cct_6'\n",
        "           ]\n",
        "\n",
        "\n",
        "class CCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 embedding_dim=768,\n",
        "                 n_input_channels=3,\n",
        "                 n_conv_layers=1,\n",
        "                 kernel_size=7,\n",
        "                 stride=2,\n",
        "                 padding=3,\n",
        "                 pooling_kernel_size=3,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(CCT, self).__init__()\n",
        "\n",
        "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
        "                                   n_output_channels=embedding_dim,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   stride=stride,\n",
        "                                   padding=padding,\n",
        "                                   pooling_kernel_size=pooling_kernel_size,\n",
        "                                   pooling_stride=pooling_stride,\n",
        "                                   pooling_padding=pooling_padding,\n",
        "                                   max_pool=True,\n",
        "                                   activation=nn.ReLU,\n",
        "                                   n_conv_layers=n_conv_layers,\n",
        "                                   conv_bias=False)\n",
        "\n",
        "        self.classifier = TransformerClassifier(\n",
        "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
        "                                                           height=img_size,\n",
        "                                                           width=img_size),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout_rate=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenizer(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "class TextCCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 seq_len=64,\n",
        "                 word_embedding_dim=300,\n",
        "                 embedding_dim=256,\n",
        "                 kernel_size=2,\n",
        "                 stride=1,\n",
        "                 padding=1,\n",
        "                 pooling_kernel_size=2,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(TextCCT, self).__init__()\n",
        "\n",
        "        self.embedder = Embedder(word_embedding_dim=word_embedding_dim,\n",
        "                                 *args, **kwargs)\n",
        "\n",
        "        self.tokenizer = TextTokenizer(n_input_channels=word_embedding_dim,\n",
        "                                       n_output_channels=embedding_dim,\n",
        "                                       kernel_size=kernel_size,\n",
        "                                       stride=stride,\n",
        "                                       padding=padding,\n",
        "                                       pooling_kernel_size=pooling_kernel_size,\n",
        "                                       pooling_stride=pooling_stride,\n",
        "                                       pooling_padding=pooling_padding,\n",
        "                                       max_pool=True,\n",
        "                                       activation=nn.ReLU)\n",
        "\n",
        "        self.classifier = MaskedTransformerClassifier(\n",
        "            seq_len=self.tokenizer.seq_len(seq_len=seq_len, embed_dim=word_embedding_dim),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x, mask = self.embedder(x, mask=mask)\n",
        "        x, mask = self.tokenizer(x, mask=mask)\n",
        "        out = self.classifier(x, mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "def _cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "         kernel_size=3, stride=None, padding=None,\n",
        "         *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "    return CCT(num_layers=num_layers,\n",
        "               num_heads=num_heads,\n",
        "               mlp_ratio=mlp_ratio,\n",
        "               embedding_dim=embedding_dim,\n",
        "               kernel_size=kernel_size,\n",
        "               stride=stride,\n",
        "               padding=padding,\n",
        "               *args, **kwargs)\n",
        "\n",
        "\n",
        "def _text_cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "              kernel_size=4, stride=None, padding=None,\n",
        "              *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "\n",
        "    return TextCCT(num_layers=num_layers,\n",
        "                   num_heads=num_heads,\n",
        "                   mlp_ratio=mlp_ratio,\n",
        "                   embedding_dim=embedding_dim,\n",
        "                   kernel_size=kernel_size,\n",
        "                   stride=stride,\n",
        "                   padding=padding,\n",
        "                   *args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cct_6(*args, **kwargs):\n",
        "    return _cct(num_layers=6, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj6UX-dzvo1l"
      },
      "source": [
        "#/src/utils/embedder.py\n",
        "class Embedder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 word_embedding_dim=300,\n",
        "                 vocab_size=100000,\n",
        "                 padding_idx=1,\n",
        "                 pretrained_weight=None,\n",
        "                 embed_freeze=False,\n",
        "                 *args, **kwargs):\n",
        "        super(Embedder, self).__init__()\n",
        "        self.embeddings = nn.Embedding.from_pretrained(pretrained_weight, freeze=embed_freeze) \\\n",
        "            if pretrained_weight is not None else \\\n",
        "            nn.Embedding(vocab_size, word_embedding_dim, padding_idx=padding_idx)\n",
        "        self.embeddings.weight.requires_grad = not embed_freeze\n",
        "\n",
        "    def forward_mask(self, mask):\n",
        "        bsz, seq_len = mask.shape\n",
        "        new_mask = mask.view(bsz, seq_len, 1)\n",
        "        new_mask = new_mask.sum(-1)\n",
        "        new_mask = (new_mask > 0)\n",
        "        return new_mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        embed = self.embeddings(x)\n",
        "        embed = embed if mask is None else embed * self.forward_mask(mask).unsqueeze(-1).float()\n",
        "        return embed, mask\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        else:\n",
        "            nn.init.normal_(m.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11WxM9ycvfy-"
      },
      "source": [
        "#/src/utils/tokenizer.py \n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 n_conv_layers=1,\n",
        "                 n_input_channels=3,\n",
        "                 n_output_channels=64,\n",
        "                 in_planes=64,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 conv_bias=False):\n",
        "        super(Tokenizer, self).__init__()\n",
        "\n",
        "        n_filter_list = [n_input_channels] + \\\n",
        "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
        "                        [n_output_channels]\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
        "                          kernel_size=(kernel_size, kernel_size),\n",
        "                          stride=(stride, stride),\n",
        "                          padding=(padding, padding), bias=conv_bias),\n",
        "                nn.Identity() if activation is None else activation(),\n",
        "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
        "                             stride=pooling_stride,\n",
        "                             padding=pooling_padding) if max_pool else nn.Identity()\n",
        "            )\n",
        "                for i in range(n_conv_layers)\n",
        "            ])\n",
        "\n",
        "        self.flattener = nn.Flatten(2, 3)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
        "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.flattener(self.conv_layers(x)).transpose(-2, -1)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "\n",
        "class TextTokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 embedding_dim=300,\n",
        "                 n_output_channels=128,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 *args, **kwargs):\n",
        "        super(TextTokenizer, self).__init__()\n",
        "\n",
        "        self.max_pool = max_pool\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, n_output_channels,\n",
        "                      kernel_size=(kernel_size, embedding_dim),\n",
        "                      stride=(stride, 1),\n",
        "                      padding=(padding, 0), bias=False),\n",
        "            nn.Identity() if activation is None else activation(),\n",
        "            nn.MaxPool2d(\n",
        "                kernel_size=(pooling_kernel_size, 1),\n",
        "                stride=(pooling_stride, 1),\n",
        "                padding=(pooling_padding, 0)\n",
        "            ) if max_pool else nn.Identity()\n",
        "        )\n",
        "\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def seq_len(self, seq_len=32, embed_dim=300):\n",
        "        return self.forward(torch.zeros((1, seq_len, embed_dim)))[0].shape[1]\n",
        "\n",
        "    def forward_mask(self, mask):\n",
        "        new_mask = mask.unsqueeze(1).float()\n",
        "        cnn_weight = torch.ones(\n",
        "            (1, 1, self.conv_layers[0].kernel_size[0]),\n",
        "            device=mask.device,\n",
        "            dtype=torch.float)\n",
        "        new_mask = F.conv1d(\n",
        "            new_mask, cnn_weight, None,\n",
        "            self.conv_layers[0].stride[0], self.conv_layers[0].padding[0], 1, 1)\n",
        "        if self.max_pool:\n",
        "            new_mask = F.max_pool1d(\n",
        "                new_mask, self.conv_layers[2].kernel_size[0],\n",
        "                self.conv_layers[2].stride[0], self.conv_layers[2].padding[0], 1, False, False)\n",
        "        new_mask = new_mask.squeeze(1)\n",
        "        new_mask = (new_mask > 0)\n",
        "        return new_mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.transpose(1, 3).squeeze(1)\n",
        "        x = x if mask is None else x * self.forward_mask(mask).unsqueeze(-1).float()\n",
        "        return x, mask\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i67i7ugxvSeM"
      },
      "source": [
        "#src/utils/transformers.py \n",
        "import torch\n",
        "from torch.nn import Module, ModuleList, Linear, Dropout, LayerNorm, Identity, Parameter, init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Attention(Module):\n",
        "    \"\"\"\n",
        "    Obtained from timm: github.com:rwightman/pytorch-image-models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MaskedAttention(Module):\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask_value = -torch.finfo(attn.dtype).max\n",
        "            assert mask.shape[-1] == attn.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
        "            attn.masked_fill_(~mask, mask_value)\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n",
        "                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "class MaskedTransformerEncoderLayer(Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(MaskedTransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = MaskedAttention(dim=d_model, num_heads=nhead,\n",
        "                                         attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, mask=None, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src), mask))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 sequence_length=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            sequence_length += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                    dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        return pe.unsqueeze(0)\n",
        "\n",
        "\n",
        "class MaskedTransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 seq_len=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert seq_len is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            seq_len += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                seq_len += 1  # padding idx\n",
        "                self.positional_emb = Parameter(torch.zeros(1, seq_len, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(seq_len,\n",
        "                                                                          embedding_dim,\n",
        "                                                                          padding_idx=True),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            MaskedTransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                          dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                          attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        if self.positional_emb is None and x.size(1) < self.seq_len:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "            if mask is not None:\n",
        "                mask = torch.cat([torch.ones(size=(mask.shape[0], 1), device=mask.device), mask.float()], dim=1)\n",
        "                mask = (mask > 0)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, mask=mask)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim, padding_idx=False):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        pe = pe.unsqueeze(0)\n",
        "        if padding_idx:\n",
        "            return torch.cat([torch.zeros((1, 1, dim)), pe], dim=1)\n",
        "        return pe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNOUNvqT7S3g"
      },
      "source": [
        "#utils/transforms.py \n",
        "class ShearX(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n",
        "            Image.BICUBIC, fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class ShearY(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n",
        "            Image.BICUBIC, fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class TranslateX(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, magnitude * x.size[0] * random.choice([-1, 1]), 0, 1, 0),\n",
        "            fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class TranslateY(object):\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.fillcolor = fillcolor\n",
        "\n",
        "    def __call__(self, x, magnitude):\n",
        "        return x.transform(\n",
        "            x.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * x.size[1] * random.choice([-1, 1])),\n",
        "            fillcolor=self.fillcolor)\n",
        "\n",
        "\n",
        "class Rotate(object):\n",
        "    # from https://stackoverflow.com/questions/\n",
        "    # 5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n",
        "    def __call__(self, x, magnitude):\n",
        "        rot = x.convert(\"RGBA\").rotate(magnitude)\n",
        "        return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(x.mode)\n",
        "\n",
        "\n",
        "class Color(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Color(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Posterize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.posterize(x, magnitude)\n",
        "\n",
        "\n",
        "class Solarize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.solarize(x, magnitude)\n",
        "\n",
        "\n",
        "class Contrast(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Contrast(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Sharpness(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Sharpness(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class Brightness(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageEnhance.Brightness(x).enhance(1 + magnitude * random.choice([-1, 1]))\n",
        "\n",
        "\n",
        "class AutoContrast(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.autocontrast(x)\n",
        "\n",
        "\n",
        "class Equalize(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.equalize(x)\n",
        "\n",
        "\n",
        "class Invert(object):\n",
        "    def __call__(self, x, magnitude):\n",
        "        return ImageOps.invert(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6L2he3-60fH"
      },
      "source": [
        "#utils/autoaug.py \n",
        "class ImageNetPolicy(object):\n",
        "    \"\"\" Randomly choose one of the best 24 Sub-policies on ImageNet.\n",
        "        Example:\n",
        "        >>> policy = ImageNetPolicy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     ImageNetPolicy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.4, \"posterize\", 8, 0.6, \"rotate\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor),\n",
        "            SubPolicy(0.6, \"posterize\", 7, 0.6, \"posterize\", 6, fillcolor),\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 4, 0.8, \"rotate\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 3, 0.6, \"equalize\", 7, fillcolor),\n",
        "            SubPolicy(0.8, \"posterize\", 5, 1.0, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.2, \"rotate\", 3, 0.6, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 8, 0.4, \"posterize\", 6, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 0.4, \"color\", 0, fillcolor),\n",
        "            SubPolicy(0.4, \"rotate\", 9, 0.6, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.0, \"equalize\", 7, 0.8, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 1.0, \"color\", 2, fillcolor),\n",
        "            SubPolicy(0.8, \"color\", 8, 0.8, \"solarize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"sharpness\", 7, 0.6, \"invert\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"shearX\", 5, 1.0, \"equalize\", 9, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 0, 0.6, \"equalize\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment ImageNet Policy\"\n",
        "\n",
        "\n",
        "class CIFAR10Policy(object):\n",
        "    \"\"\" Randomly choose one of the best 25 Sub-policies on CIFAR10.\n",
        "        Example:\n",
        "        >>> policy = CIFAR10Policy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     CIFAR10Policy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n",
        "            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n",
        "            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n",
        "            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n",
        "            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n",
        "            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n",
        "\n",
        "            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n",
        "            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n",
        "            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n",
        "            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n",
        "            SubPolicy(0.2, \"equalize\", 8, 0.6, \"equalize\", 4, fillcolor),\n",
        "            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n",
        "            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n",
        "            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n",
        "            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment CIFAR10 Policy\"\n",
        "\n",
        "\n",
        "class SubPolicy(object):\n",
        "    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n",
        "        ranges = {\n",
        "            \"shearX\": np.linspace(0, 0.3, 10),\n",
        "            \"shearY\": np.linspace(0, 0.3, 10),\n",
        "            \"translateX\": np.linspace(0, 150 / 331, 10),\n",
        "            \"translateY\": np.linspace(0, 150 / 331, 10),\n",
        "            \"rotate\": np.linspace(0, 30, 10),\n",
        "            \"color\": np.linspace(0.0, 0.9, 10),\n",
        "            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n",
        "            \"solarize\": np.linspace(256, 0, 10),\n",
        "            \"contrast\": np.linspace(0.0, 0.9, 10),\n",
        "            \"sharpness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"brightness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"autocontrast\": [0] * 10,\n",
        "            \"equalize\": [0] * 10,\n",
        "            \"invert\": [0] * 10\n",
        "        }\n",
        "\n",
        "        func = {\n",
        "            \"shearX\": ShearX(fillcolor=fillcolor),\n",
        "            \"shearY\": ShearY(fillcolor=fillcolor),\n",
        "            \"translateX\": TranslateX(fillcolor=fillcolor),\n",
        "            \"translateY\": TranslateY(fillcolor=fillcolor),\n",
        "            \"rotate\": Rotate(),\n",
        "            \"color\": Color(),\n",
        "            \"posterize\": Posterize(),\n",
        "            \"solarize\": Solarize(),\n",
        "            \"contrast\": Contrast(),\n",
        "            \"sharpness\": Sharpness(),\n",
        "            \"brightness\": Brightness(),\n",
        "            \"autocontrast\": AutoContrast(),\n",
        "            \"equalize\": Equalize(),\n",
        "            \"invert\": Invert()\n",
        "        }\n",
        "\n",
        "        self.p1 = p1\n",
        "        self.operation1 = func[operation1]\n",
        "        self.magnitude1 = ranges[operation1][magnitude_idx1]\n",
        "        self.p2 = p2\n",
        "        self.operation2 = func[operation2]\n",
        "        self.magnitude2 = ranges[operation2][magnitude_idx2]\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p1:\n",
        "            img = self.operation1(img, self.magnitude1)\n",
        "        if random.random() < self.p2:\n",
        "            img = self.operation2(img, self.magnitude2)\n",
        "        return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR3mRPNHv1iY"
      },
      "source": [
        "#utils/stochastic_depth.py \n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npCm0MjVWfjU"
      },
      "source": [
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lrr = lr\n",
        "    if epoch < warmup:\n",
        "        lrr = lr / (warmup - epoch)\n",
        "    elif not disable_cos:\n",
        "        lrr *= 0.5 * (1. + math.cos(math.pi * (epoch - warmup) / (epochs - warmup)))\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lrr\n",
        "\n",
        "\n",
        "def accuracy(output, target):\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(1, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        correct_k = correct[:1].flatten().float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def cls_train(loss_train_arr, train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    loss_val, acc1_val = 0, 0\n",
        "    n = 0\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        if (not no_cuda) and torch.cuda.is_available():\n",
        "            images = images.cuda(gpu_id, non_blocking=True)\n",
        "            target = target.cuda(gpu_id, non_blocking=True)\n",
        "        output = model(images)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        acc1 = accuracy(output, target)\n",
        "        n += images.size(0)\n",
        "        loss_val += float(loss.item() * images.size(0))\n",
        "        acc1_val += float(acc1[0] * images.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        if clip_grad_norm > 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm, norm_type=2)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        #if print_freq >= 0 and i % print_freq == 0:\n",
        "    avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "    print(f'[Epoch {epoch + 1}][Train] \\t Loss: {avg_loss:.4e} \\t Top-1 {avg_acc1:6.2f}')\n",
        "    loss_train_arr.append(avg_loss)\n",
        "    \n",
        "    return loss_train_arr\n",
        "\n",
        "def cls_validate(loss_val_arr, val_loader, model, criterion,  epoch=None, time_begin=None):\n",
        "    model.eval()\n",
        "    loss_val, acc1_val = 0, 0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            if (not no_cuda) and torch.cuda.is_available():\n",
        "                images = images.cuda(gpu_id, non_blocking=True)\n",
        "                target = target.cuda(gpu_id, non_blocking=True)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc1 = accuracy(output, target)\n",
        "            n += images.size(0)\n",
        "            loss_val += float(loss.item() * images.size(0))\n",
        "            acc1_val += float(acc1[0] * images.size(0))\n",
        "\n",
        "            #if print_freq >= 0 and i % print_freq == 0:\n",
        "                #avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "                #print(f'[Epoch {epoch + 1}][Eval][{i}] \\t Loss: {avg_loss:.4e} \\t Top-1 {avg_acc1:6.2f}')\n",
        "                \n",
        "               \n",
        "\n",
        "    avg_loss, avg_acc1 = (loss_val / n), (acc1_val / n)\n",
        "    total_mins = -1 if time_begin is None else (time() - time_begin) / 60\n",
        "    print(f'[Epoch {epoch + 1}][Eval] \\t Loss: {avg_loss:.4e} \\t Top-1 {avg_acc1:6.2f} \\t \\t Time: {total_mins:.2f}')\n",
        "    loss_val_arr.append(avg_loss)\n",
        "\n",
        "    return loss_val_arr, avg_acc1\n",
        "\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    NLL loss with label smoothing.\n",
        "    \"\"\"\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        \"\"\"\n",
        "        Constructor for the LabelSmoothing module.\n",
        "        :param smoothing: label smoothing factor\n",
        "        \"\"\"\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        assert smoothing < 1.0\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1. - smoothing\n",
        "\n",
        "    def _compute_losses(self, x, target):\n",
        "        log_prob = F.log_softmax(x, dim=-1)\n",
        "        nll_loss = -log_prob.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -log_prob.mean(dim=-1)\n",
        "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        return self._compute_losses(x, target).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ZPaXDFx3w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e36d06-0c78-4e72-8ae3-7a3816554802"
      },
      "source": [
        "#main.py \n",
        "\n",
        "\n",
        "\n",
        "best_acc1 = 0\n",
        "loss_train_arr =[]\n",
        "loss_val_arr = []\n",
        "\n",
        "\n",
        "img_size = DATASETS[dataset]['img_size']\n",
        "num_classes = DATASETS[dataset]['num_classes']\n",
        "img_mean, img_std = DATASETS[dataset]['mean'], DATASETS[dataset]['std']\n",
        "\n",
        "model = cct_6(img_size=img_size,\n",
        "              num_classes=num_classes,\n",
        "              positional_embedding=positional_embedding,\n",
        "              n_conv_layers=conv_layers,\n",
        "              kernel_size=conv_size,\n",
        "              patch_size=patch_size)\n",
        "\n",
        "model.load_state_dict(torch.load(pretrain_path, map_location='cpu'))\n",
        "print(\"Loaded checkpoint.\")\n",
        "\n",
        "\n",
        "\n",
        "criterion = LabelSmoothingCrossEntropy()\n",
        "\n",
        "if (not no_cuda) and torch.cuda.is_available():\n",
        "    torch.cuda.set_device(gpu_id)\n",
        "    model.cuda(gpu_id)\n",
        "    criterion = criterion.cuda(gpu_id)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "normalize = [transforms.Normalize(mean=img_mean, std=img_std)]\n",
        "\n",
        "augmentations = []\n",
        "if not disable_aug:\n",
        "    augmentations += [CIFAR10Policy()]\n",
        "    \n",
        "augmentations += [transforms.RandomCrop(img_size, padding=4),\n",
        "                  transforms.RandomHorizontalFlip(),\n",
        "                  transforms.ToTensor(),\n",
        "                  *normalize,\n",
        "                  ]\n",
        "\n",
        "augmentations = transforms.Compose(augmentations)\n",
        "train_dataset = ImageFolder(root=data_dir+\"/TRAIN\", transform=augmentations)\n",
        "\n",
        "val_dataset = ImageFolder(root=data_dir+\"/TEST\", \n",
        "            transform=transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            *normalize,\n",
        "            ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=workers)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size, shuffle=True,\n",
        "        num_workers=workers)\n",
        "\n",
        "print(\"Beginning training\")\n",
        "\n",
        "time_begin = time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    loss_train_arr = cls_train(loss_train_arr, train_loader, model, criterion, optimizer, epoch)\n",
        "    loss_val_arr, acc1 = cls_validate(loss_val_arr, val_loader, model, criterion, epoch=epoch, time_begin=time_begin)\n",
        "    best_acc1 = max(acc1, best_acc1)\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "total_mins = (time() - time_begin) / 60\n",
        "print(f'Script finished in {total_mins:.2f} minutes, '\n",
        "      f'best top-1: {best_acc1:.2f}, '\n",
        "      f'final top-1: {acc1:.2f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded checkpoint.\n",
            "Beginning training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1][Train] \t Loss: 1.1052e+00 \t Top-1  91.58\n",
            "[Epoch 1][Eval] \t Loss: 1.6708e+00 \t Top-1  73.93 \t \t Time: 1.52\n",
            "[Epoch 2][Train] \t Loss: 1.1160e+00 \t Top-1  91.30\n",
            "[Epoch 2][Eval] \t Loss: 1.6962e+00 \t Top-1  73.52 \t \t Time: 3.04\n",
            "[Epoch 3][Train] \t Loss: 1.1400e+00 \t Top-1  90.44\n",
            "[Epoch 3][Eval] \t Loss: 1.7072e+00 \t Top-1  73.19 \t \t Time: 4.55\n",
            "[Epoch 4][Train] \t Loss: 1.1905e+00 \t Top-1  88.84\n",
            "[Epoch 4][Eval] \t Loss: 1.7347e+00 \t Top-1  72.16 \t \t Time: 6.06\n",
            "[Epoch 5][Train] \t Loss: 1.3888e+00 \t Top-1  81.97\n",
            "[Epoch 5][Eval] \t Loss: 1.8301e+00 \t Top-1  69.16 \t \t Time: 7.58\n",
            "[Epoch 6][Train] \t Loss: 1.3924e+00 \t Top-1  81.68\n",
            "[Epoch 6][Eval] \t Loss: 1.8045e+00 \t Top-1  69.74 \t \t Time: 9.10\n",
            "[Epoch 7][Train] \t Loss: 1.3837e+00 \t Top-1  82.10\n",
            "[Epoch 7][Eval] \t Loss: 1.8077e+00 \t Top-1  69.22 \t \t Time: 10.62\n",
            "[Epoch 8][Train] \t Loss: 1.3726e+00 \t Top-1  82.79\n",
            "[Epoch 8][Eval] \t Loss: 1.7973e+00 \t Top-1  69.75 \t \t Time: 12.13\n",
            "[Epoch 9][Train] \t Loss: 1.3766e+00 \t Top-1  82.48\n",
            "[Epoch 9][Eval] \t Loss: 1.8037e+00 \t Top-1  69.99 \t \t Time: 13.65\n",
            "[Epoch 10][Train] \t Loss: 1.3671e+00 \t Top-1  82.88\n",
            "[Epoch 10][Eval] \t Loss: 1.8255e+00 \t Top-1  69.27 \t \t Time: 15.17\n",
            "[Epoch 11][Train] \t Loss: 1.3693e+00 \t Top-1  82.76\n",
            "[Epoch 11][Eval] \t Loss: 1.7937e+00 \t Top-1  70.22 \t \t Time: 16.68\n",
            "[Epoch 12][Train] \t Loss: 1.3668e+00 \t Top-1  83.12\n",
            "[Epoch 12][Eval] \t Loss: 1.8085e+00 \t Top-1  69.66 \t \t Time: 18.20\n",
            "[Epoch 13][Train] \t Loss: 1.3518e+00 \t Top-1  83.42\n",
            "[Epoch 13][Eval] \t Loss: 1.7985e+00 \t Top-1  70.03 \t \t Time: 19.72\n",
            "[Epoch 14][Train] \t Loss: 1.3411e+00 \t Top-1  83.63\n",
            "[Epoch 14][Eval] \t Loss: 1.7950e+00 \t Top-1  70.07 \t \t Time: 21.23\n",
            "[Epoch 15][Train] \t Loss: 1.3362e+00 \t Top-1  84.04\n",
            "[Epoch 15][Eval] \t Loss: 1.7946e+00 \t Top-1  69.70 \t \t Time: 22.75\n",
            "[Epoch 16][Train] \t Loss: 1.3240e+00 \t Top-1  84.31\n",
            "[Epoch 16][Eval] \t Loss: 1.7841e+00 \t Top-1  70.16 \t \t Time: 24.27\n",
            "[Epoch 17][Train] \t Loss: 1.3189e+00 \t Top-1  84.50\n",
            "[Epoch 17][Eval] \t Loss: 1.7730e+00 \t Top-1  70.73 \t \t Time: 25.79\n",
            "[Epoch 18][Train] \t Loss: 1.3109e+00 \t Top-1  84.87\n",
            "[Epoch 18][Eval] \t Loss: 1.7788e+00 \t Top-1  70.86 \t \t Time: 27.30\n",
            "[Epoch 19][Train] \t Loss: 1.2992e+00 \t Top-1  85.05\n",
            "[Epoch 19][Eval] \t Loss: 1.8018e+00 \t Top-1  70.23 \t \t Time: 28.82\n",
            "[Epoch 20][Train] \t Loss: 1.2876e+00 \t Top-1  85.55\n",
            "[Epoch 20][Eval] \t Loss: 1.7648e+00 \t Top-1  70.73 \t \t Time: 30.33\n",
            "[Epoch 21][Train] \t Loss: 1.2784e+00 \t Top-1  85.80\n",
            "[Epoch 21][Eval] \t Loss: 1.7563e+00 \t Top-1  71.24 \t \t Time: 31.85\n",
            "[Epoch 22][Train] \t Loss: 1.2716e+00 \t Top-1  85.91\n",
            "[Epoch 22][Eval] \t Loss: 1.7486e+00 \t Top-1  71.23 \t \t Time: 33.37\n",
            "[Epoch 23][Train] \t Loss: 1.2626e+00 \t Top-1  86.45\n",
            "[Epoch 23][Eval] \t Loss: 1.7701e+00 \t Top-1  71.22 \t \t Time: 34.88\n",
            "[Epoch 24][Train] \t Loss: 1.2532e+00 \t Top-1  86.63\n",
            "[Epoch 24][Eval] \t Loss: 1.7482e+00 \t Top-1  71.59 \t \t Time: 36.40\n",
            "[Epoch 25][Train] \t Loss: 1.2391e+00 \t Top-1  87.14\n",
            "[Epoch 25][Eval] \t Loss: 1.7403e+00 \t Top-1  71.88 \t \t Time: 37.91\n",
            "[Epoch 26][Train] \t Loss: 1.2267e+00 \t Top-1  87.65\n",
            "[Epoch 26][Eval] \t Loss: 1.7391e+00 \t Top-1  71.87 \t \t Time: 39.43\n",
            "[Epoch 27][Train] \t Loss: 1.2172e+00 \t Top-1  87.94\n",
            "[Epoch 27][Eval] \t Loss: 1.7235e+00 \t Top-1  71.81 \t \t Time: 40.94\n",
            "[Epoch 28][Train] \t Loss: 1.2107e+00 \t Top-1  88.14\n",
            "[Epoch 28][Eval] \t Loss: 1.7351e+00 \t Top-1  71.78 \t \t Time: 42.45\n",
            "[Epoch 29][Train] \t Loss: 1.1890e+00 \t Top-1  88.89\n",
            "[Epoch 29][Eval] \t Loss: 1.7273e+00 \t Top-1  72.00 \t \t Time: 43.97\n",
            "[Epoch 30][Train] \t Loss: 1.1826e+00 \t Top-1  89.14\n",
            "[Epoch 30][Eval] \t Loss: 1.7283e+00 \t Top-1  71.94 \t \t Time: 45.48\n",
            "[Epoch 31][Train] \t Loss: 1.1753e+00 \t Top-1  89.24\n",
            "[Epoch 31][Eval] \t Loss: 1.7181e+00 \t Top-1  72.68 \t \t Time: 46.99\n",
            "[Epoch 32][Train] \t Loss: 1.1694e+00 \t Top-1  89.45\n",
            "[Epoch 32][Eval] \t Loss: 1.7122e+00 \t Top-1  72.73 \t \t Time: 48.51\n",
            "[Epoch 33][Train] \t Loss: 1.1520e+00 \t Top-1  90.01\n",
            "[Epoch 33][Eval] \t Loss: 1.7052e+00 \t Top-1  72.92 \t \t Time: 50.02\n",
            "[Epoch 34][Train] \t Loss: 1.1389e+00 \t Top-1  90.35\n",
            "[Epoch 34][Eval] \t Loss: 1.7032e+00 \t Top-1  73.11 \t \t Time: 51.53\n",
            "[Epoch 35][Train] \t Loss: 1.1290e+00 \t Top-1  90.62\n",
            "[Epoch 35][Eval] \t Loss: 1.6928e+00 \t Top-1  72.94 \t \t Time: 53.04\n",
            "[Epoch 36][Train] \t Loss: 1.1278e+00 \t Top-1  90.72\n",
            "[Epoch 36][Eval] \t Loss: 1.6895e+00 \t Top-1  73.20 \t \t Time: 54.55\n",
            "[Epoch 37][Train] \t Loss: 1.1176e+00 \t Top-1  91.03\n",
            "[Epoch 37][Eval] \t Loss: 1.6871e+00 \t Top-1  73.64 \t \t Time: 56.06\n",
            "[Epoch 38][Train] \t Loss: 1.1086e+00 \t Top-1  91.28\n",
            "[Epoch 38][Eval] \t Loss: 1.7021e+00 \t Top-1  73.04 \t \t Time: 57.58\n",
            "[Epoch 39][Train] \t Loss: 1.1086e+00 \t Top-1  91.28\n",
            "[Epoch 39][Eval] \t Loss: 1.6900e+00 \t Top-1  73.66 \t \t Time: 59.09\n",
            "[Epoch 40][Train] \t Loss: 1.0971e+00 \t Top-1  91.75\n",
            "[Epoch 40][Eval] \t Loss: 1.6824e+00 \t Top-1  73.71 \t \t Time: 60.61\n",
            "[Epoch 41][Train] \t Loss: 1.0965e+00 \t Top-1  91.61\n",
            "[Epoch 41][Eval] \t Loss: 1.6775e+00 \t Top-1  73.67 \t \t Time: 62.12\n",
            "[Epoch 42][Train] \t Loss: 1.0830e+00 \t Top-1  92.12\n",
            "[Epoch 42][Eval] \t Loss: 1.6770e+00 \t Top-1  73.93 \t \t Time: 63.63\n",
            "[Epoch 43][Train] \t Loss: 1.0819e+00 \t Top-1  92.12\n",
            "[Epoch 43][Eval] \t Loss: 1.6753e+00 \t Top-1  74.11 \t \t Time: 65.14\n",
            "[Epoch 44][Train] \t Loss: 1.0823e+00 \t Top-1  92.08\n",
            "[Epoch 44][Eval] \t Loss: 1.6701e+00 \t Top-1  74.24 \t \t Time: 66.66\n",
            "[Epoch 45][Train] \t Loss: 1.0673e+00 \t Top-1  92.53\n",
            "[Epoch 45][Eval] \t Loss: 1.6694e+00 \t Top-1  74.21 \t \t Time: 68.17\n",
            "[Epoch 46][Train] \t Loss: 1.0808e+00 \t Top-1  92.21\n",
            "[Epoch 46][Eval] \t Loss: 1.6640e+00 \t Top-1  74.28 \t \t Time: 69.69\n",
            "[Epoch 47][Train] \t Loss: 1.0755e+00 \t Top-1  92.38\n",
            "[Epoch 47][Eval] \t Loss: 1.6665e+00 \t Top-1  74.17 \t \t Time: 71.22\n",
            "[Epoch 48][Train] \t Loss: 1.0711e+00 \t Top-1  92.54\n",
            "[Epoch 48][Eval] \t Loss: 1.6634e+00 \t Top-1  74.40 \t \t Time: 72.75\n",
            "[Epoch 49][Train] \t Loss: 1.0703e+00 \t Top-1  92.55\n",
            "[Epoch 49][Eval] \t Loss: 1.6640e+00 \t Top-1  74.43 \t \t Time: 74.28\n",
            "[Epoch 50][Train] \t Loss: 1.0692e+00 \t Top-1  92.53\n",
            "[Epoch 50][Eval] \t Loss: 1.6641e+00 \t Top-1  74.47 \t \t Time: 75.81\n",
            "Script finished in 75.81 minutes, best top-1: 74.47, final top-1: 74.47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt6ogGMRWlzY",
        "outputId": "f02cf2be-9d6f-4912-ac33-ac5bbc7f3960"
      },
      "source": [
        "from torchsummary import summary\n",
        "# Print model\n",
        "print(model)\n",
        "\n",
        "# Print parameter\n",
        "size = summary(model, (3, 32, 32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CCT(\n",
            "  (tokenizer): Tokenizer(\n",
            "    (conv_layers): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): ReLU()\n",
            "        (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): ReLU()\n",
            "        (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "    )\n",
            "    (flattener): Flatten(start_dim=2, end_dim=3)\n",
            "  )\n",
            "  (classifier): TransformerClassifier(\n",
            "    (attention_pool): Linear(in_features=256, out_features=1, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "    (blocks): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (fc): Linear(in_features=256, out_features=100, bias=True)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "              ReLU-2           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-3           [-1, 64, 16, 16]               0\n",
            "            Conv2d-4          [-1, 256, 16, 16]         147,456\n",
            "              ReLU-5          [-1, 256, 16, 16]               0\n",
            "         MaxPool2d-6            [-1, 256, 8, 8]               0\n",
            "           Flatten-7              [-1, 256, 64]               0\n",
            "         Tokenizer-8              [-1, 64, 256]               0\n",
            "           Dropout-9              [-1, 64, 256]               0\n",
            "        LayerNorm-10              [-1, 64, 256]             512\n",
            "           Linear-11              [-1, 64, 768]         196,608\n",
            "          Dropout-12            [-1, 4, 64, 64]               0\n",
            "           Linear-13              [-1, 64, 256]          65,792\n",
            "          Dropout-14              [-1, 64, 256]               0\n",
            "        Attention-15              [-1, 64, 256]               0\n",
            "         Identity-16              [-1, 64, 256]               0\n",
            "        LayerNorm-17              [-1, 64, 256]             512\n",
            "           Linear-18              [-1, 64, 512]         131,584\n",
            "          Dropout-19              [-1, 64, 512]               0\n",
            "           Linear-20              [-1, 64, 256]         131,328\n",
            "          Dropout-21              [-1, 64, 256]               0\n",
            "         Identity-22              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-23              [-1, 64, 256]               0\n",
            "        LayerNorm-24              [-1, 64, 256]             512\n",
            "           Linear-25              [-1, 64, 768]         196,608\n",
            "          Dropout-26            [-1, 4, 64, 64]               0\n",
            "           Linear-27              [-1, 64, 256]          65,792\n",
            "          Dropout-28              [-1, 64, 256]               0\n",
            "        Attention-29              [-1, 64, 256]               0\n",
            "         DropPath-30              [-1, 64, 256]               0\n",
            "        LayerNorm-31              [-1, 64, 256]             512\n",
            "           Linear-32              [-1, 64, 512]         131,584\n",
            "          Dropout-33              [-1, 64, 512]               0\n",
            "           Linear-34              [-1, 64, 256]         131,328\n",
            "          Dropout-35              [-1, 64, 256]               0\n",
            "         DropPath-36              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-37              [-1, 64, 256]               0\n",
            "        LayerNorm-38              [-1, 64, 256]             512\n",
            "           Linear-39              [-1, 64, 768]         196,608\n",
            "          Dropout-40            [-1, 4, 64, 64]               0\n",
            "           Linear-41              [-1, 64, 256]          65,792\n",
            "          Dropout-42              [-1, 64, 256]               0\n",
            "        Attention-43              [-1, 64, 256]               0\n",
            "         DropPath-44              [-1, 64, 256]               0\n",
            "        LayerNorm-45              [-1, 64, 256]             512\n",
            "           Linear-46              [-1, 64, 512]         131,584\n",
            "          Dropout-47              [-1, 64, 512]               0\n",
            "           Linear-48              [-1, 64, 256]         131,328\n",
            "          Dropout-49              [-1, 64, 256]               0\n",
            "         DropPath-50              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-51              [-1, 64, 256]               0\n",
            "        LayerNorm-52              [-1, 64, 256]             512\n",
            "           Linear-53              [-1, 64, 768]         196,608\n",
            "          Dropout-54            [-1, 4, 64, 64]               0\n",
            "           Linear-55              [-1, 64, 256]          65,792\n",
            "          Dropout-56              [-1, 64, 256]               0\n",
            "        Attention-57              [-1, 64, 256]               0\n",
            "         DropPath-58              [-1, 64, 256]               0\n",
            "        LayerNorm-59              [-1, 64, 256]             512\n",
            "           Linear-60              [-1, 64, 512]         131,584\n",
            "          Dropout-61              [-1, 64, 512]               0\n",
            "           Linear-62              [-1, 64, 256]         131,328\n",
            "          Dropout-63              [-1, 64, 256]               0\n",
            "         DropPath-64              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-65              [-1, 64, 256]               0\n",
            "        LayerNorm-66              [-1, 64, 256]             512\n",
            "           Linear-67              [-1, 64, 768]         196,608\n",
            "          Dropout-68            [-1, 4, 64, 64]               0\n",
            "           Linear-69              [-1, 64, 256]          65,792\n",
            "          Dropout-70              [-1, 64, 256]               0\n",
            "        Attention-71              [-1, 64, 256]               0\n",
            "         DropPath-72              [-1, 64, 256]               0\n",
            "        LayerNorm-73              [-1, 64, 256]             512\n",
            "           Linear-74              [-1, 64, 512]         131,584\n",
            "          Dropout-75              [-1, 64, 512]               0\n",
            "           Linear-76              [-1, 64, 256]         131,328\n",
            "          Dropout-77              [-1, 64, 256]               0\n",
            "         DropPath-78              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-79              [-1, 64, 256]               0\n",
            "        LayerNorm-80              [-1, 64, 256]             512\n",
            "           Linear-81              [-1, 64, 768]         196,608\n",
            "          Dropout-82            [-1, 4, 64, 64]               0\n",
            "           Linear-83              [-1, 64, 256]          65,792\n",
            "          Dropout-84              [-1, 64, 256]               0\n",
            "        Attention-85              [-1, 64, 256]               0\n",
            "         DropPath-86              [-1, 64, 256]               0\n",
            "        LayerNorm-87              [-1, 64, 256]             512\n",
            "           Linear-88              [-1, 64, 512]         131,584\n",
            "          Dropout-89              [-1, 64, 512]               0\n",
            "           Linear-90              [-1, 64, 256]         131,328\n",
            "          Dropout-91              [-1, 64, 256]               0\n",
            "         DropPath-92              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-93              [-1, 64, 256]               0\n",
            "        LayerNorm-94              [-1, 64, 256]             512\n",
            "           Linear-95                [-1, 64, 1]             257\n",
            "           Linear-96                  [-1, 100]          25,700\n",
            "TransformerClassifier-97                  [-1, 100]               0\n",
            "================================================================\n",
            "Total params: 3,333,669\n",
            "Trainable params: 3,333,669\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 16.25\n",
            "Params size (MB): 12.72\n",
            "Estimated Total Size (MB): 28.98\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "BaEQExMDWoEV",
        "outputId": "860c4f3b-6e87-489a-f4a9-b98157db7322"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n",
        "\n",
        "\n",
        "plt.plot([x for x in loss_train_arr], \"-bx\")\n",
        "plt.plot([x for x in loss_val_arr],\"-rx\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"train loss\",\"val loss\"])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc583f4c590>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8qPkn-aWs6O"
      },
      "source": [
        "def test(model, val_loader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        correct = 0\n",
        "        bs = batch_size\n",
        "        result = []\n",
        "        check_names = []\n",
        "        for i, (data, target) in enumerate(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            arr = pred.data.cpu().numpy()\n",
        "            for j in range(pred.size()[0]):\n",
        "                file_name = val_dataset.samples[i*bs+j][0].split('/')[-1]\n",
        "                result.append((file_name,pred[j].cpu().numpy()[0])) \n",
        "        \n",
        "    with open ('ID_result.csv','w') as f:\n",
        "        f.write('Id,Category\\n')\n",
        "        for data in result:\n",
        "            f.write(data[0]+','+str(data[1])+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLqrhZjDWp4x",
        "outputId": "5ea608a3-12c3-4eae-d50a-1da5242dac1e"
      },
      "source": [
        "result = test(model, val_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}