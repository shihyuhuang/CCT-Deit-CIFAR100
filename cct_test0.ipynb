{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cct_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-z3hMKqFKPD",
        "outputId": "d599c001-663b-4874-878f-ff815ae0e709"
      },
      "source": [
        "import argparse\n",
        "from time import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.datasets import ImageFolder, CIFAR100\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "\n",
        "od.download('https://www.kaggle.com/c/2021-ai-training-final-project/data')\n",
        "data_dir = './2021-ai-training-final-project/CIFAR100'"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.7/dist-packages (0.1.20)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.62.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.5.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Skipping, found downloaded files in \"./2021-ai-training-final-project\" (use force=True to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72RxBFQxLhp4",
        "outputId": "0d7eeb82-203a-4583-acbe-d1513ba9e1eb"
      },
      "source": [
        "!pip install torchtoolbox"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtoolbox in /usr/local/lib/python3.7/dist-packages (0.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.62.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.22.2.post1)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.99)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.15.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torchtoolbox) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKBF0_XGN4B4",
        "outputId": "2eb00bc2-7079-45f2-9714-ecd0124268f1"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "device = get_default_device()\n",
        "print(device)\n",
        "!nvidia-smi"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYiZtmuvFNN7"
      },
      "source": [
        "\n",
        "# Data args\n",
        "dataset='cifar100'\n",
        "workers=4\n",
        "print_freq=10\n",
        "checkpoint_path='cct6-3x2_cifar100_best.pth'\n",
        "\n",
        "# Optimization hyperparams\n",
        "epochs=1\n",
        "warmup=5\n",
        "batch_size=128\n",
        "lr=0.0005\n",
        "weight_decay=3e-2\n",
        "clip_grad_norm=0.\n",
        "model='cct_6'\n",
        "positional_embedding='learnable'\n",
        "conv_layers=2\n",
        "conv_size=3\n",
        "patch_size=4\n",
        "gpu_id=0\n",
        "no_cuda=False\n",
        "\n",
        "\n",
        "def accuracy(output, target):\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(1, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        correct_k = correct[:1].flatten().float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def cls_validate(val_loader, model, history, time_begin=None):\n",
        "    model.eval()\n",
        "    acc1_val = 0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            if (not no_cuda) and torch.cuda.is_available():\n",
        "                images = images.cuda(gpu_id, non_blocking=True)\n",
        "                target = target.cuda(gpu_id, non_blocking=True)\n",
        "\n",
        "            output = model(images)\n",
        "\n",
        "            acc1 = accuracy(output, target)\n",
        "            n += images.size(0)\n",
        "            acc1_val += float(acc1[0] * images.size(0))\n",
        "\n",
        "            if print_freq >= 0 and i % print_freq == 0:\n",
        "                avg_acc1 = (acc1_val / n)\n",
        "                print(f'[Eval][{i}] \\t Top-1 {avg_acc1:6.2f}')\n",
        "                history.append(avg_acc1)\n",
        "\n",
        "    avg_acc1 = (acc1_val / n)\n",
        "    total_mins = -1 if time_begin is None else (time() - time_begin) / 60\n",
        "    print(f'[Final]\\t \\t Top-1 {avg_acc1:6.2f} \\t \\t Time: {total_mins:.2f}')\n",
        "\n",
        "    return avg_acc1, history"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umLtwKxtFcNJ"
      },
      "source": [
        "#src/cct.py \n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 embedding_dim=768,\n",
        "                 n_input_channels=3,\n",
        "                 n_conv_layers=1,\n",
        "                 kernel_size=7,\n",
        "                 stride=2,\n",
        "                 padding=3,\n",
        "                 pooling_kernel_size=3,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(CCT, self).__init__()\n",
        "\n",
        "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
        "                                   n_output_channels=embedding_dim,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   stride=stride,\n",
        "                                   padding=padding,\n",
        "                                   pooling_kernel_size=pooling_kernel_size,\n",
        "                                   pooling_stride=pooling_stride,\n",
        "                                   pooling_padding=pooling_padding,\n",
        "                                   max_pool=True,\n",
        "                                   activation=nn.ReLU,\n",
        "                                   n_conv_layers=n_conv_layers,\n",
        "                                   conv_bias=False)\n",
        "\n",
        "        self.classifier = TransformerClassifier(\n",
        "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
        "                                                           height=img_size,\n",
        "                                                           width=img_size),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout_rate=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenizer(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "\n",
        "def _cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "         kernel_size=3, stride=None, padding=None,\n",
        "         *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "    return CCT(num_layers=num_layers,\n",
        "               num_heads=num_heads,\n",
        "               mlp_ratio=mlp_ratio,\n",
        "               embedding_dim=embedding_dim,\n",
        "               kernel_size=kernel_size,\n",
        "               stride=stride,\n",
        "               padding=padding,\n",
        "               *args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def cct_6(*args, **kwargs):\n",
        "    return _cct(num_layers=6, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDIaxmdOItPK"
      },
      "source": [
        "#/src/utils/tokenizer.py \n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 n_conv_layers=1,\n",
        "                 n_input_channels=3,\n",
        "                 n_output_channels=64,\n",
        "                 in_planes=64,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 conv_bias=False):\n",
        "        super(Tokenizer, self).__init__()\n",
        "\n",
        "        n_filter_list = [n_input_channels] + \\\n",
        "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
        "                        [n_output_channels]\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
        "                          kernel_size=(kernel_size, kernel_size),\n",
        "                          stride=(stride, stride),\n",
        "                          padding=(padding, padding), bias=conv_bias),\n",
        "                nn.Identity() if activation is None else activation(),\n",
        "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
        "                             stride=pooling_stride,\n",
        "                             padding=pooling_padding) if max_pool else nn.Identity()\n",
        "            )\n",
        "                for i in range(n_conv_layers)\n",
        "            ])\n",
        "\n",
        "        self.flattener = nn.Flatten(2, 3)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
        "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.flattener(self.conv_layers(x)).transpose(-2, -1)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "\n",
        "class TextTokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 embedding_dim=300,\n",
        "                 n_output_channels=128,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 *args, **kwargs):\n",
        "        super(TextTokenizer, self).__init__()\n",
        "\n",
        "        self.max_pool = max_pool\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, n_output_channels,\n",
        "                      kernel_size=(kernel_size, embedding_dim),\n",
        "                      stride=(stride, 1),\n",
        "                      padding=(padding, 0), bias=False),\n",
        "            nn.Identity() if activation is None else activation(),\n",
        "            nn.MaxPool2d(\n",
        "                kernel_size=(pooling_kernel_size, 1),\n",
        "                stride=(pooling_stride, 1),\n",
        "                padding=(pooling_padding, 0)\n",
        "            ) if max_pool else nn.Identity()\n",
        "        )\n",
        "\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def seq_len(self, seq_len=32, embed_dim=300):\n",
        "        return self.forward(torch.zeros((1, seq_len, embed_dim)))[0].shape[1]\n",
        "\n",
        "    def forward_mask(self, mask):\n",
        "        new_mask = mask.unsqueeze(1).float()\n",
        "        cnn_weight = torch.ones(\n",
        "            (1, 1, self.conv_layers[0].kernel_size[0]),\n",
        "            device=mask.device,\n",
        "            dtype=torch.float)\n",
        "        new_mask = F.conv1d(\n",
        "            new_mask, cnn_weight, None,\n",
        "            self.conv_layers[0].stride[0], self.conv_layers[0].padding[0], 1, 1)\n",
        "        if self.max_pool:\n",
        "            new_mask = F.max_pool1d(\n",
        "                new_mask, self.conv_layers[2].kernel_size[0],\n",
        "                self.conv_layers[2].stride[0], self.conv_layers[2].padding[0], 1, False, False)\n",
        "        new_mask = new_mask.squeeze(1)\n",
        "        new_mask = (new_mask > 0)\n",
        "        return new_mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.transpose(1, 3).squeeze(1)\n",
        "        x = x if mask is None else x * self.forward_mask(mask).unsqueeze(-1).float()\n",
        "        return x, mask\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqbChR6LI1fc"
      },
      "source": [
        "#src/utils/transformers.py \n",
        "import torch\n",
        "from torch.nn import Module, ModuleList, Linear, Dropout, LayerNorm, Identity, Parameter, init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Attention(Module):\n",
        "    \"\"\"\n",
        "    Obtained from timm: github.com:rwightman/pytorch-image-models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MaskedAttention(Module):\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // self.num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = Dropout(attention_dropout)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask_value = -torch.finfo(attn.dtype).max\n",
        "            assert mask.shape[-1] == attn.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
        "            attn.masked_fill_(~mask, mask_value)\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n",
        "                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "class MaskedTransformerEncoderLayer(Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(MaskedTransformerEncoderLayer, self).__init__()\n",
        "        self.pre_norm = LayerNorm(d_model)\n",
        "        self.self_attn = MaskedAttention(dim=d_model, num_heads=nhead,\n",
        "                                         attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, mask=None, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src), mask))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 sequence_length=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            sequence_length += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                    dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        return pe.unsqueeze(0)\n",
        "\n",
        "\n",
        "class MaskedTransformerClassifier(Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 seq_len=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert seq_len is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            seq_len += 1\n",
        "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                seq_len += 1  # padding idx\n",
        "                self.positional_emb = Parameter(torch.zeros(1, seq_len, embedding_dim),\n",
        "                                                requires_grad=True)\n",
        "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = Parameter(self.sinusoidal_embedding(seq_len,\n",
        "                                                                          embedding_dim,\n",
        "                                                                          padding_idx=True),\n",
        "                                                requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = ModuleList([\n",
        "            MaskedTransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                          dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                          attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        if self.positional_emb is None and x.size(1) < self.seq_len:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "            if mask is not None:\n",
        "                mask = torch.cat([torch.ones(size=(mask.shape[0], 1), device=mask.device), mask.float()], dim=1)\n",
        "                mask = (mask > 0)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, mask=mask)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, Linear):\n",
        "            init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, Linear) and m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm):\n",
        "            init.constant_(m.bias, 0)\n",
        "            init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim, padding_idx=False):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        pe = pe.unsqueeze(0)\n",
        "        if padding_idx:\n",
        "            return torch.cat([torch.zeros((1, 1, dim)), pe], dim=1)\n",
        "        return pe"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGSucwvUI6ne"
      },
      "source": [
        "#utils/stochastic_depth.py \n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQKcBO0LKNHA"
      },
      "source": [
        "def test(model, val_loader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        correct = 0\n",
        "        bs = batch_size\n",
        "        result = []\n",
        "        check_names = []\n",
        "        for i, (data, target) in enumerate(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            arr = pred.data.cpu().numpy()\n",
        "            for j in range(pred.size()[0]):\n",
        "                file_name = val_dataset.samples[i*bs+j][0].split('/')[-1]\n",
        "                result.append((file_name,pred[j].cpu().numpy()[0])) \n",
        "        \n",
        "    with open ('ID_result.csv','w') as f:\n",
        "        f.write('Id,Category\\n')\n",
        "        for data in result:\n",
        "            f.write(data[0]+','+str(data[1])+'\\n')"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcqrNaZjNUyR",
        "outputId": "816ff732-8362-4bd9-b984-ff2dab509b06"
      },
      "source": [
        "img_size = 32\n",
        "num_classes = 100\n",
        "img_mean, img_std = [0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761]\n",
        "\n",
        "\n",
        "model = cct_6(img_size=img_size,\n",
        "              num_classes=num_classes,\n",
        "              positional_embedding=positional_embedding,\n",
        "              n_conv_layers=conv_layers,\n",
        "              kernel_size=conv_size,\n",
        "              patch_size=patch_size)\n",
        "\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
        "print(\"Loaded checkpoint.\")\n",
        "\n",
        "normalize = [transforms.Normalize(mean=img_mean, std=img_std)]\n",
        "\n",
        "if (not no_cuda) and torch.cuda.is_available():\n",
        "    torch.cuda.set_device(gpu_id)\n",
        "    model.cuda(gpu_id)\n",
        "\n",
        "val_dataset = ImageFolder(root=data_dir+\"/TEST\", \n",
        "            transform=transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            *normalize,\n",
        "        ]))\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size, shuffle=True,\n",
        "        num_workers=workers)\n",
        "\n",
        "print(\"Beginning evaluation\")\n",
        "time_begin = time()\n",
        "history = []\n",
        "\n",
        "acc1, history = cls_validate(val_loader, model, history, time_begin=time_begin)\n",
        "\n",
        "total_mins = (time() - time_begin) / 60\n",
        "print(f'Script finished in {total_mins:.2f} minutes, '\n",
        "      f'final top-1: {acc1:.2f}')\n",
        "    \n"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded checkpoint.\n",
            "Beginning evaluation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Eval][0] \t Top-1  80.47\n",
            "[Eval][10] \t Top-1  73.08\n",
            "[Eval][20] \t Top-1  75.07\n",
            "[Eval][30] \t Top-1  74.92\n",
            "[Eval][40] \t Top-1  75.06\n",
            "[Eval][50] \t Top-1  74.86\n",
            "[Eval][60] \t Top-1  74.76\n",
            "[Eval][70] \t Top-1  74.72\n",
            "[Final]\t \t Top-1  74.48 \t \t Time: 1.42\n",
            "Script finished in 1.42 minutes, final top-1: 74.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT146XYyJmL4",
        "outputId": "c527a92b-fa6c-46e4-9d0e-ab2cf9608c53"
      },
      "source": [
        "# Print model\n",
        "print(model)\n",
        "\n",
        "# Print parameter\n",
        "size = summary(model, (3, 32, 32))"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CCT(\n",
            "  (tokenizer): Tokenizer(\n",
            "    (conv_layers): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): ReLU()\n",
            "        (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): ReLU()\n",
            "        (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "    )\n",
            "    (flattener): Flatten(start_dim=2, end_dim=3)\n",
            "  )\n",
            "  (classifier): TransformerClassifier(\n",
            "    (attention_pool): Linear(in_features=256, out_features=1, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "    (blocks): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): Attention(\n",
            "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (dropout1): Dropout(p=0.0, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (fc): Linear(in_features=256, out_features=100, bias=True)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "              ReLU-2           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-3           [-1, 64, 16, 16]               0\n",
            "            Conv2d-4          [-1, 256, 16, 16]         147,456\n",
            "              ReLU-5          [-1, 256, 16, 16]               0\n",
            "         MaxPool2d-6            [-1, 256, 8, 8]               0\n",
            "           Flatten-7              [-1, 256, 64]               0\n",
            "         Tokenizer-8              [-1, 64, 256]               0\n",
            "           Dropout-9              [-1, 64, 256]               0\n",
            "        LayerNorm-10              [-1, 64, 256]             512\n",
            "           Linear-11              [-1, 64, 768]         196,608\n",
            "          Dropout-12            [-1, 4, 64, 64]               0\n",
            "           Linear-13              [-1, 64, 256]          65,792\n",
            "          Dropout-14              [-1, 64, 256]               0\n",
            "        Attention-15              [-1, 64, 256]               0\n",
            "         Identity-16              [-1, 64, 256]               0\n",
            "        LayerNorm-17              [-1, 64, 256]             512\n",
            "           Linear-18              [-1, 64, 512]         131,584\n",
            "          Dropout-19              [-1, 64, 512]               0\n",
            "           Linear-20              [-1, 64, 256]         131,328\n",
            "          Dropout-21              [-1, 64, 256]               0\n",
            "         Identity-22              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-23              [-1, 64, 256]               0\n",
            "        LayerNorm-24              [-1, 64, 256]             512\n",
            "           Linear-25              [-1, 64, 768]         196,608\n",
            "          Dropout-26            [-1, 4, 64, 64]               0\n",
            "           Linear-27              [-1, 64, 256]          65,792\n",
            "          Dropout-28              [-1, 64, 256]               0\n",
            "        Attention-29              [-1, 64, 256]               0\n",
            "         DropPath-30              [-1, 64, 256]               0\n",
            "        LayerNorm-31              [-1, 64, 256]             512\n",
            "           Linear-32              [-1, 64, 512]         131,584\n",
            "          Dropout-33              [-1, 64, 512]               0\n",
            "           Linear-34              [-1, 64, 256]         131,328\n",
            "          Dropout-35              [-1, 64, 256]               0\n",
            "         DropPath-36              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-37              [-1, 64, 256]               0\n",
            "        LayerNorm-38              [-1, 64, 256]             512\n",
            "           Linear-39              [-1, 64, 768]         196,608\n",
            "          Dropout-40            [-1, 4, 64, 64]               0\n",
            "           Linear-41              [-1, 64, 256]          65,792\n",
            "          Dropout-42              [-1, 64, 256]               0\n",
            "        Attention-43              [-1, 64, 256]               0\n",
            "         DropPath-44              [-1, 64, 256]               0\n",
            "        LayerNorm-45              [-1, 64, 256]             512\n",
            "           Linear-46              [-1, 64, 512]         131,584\n",
            "          Dropout-47              [-1, 64, 512]               0\n",
            "           Linear-48              [-1, 64, 256]         131,328\n",
            "          Dropout-49              [-1, 64, 256]               0\n",
            "         DropPath-50              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-51              [-1, 64, 256]               0\n",
            "        LayerNorm-52              [-1, 64, 256]             512\n",
            "           Linear-53              [-1, 64, 768]         196,608\n",
            "          Dropout-54            [-1, 4, 64, 64]               0\n",
            "           Linear-55              [-1, 64, 256]          65,792\n",
            "          Dropout-56              [-1, 64, 256]               0\n",
            "        Attention-57              [-1, 64, 256]               0\n",
            "         DropPath-58              [-1, 64, 256]               0\n",
            "        LayerNorm-59              [-1, 64, 256]             512\n",
            "           Linear-60              [-1, 64, 512]         131,584\n",
            "          Dropout-61              [-1, 64, 512]               0\n",
            "           Linear-62              [-1, 64, 256]         131,328\n",
            "          Dropout-63              [-1, 64, 256]               0\n",
            "         DropPath-64              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-65              [-1, 64, 256]               0\n",
            "        LayerNorm-66              [-1, 64, 256]             512\n",
            "           Linear-67              [-1, 64, 768]         196,608\n",
            "          Dropout-68            [-1, 4, 64, 64]               0\n",
            "           Linear-69              [-1, 64, 256]          65,792\n",
            "          Dropout-70              [-1, 64, 256]               0\n",
            "        Attention-71              [-1, 64, 256]               0\n",
            "         DropPath-72              [-1, 64, 256]               0\n",
            "        LayerNorm-73              [-1, 64, 256]             512\n",
            "           Linear-74              [-1, 64, 512]         131,584\n",
            "          Dropout-75              [-1, 64, 512]               0\n",
            "           Linear-76              [-1, 64, 256]         131,328\n",
            "          Dropout-77              [-1, 64, 256]               0\n",
            "         DropPath-78              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-79              [-1, 64, 256]               0\n",
            "        LayerNorm-80              [-1, 64, 256]             512\n",
            "           Linear-81              [-1, 64, 768]         196,608\n",
            "          Dropout-82            [-1, 4, 64, 64]               0\n",
            "           Linear-83              [-1, 64, 256]          65,792\n",
            "          Dropout-84              [-1, 64, 256]               0\n",
            "        Attention-85              [-1, 64, 256]               0\n",
            "         DropPath-86              [-1, 64, 256]               0\n",
            "        LayerNorm-87              [-1, 64, 256]             512\n",
            "           Linear-88              [-1, 64, 512]         131,584\n",
            "          Dropout-89              [-1, 64, 512]               0\n",
            "           Linear-90              [-1, 64, 256]         131,328\n",
            "          Dropout-91              [-1, 64, 256]               0\n",
            "         DropPath-92              [-1, 64, 256]               0\n",
            "TransformerEncoderLayer-93              [-1, 64, 256]               0\n",
            "        LayerNorm-94              [-1, 64, 256]             512\n",
            "           Linear-95                [-1, 64, 1]             257\n",
            "           Linear-96                  [-1, 100]          25,700\n",
            "TransformerClassifier-97                  [-1, 100]               0\n",
            "================================================================\n",
            "Total params: 3,333,669\n",
            "Trainable params: 3,333,669\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 16.25\n",
            "Params size (MB): 12.72\n",
            "Estimated Total Size (MB): 28.98\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "P0DK63rSKG6t",
        "outputId": "79028d6d-2be5-467a-e2af-2e85d208e0a7"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n",
        "\n",
        "def plot_loss(history):\n",
        "    plt.plot([x for x in history],\"-rx\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend([\"val loss\"])\n",
        "\n",
        "\n",
        "plot_loss(history)\n"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhRzC4vsQ3hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1a5f9a-b5cb-4069-dc94-c6c8bb89dacf"
      },
      "source": [
        "result = test(model, val_loader)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}