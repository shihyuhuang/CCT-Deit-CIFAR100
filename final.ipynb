{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxdwi7bDf7M9"
      },
      "source": [
        "# 7/25更新\n",
        "- 可以使用pretrain weight嗎? Ans: No\n",
        "- 一定要使用Resnet18 model嗎? Ans: 不一定，可以使用其他model\n",
        "\n",
        "# 常見問題：\n",
        "- 沒有GPU或其他環境問題：請你使用 colab\n",
        "- Train 很慢：請你檢查 get_device() 看有沒有成功使用到 GPU\n",
        "- 被 colab 限制 GPU 用量：好像使用超過12小時會被 google 限制用量，\n",
        "請你開其他 google 帳號，或是等個幾小時再繼續\n",
        "- dimension 對不起來：請你檢查 tensor.shape\n",
        "- dataset permission denied：google 有流量限制，請你嘗試其他下載連結"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VU_bea_h8Yf"
      },
      "source": [
        "# Install & import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvXSh2mXVZuv",
        "outputId": "422a2802-fff8-4bc0-96f6-b71860845558"
      },
      "source": [
        "!pip install torchtoolbox"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtoolbox\n",
            "  Downloading torchtoolbox-0.1.5-py3-none-any.whl (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 40 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.99)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.62.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torchtoolbox) (1.0.1)\n",
            "Installing collected packages: torchtoolbox\n",
            "Successfully installed torchtoolbox-0.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onvE_O-BiBWu"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder, CIFAR100\n",
        "from torchtoolbox.tools import summary\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as tt\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxfwwt_Hhhy0"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbF56ImQhWKn",
        "outputId": "653fed5f-6754-4f03-f2da-4bb01b62e9b9"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "device = get_default_device()\n",
        "print(device)\n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Tue Aug 24 10:01:36 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8    32W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWDyhi0afvQI"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2WIagHuQgxE",
        "outputId": "87ed0f98-06eb-4a8b-e809-6960d16516f2"
      },
      "source": [
        "!pip install opendatasets\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.20-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.62.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.5.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9YLruq3Qkqt",
        "outputId": "09861eb4-99b7-4851-9b0e-c41fe396fd10"
      },
      "source": [
        "import opendatasets as od\n",
        "\n",
        "od.download('https://www.kaggle.com/c/2021-ai-training-final-project/data')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: shihyuhuang\n",
            "Your Kaggle Key: ··········\n",
            "Downloading 2021-ai-training-final-project.zip to ./2021-ai-training-final-project\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 140M/140M [00:06<00:00, 23.4MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting archive ./2021-ai-training-final-project/2021-ai-training-final-project.zip to ./2021-ai-training-final-project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hrzny0pis-n"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl1GrQ_fQnHz",
        "outputId": "68ae2c58-3572-4c0a-9602-9e7a288a7f84"
      },
      "source": [
        "data_dir = './2021-ai-training-final-project/CIFAR100'\n",
        "classes = os.listdir(data_dir + \"/TRAIN\")\n",
        "print(classes)\n",
        "print(len(classes))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['mouse', 'oak_tree', 'camel', 'raccoon', 'snake', 'seal', 'crab', 'boy', 'crocodile', 'squirrel', 'tractor', 'man', 'possum', 'rose', 'willow_tree', 'keyboard', 'lamp', 'apple', 'poppy', 'caterpillar', 'wardrobe', 'bed', 'cloud', 'fox', 'flatfish', 'bicycle', 'sea', 'wolf', 'pear', 'table', 'tulip', 'dinosaur', 'mushroom', 'rabbit', 'clock', 'mountain', 'palm_tree', 'train', 'beaver', 'dolphin', 'shrew', 'whale', 'streetcar', 'bridge', 'castle', 'bear', 'lawn_mower', 'cup', 'couch', 'cockroach', 'tank', 'otter', 'woman', 'tiger', 'pine_tree', 'house', 'turtle', 'shark', 'road', 'sunflower', 'pickup_truck', 'leopard', 'rocket', 'skyscraper', 'bus', 'orchid', 'hamster', 'butterfly', 'trout', 'orange', 'maple_tree', 'television', 'skunk', 'girl', 'bottle', 'lion', 'telephone', 'plate', 'bowl', 'sweet_pepper', 'plain', 'spider', 'worm', 'can', 'motorcycle', 'bee', 'chimpanzee', 'lizard', 'snail', 'beetle', 'chair', 'forest', 'kangaroo', 'lobster', 'elephant', 'ray', 'cattle', 'porcupine', 'baby', 'aquarium_fish']\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmRWOIGAQw26"
      },
      "source": [
        "# Data transforms (normalization & data augmentation)\n",
        "normalize = tt.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
        "train_tfms = tt.Compose([\n",
        "        tt.RandomCrop(32, padding=4),\n",
        "        tt.RandomHorizontalFlip(),\n",
        "        tt.ToTensor(),\n",
        "        normalize,\n",
        "])\n",
        "\n",
        "valid_tfms = tt.Compose([\n",
        "        tt.ToTensor(),\n",
        "        normalize,\n",
        "])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le19U-sEQ1DI"
      },
      "source": [
        "# PyTorch datasets\n",
        "train_ds = ImageFolder(root=data_dir+\"/TRAIN\",transform=train_tfms)\n",
        "valid_ds = ImageFolder(root=data_dir+\"/TEST\",transform=valid_tfms)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwCoTdSSu17h"
      },
      "source": [
        "# PyTorch data loaders\n",
        "BATCH_SIZE=64\n",
        "train_dl = DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True, num_workers=2)\n",
        "valid_dl = DataLoader(valid_ds,batch_size=64, shuffle=False, num_workers=2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WscSxO5sx4Kx"
      },
      "source": [
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShFAAMhtx3sG"
      },
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "valid_dl = DeviceDataLoader(valid_dl, device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2h1It5lkY4p"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P96I9DBlAdR"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    if epoch < 100:\n",
        "        lrr = lr\n",
        "    elif epoch < 150:\n",
        "        lrr = lr * 0.1\n",
        "    else:\n",
        "        lrr = lr * 0.01\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lrr\n",
        "\n",
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self,batch):\n",
        "        images,labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out,labels)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self,batch):\n",
        "        images,labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out,labels)\n",
        "        acc = accuracy(out,labels)\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "    \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
        "        "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKoC39FKRG5Q"
      },
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "                in_chans,\n",
        "                embed_dim,\n",
        "                kernel_size=patch_size,\n",
        "                stride=patch_size,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(\n",
        "                x\n",
        "            )  # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
        "        x = x.flatten(2)  # (n_samples, embed_dim, n_patches)\n",
        "        x = x.transpose(1, 2)  # (n_samples, n_patches, embed_dim)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.dim = dim\n",
        "        self.head_dim = dim // n_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_p)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n_samples, n_tokens, dim = x.shape\n",
        "\n",
        "        if dim != self.dim:\n",
        "            raise ValueError\n",
        "\n",
        "        qkv = self.qkv(x)  # (n_samples, n_patches + 1, 3 * dim)\n",
        "        qkv = qkv.reshape(\n",
        "                n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
        "        )  # (n_smaples, n_patches + 1, 3, n_heads, head_dim)\n",
        "        qkv = qkv.permute(\n",
        "                2, 0, 3, 1, 4\n",
        "        )  # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
        "\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        k_t = k.transpose(-2, -1)  # (n_samples, n_heads, head_dim, n_patches + 1)\n",
        "        dp = (\n",
        "           q @ k_t\n",
        "        ) * self.scale # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "        attn = dp.softmax(dim=-1)  # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        weighted_avg = attn @ v  # (n_samples, n_heads, n_patches +1, head_dim)\n",
        "        weighted_avg = weighted_avg.transpose(\n",
        "                1, 2\n",
        "        )  # (n_samples, n_patches + 1, n_heads, head_dim)\n",
        "        weighted_avg = weighted_avg.flatten(2)  # (n_samples, n_patches + 1, dim)\n",
        "\n",
        "        x = self.proj(weighted_avg)  # (n_samples, n_patches + 1, dim)\n",
        "        x = self.proj_drop(x)  # (n_samples, n_patches + 1, dim)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(\n",
        "                x\n",
        "        ) # (n_samples, n_patches + 1, hidden_features)\n",
        "        x = self.act(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "        x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "        x = self.fc2(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "        x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
        "        self.attn = Attention(\n",
        "                dim,\n",
        "                n_heads=n_heads,\n",
        "                qkv_bias=qkv_bias,\n",
        "                attn_p=attn_p,\n",
        "                proj_p=p\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
        "        hidden_features = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(\n",
        "                in_features=dim,\n",
        "                hidden_features=hidden_features,\n",
        "                out_features=dim,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(ImageClassificationBase):\n",
        "    def __init__(\n",
        "            self,\n",
        "            img_size=32,\n",
        "            patch_size=32,\n",
        "            in_chans=3,\n",
        "            n_classes=100,\n",
        "            embed_dim=768,\n",
        "            depth=12,\n",
        "            n_heads=12,\n",
        "            mlp_ratio=4.,\n",
        "            qkv_bias=True,\n",
        "            p=0.,\n",
        "            attn_p=0.,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbed(\n",
        "                img_size=img_size,\n",
        "                patch_size=patch_size,\n",
        "                in_chans=in_chans,\n",
        "                embed_dim=embed_dim,\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(\n",
        "                torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n",
        "        )\n",
        "        self.pos_drop = nn.Dropout(p=p)\n",
        "\n",
        "        self.Transformer = nn.ModuleList(\n",
        "            [\n",
        "                Transformer(\n",
        "                    dim=embed_dim,\n",
        "                    n_heads=n_heads,\n",
        "                    mlp_ratio=mlp_ratio,\n",
        "                    qkv_bias=qkv_bias,\n",
        "                    p=p,\n",
        "                    attn_p=attn_p,\n",
        "                )\n",
        "                for _ in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.head = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        n_samples = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_token = self.cls_token.expand(\n",
        "                n_samples, -1, -1\n",
        "        )  # (n_samples, 1, embed_dim)\n",
        "        x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1 + n_patches, embed_dim)\n",
        "        x = x + self.pos_embed  # (n_samples, 1 + n_patches, embed_dim)\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for Transformer in self.Transformer:\n",
        "            x = Transformer(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        cls_token_final = x[:, 0]  # just the CLS token\n",
        "        x = self.head(cls_token_final)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9KSsamal8qn"
      },
      "source": [
        "model = ViT().to(device)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhBKbSoHlwD5"
      },
      "source": [
        "# Set Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RRcofFsRgwN"
      },
      "source": [
        "epochs=200\n",
        "lr=0.1\n",
        "momentum=0.9\n",
        "weight_decay=1e-4\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, weight_decay=weight_decay)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEWNYsSImNhA"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m84S0Bk-mSt2"
      },
      "source": [
        "def Train (epochs,train_dl,valid_dl,model,optimizer):\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    history = []\n",
        "        \n",
        "    for epoch in range(epochs):\n",
        "        adjust_learning_rate(optimizer, epoch)\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        lrs = []\n",
        "        \n",
        "        for batch in train_dl:\n",
        "            loss = model.training_step(batch)\n",
        "            train_loss.append(loss)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lrs.append(get_lr(optimizer))\n",
        "   \n",
        "        result = evaluate(model,valid_dl)\n",
        "        result[\"train_loss\"] = torch.stack(train_loss).mean().item()\n",
        "        result[\"lrs\"] = lrs\n",
        "        \n",
        "        model.epoch_end(epoch,result)\n",
        "        history.append(result)\n",
        "        \n",
        "    return history\n",
        "            \n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model,valid_dl):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in valid_dl]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uNEAlOh1RkPS",
        "outputId": "3b54bce6-bab8-4d30-acb2-1b0185e6fb77"
      },
      "source": [
        "%%time\n",
        "history = []\n",
        "history += Train(epochs=epochs,train_dl=train_dl,valid_dl=valid_dl,model=model,optimizer=optimizer)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], last_lr: 0.10000, train_loss: 5.5023, val_loss: 4.5672, val_acc: 0.0231\n",
            "Epoch [1], last_lr: 0.10000, train_loss: 4.4231, val_loss: 4.4221, val_acc: 0.0397\n",
            "Epoch [2], last_lr: 0.10000, train_loss: 4.3256, val_loss: 4.3806, val_acc: 0.0451\n",
            "Epoch [3], last_lr: 0.10000, train_loss: 4.2360, val_loss: 4.3341, val_acc: 0.0509\n",
            "Epoch [4], last_lr: 0.10000, train_loss: 4.1606, val_loss: 4.2309, val_acc: 0.0522\n",
            "Epoch [5], last_lr: 0.10000, train_loss: 4.0910, val_loss: 4.1952, val_acc: 0.0738\n",
            "Epoch [6], last_lr: 0.10000, train_loss: 4.0338, val_loss: 4.1095, val_acc: 0.0703\n",
            "Epoch [7], last_lr: 0.10000, train_loss: 3.9983, val_loss: 4.1253, val_acc: 0.0770\n",
            "Epoch [8], last_lr: 0.10000, train_loss: 3.9491, val_loss: 4.0378, val_acc: 0.0864\n",
            "Epoch [9], last_lr: 0.10000, train_loss: 3.9039, val_loss: 3.9563, val_acc: 0.0970\n",
            "Epoch [10], last_lr: 0.10000, train_loss: 3.8609, val_loss: 3.9406, val_acc: 0.0983\n",
            "Epoch [11], last_lr: 0.10000, train_loss: 3.8281, val_loss: 3.8482, val_acc: 0.1111\n",
            "Epoch [12], last_lr: 0.10000, train_loss: 3.7789, val_loss: 3.8605, val_acc: 0.1149\n",
            "Epoch [13], last_lr: 0.10000, train_loss: 3.7279, val_loss: 3.7454, val_acc: 0.1219\n",
            "Epoch [14], last_lr: 0.10000, train_loss: 3.6955, val_loss: 3.6952, val_acc: 0.1303\n",
            "Epoch [15], last_lr: 0.10000, train_loss: 3.6617, val_loss: 3.6760, val_acc: 0.1406\n",
            "Epoch [16], last_lr: 0.10000, train_loss: 3.6289, val_loss: 3.5982, val_acc: 0.1510\n",
            "Epoch [17], last_lr: 0.10000, train_loss: 3.5896, val_loss: 3.5842, val_acc: 0.1520\n",
            "Epoch [18], last_lr: 0.10000, train_loss: 3.5650, val_loss: 3.5441, val_acc: 0.1617\n",
            "Epoch [19], last_lr: 0.10000, train_loss: 3.5376, val_loss: 3.5575, val_acc: 0.1603\n",
            "Epoch [20], last_lr: 0.10000, train_loss: 3.5140, val_loss: 3.4925, val_acc: 0.1657\n",
            "Epoch [21], last_lr: 0.10000, train_loss: 3.4757, val_loss: 3.4863, val_acc: 0.1714\n",
            "Epoch [22], last_lr: 0.10000, train_loss: 3.4687, val_loss: 3.4751, val_acc: 0.1752\n",
            "Epoch [23], last_lr: 0.10000, train_loss: 3.4451, val_loss: 3.3988, val_acc: 0.1805\n",
            "Epoch [24], last_lr: 0.10000, train_loss: 3.4202, val_loss: 3.3943, val_acc: 0.1896\n",
            "Epoch [25], last_lr: 0.10000, train_loss: 3.4052, val_loss: 3.3603, val_acc: 0.1867\n",
            "Epoch [26], last_lr: 0.10000, train_loss: 3.3893, val_loss: 3.3742, val_acc: 0.1911\n",
            "Epoch [27], last_lr: 0.10000, train_loss: 3.3706, val_loss: 3.3391, val_acc: 0.1974\n",
            "Epoch [28], last_lr: 0.10000, train_loss: 3.3516, val_loss: 3.3505, val_acc: 0.1946\n",
            "Epoch [29], last_lr: 0.10000, train_loss: 3.3367, val_loss: 3.3172, val_acc: 0.2002\n",
            "Epoch [30], last_lr: 0.10000, train_loss: 3.3245, val_loss: 3.3314, val_acc: 0.2051\n",
            "Epoch [31], last_lr: 0.10000, train_loss: 3.3055, val_loss: 3.2941, val_acc: 0.2049\n",
            "Epoch [32], last_lr: 0.10000, train_loss: 3.2924, val_loss: 3.2705, val_acc: 0.2163\n",
            "Epoch [33], last_lr: 0.10000, train_loss: 3.2801, val_loss: 3.2476, val_acc: 0.2188\n",
            "Epoch [34], last_lr: 0.10000, train_loss: 3.2650, val_loss: 3.2148, val_acc: 0.2206\n",
            "Epoch [35], last_lr: 0.10000, train_loss: 3.2508, val_loss: 3.2335, val_acc: 0.2157\n",
            "Epoch [36], last_lr: 0.10000, train_loss: 3.2389, val_loss: 3.2220, val_acc: 0.2214\n",
            "Epoch [37], last_lr: 0.10000, train_loss: 3.2230, val_loss: 3.2168, val_acc: 0.2234\n",
            "Epoch [38], last_lr: 0.10000, train_loss: 3.2142, val_loss: 3.2332, val_acc: 0.2211\n",
            "Epoch [39], last_lr: 0.10000, train_loss: 3.2049, val_loss: 3.1810, val_acc: 0.2307\n",
            "Epoch [40], last_lr: 0.10000, train_loss: 3.1890, val_loss: 3.1691, val_acc: 0.2289\n",
            "Epoch [41], last_lr: 0.10000, train_loss: 3.1742, val_loss: 3.1762, val_acc: 0.2327\n",
            "Epoch [42], last_lr: 0.10000, train_loss: 3.1629, val_loss: 3.1966, val_acc: 0.2304\n",
            "Epoch [43], last_lr: 0.10000, train_loss: 3.1511, val_loss: 3.1753, val_acc: 0.2315\n",
            "Epoch [44], last_lr: 0.10000, train_loss: 3.1367, val_loss: 3.1705, val_acc: 0.2335\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-55b6e71e823f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'history = []\\nhistory += Train(epochs=epochs,train_dl=train_dl,valid_dl=valid_dl,model=model,optimizer=optimizer)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-cde050755630>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(epochs, train_dl, valid_dl, model, optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-62a2be3a8bd2>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"\"\"Yield a batch of data after moving it to device\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkyW-fhrUZkb"
      },
      "source": [
        "# Model / Parameter statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ymMmI0TUvJP"
      },
      "source": [
        "# Print model\n",
        "print(model)\n",
        "\n",
        "# Print parameter\n",
        "net = ViT().to(device)\n",
        "input = torch.randn(1,3, 32, 32)\n",
        "input = input.to(device)\n",
        "summary(net, input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpLDHxZaR29V"
      },
      "source": [
        "# Plot Learning curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3Hya-vRSDCd"
      },
      "source": [
        "def plot_loss(history):\n",
        "    plt.plot([x.get(\"train_loss\") for x in history], \"-bx\")\n",
        "    plt.plot([x[\"val_loss\"] for x in history],\"-rx\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend([\"train loss\",\"val loss\"])\n",
        "\n",
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc0SOdZvmWeX"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlGEEjYcY2yf"
      },
      "source": [
        "def test(model,data_loader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        correct = 0\n",
        "        bs = 64\n",
        "        result = []\n",
        "        check_names = []\n",
        "        for i, (data, target) in enumerate(valid_dl):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            arr = pred.data.cpu().numpy()\n",
        "            for j in range(pred.size()[0]):\n",
        "                file_name = valid_ds.samples[i*bs+j][0].split('/')[-1]\n",
        "                result.append((file_name,pred[j].cpu().numpy()[0])) \n",
        "        \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Nsco9zZJX5"
      },
      "source": [
        "result = test(model, valid_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DiZ2EDCrFXP"
      },
      "source": [
        "# Save Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thLeLAP6rK-X"
      },
      "source": [
        "with open ('ID_result.csv','w') as f:\n",
        "    f.write('Id,Category\\n')\n",
        "    for data in result:\n",
        "        f.write(data[0]+','+str(data[1])+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}