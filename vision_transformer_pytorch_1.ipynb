{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vision-transformer-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2NJsj-wXdZI"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "from datetime import datetime\n",
        "import importlib\n",
        "import os\n",
        "import torch\n",
        "from tensorflow.io import gfile\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms import transforms"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRg_gOOLaVBi"
      },
      "source": [
        "exp_name=\"ft\"\n",
        "n_gpu=1\n",
        "tensorboard=True\n",
        "model_arch=\"b16\"\n",
        "checkpoint_path=\"imagenet21k+imagenet2012_ViT-B_16.pth\"\n",
        "image_size=384\n",
        "batch_size=32\n",
        "num_workers=4\n",
        "train_steps=10000\n",
        "lr=1e-3\n",
        "wd=1e-4\n",
        "warmup_steps=500\n",
        "data_dir='./data/'\n",
        "dataset='CIFAR100'\n",
        "num_classes=1000    \n",
        "patch_size = 16\n",
        "emb_dim = 768\n",
        "mlp_dim = 3072\n",
        "num_heads = 12\n",
        "num_layers = 12\n",
        "attn_dropout_rate = 0.0\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFJBe9ORbYSD"
      },
      "source": [
        "def ensure_dir(dirname):\n",
        "    dirname = Path(dirname)\n",
        "    if not dirname.is_dir():\n",
        "        dirname.mkdir(parents=True, exist_ok=False)\n",
        "\n",
        "\n",
        "def read_json(fname):\n",
        "    fname = Path(fname)\n",
        "    with fname.open('rt') as handle:\n",
        "        return json.load(handle, object_hook=OrderedDict)\n",
        "\n",
        "\n",
        "def write_json(content, fname):\n",
        "    fname = Path(fname)\n",
        "    with fname.open('wt') as handle:\n",
        "        json.dump(content, handle, indent=4, sort_keys=False)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k / batch_size * 100.0)\n",
        "    return res\n",
        "\n",
        "def setup_device(n_gpu_use):\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    if n_gpu_use > 0 and n_gpu == 0:\n",
        "        print(\"Warning: There\\'s no GPU available on this machine, training will be performed on CPU.\")\n",
        "        n_gpu_use = 0\n",
        "    if n_gpu_use > n_gpu:\n",
        "        print(\"Warning: The number of GPU\\'s configured to use is {}, but only {} are available on this machine.\".format(n_gpu_use, n_gpu))\n",
        "        n_gpu_use = n_gpu\n",
        "    device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n",
        "    list_ids = list(range(n_gpu_use))\n",
        "    return device, list_ids\n",
        "\n",
        "\n",
        "class MetricTracker:\n",
        "    def __init__(self, *keys, writer=None):\n",
        "        self.writer = writer\n",
        "        self._data = pd.DataFrame(index=keys, columns=['total', 'counts', 'average'])\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        for col in self._data.columns:\n",
        "            self._data[col].values[:] = 0\n",
        "\n",
        "    def update(self, key, value, n=1):\n",
        "        if self.writer is not None:\n",
        "            self.writer.add_scalar(key, value)\n",
        "        self._data.total[key] += value * n\n",
        "        self._data.counts[key] += n\n",
        "        self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
        "\n",
        "    def avg(self, key):\n",
        "        return self._data.average[key]\n",
        "\n",
        "    def result(self):\n",
        "        return dict(self._data.average)\n",
        "\n",
        "\n",
        "class TensorboardWriter():\n",
        "    def __init__(self, log_dir, enabled):\n",
        "        self.writer = None\n",
        "        self.selected_module = \"\"\n",
        "\n",
        "        if enabled:\n",
        "            log_dir = str(log_dir)\n",
        "\n",
        "            # Retrieve vizualization writer.\n",
        "            succeeded = False\n",
        "            for module in [\"torch.utils.tensorboard\", \"tensorboardX\"]:\n",
        "                try:\n",
        "                    self.writer = importlib.import_module(module).SummaryWriter(log_dir)\n",
        "                    succeeded = True\n",
        "                    break\n",
        "                except ImportError:\n",
        "                    succeeded = False\n",
        "                self.selected_module = module\n",
        "\n",
        "            if not succeeded:\n",
        "                message = \"Warning: visualization (Tensorboard) is configured to use, but currently not installed on \" \\\n",
        "                    \"this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to \" \\\n",
        "                    \"version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.\"\n",
        "                print(message)\n",
        "\n",
        "        self.step = 0\n",
        "        self.mode = ''\n",
        "\n",
        "        self.tb_writer_ftns = {\n",
        "            'add_scalar', 'add_scalars', 'add_image', 'add_images', 'add_audio',\n",
        "            'add_text', 'add_histogram', 'add_pr_curve', 'add_embedding'\n",
        "        }\n",
        "        self.tag_mode_exceptions = {'add_histogram', 'add_embedding'}\n",
        "        self.timer = datetime.now()\n",
        "\n",
        "    def set_step(self, step, mode='train'):\n",
        "        self.mode = mode\n",
        "        self.step = step\n",
        "        if step == 0:\n",
        "            self.timer = datetime.now()\n",
        "        else:\n",
        "            duration = datetime.now() - self.timer\n",
        "            self.add_scalar('steps_per_sec', 1 / duration.total_seconds())\n",
        "            self.timer = datetime.now()\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        \"\"\"\n",
        "        If visualization is configured to use:\n",
        "            return add_data() methods of tensorboard with additional information (step, tag) added.\n",
        "        Otherwise:\n",
        "            return a blank function handle that does nothing\n",
        "        \"\"\"\n",
        "        if name in self.tb_writer_ftns:\n",
        "            add_data = getattr(self.writer, name, None)\n",
        "\n",
        "            def wrapper(tag, data, *args, **kwargs):\n",
        "                if add_data is not None:\n",
        "                    # add mode(train/valid) tag\n",
        "                    if name not in self.tag_mode_exceptions:\n",
        "                        tag = '{}/{}'.format(tag, self.mode)\n",
        "                    if name == 'add_embedding':\n",
        "                        add_data(tag=tag, mat=data, global_step=self.step, *args, **kwargs)\n",
        "                    else:\n",
        "                        add_data(tag, data, self.step, *args, **kwargs)\n",
        "            return wrapper\n",
        "        else:\n",
        "            # default action for returning methods defined in this class, set_step() for instance.\n",
        "            try:\n",
        "                attr = object.__getattr__(name)\n",
        "            except AttributeError:\n",
        "                raise AttributeError(\"type object '{}' has no attribute '{}'\".format(self.selected_module, name))\n",
        "            return attr\n",
        "\n",
        "# add datetime postfix\n",
        "timestamp = datetime.now().strftime('%y%m%d_%H%M%S')\n",
        "exp_name = exp_name + '_{}_bs{}_lr{}_wd{}'.format(dataset, batch_size, lr, wd)\n",
        "exp_name += ('_' + timestamp)\n",
        "\n",
        "# create some important directories to be used for that experiments\n",
        "summary_dir = os.path.join('experiments', 'tb', exp_name)\n",
        "checkpoint_dir = os.path.join('experiments', 'save', exp_name, 'checkpoints/')\n",
        "result_dir = os.path.join('experiments', 'save', exp_name, 'results/')\n",
        "for dir in [summary_dir, checkpoint_dir, result_dir]:\n",
        "    ensure_dir(dir)\n",
        "\n",
        "# save config\n",
        "#write_json(vars(config), os.path.join('experiments', 'save', exp_name, 'config.json'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTYC3IFie0Ei"
      },
      "source": [
        "def load_checkpoint(path):\n",
        "    \"\"\" Load weights from a given checkpoint path in npz/pth \"\"\"\n",
        "    if path.endswith('npz'):\n",
        "        keys, values = load_jax(path)\n",
        "        state_dict = convert_jax_pytorch(keys, values)\n",
        "    elif path.endswith('pth'):\n",
        "        state_dict = torch.load(path)['state_dict']\n",
        "    else:\n",
        "        raise ValueError(\"checkpoint format {} not supported yet!\".format(path.split('.')[-1]))\n",
        "\n",
        "    return state_dict\n",
        "\n",
        "def load_jax(path):\n",
        "    \"\"\" Loads params from a npz checkpoint previously stored with `save()` in jax implemetation \"\"\"\n",
        "    with gfile.GFile(path, 'rb') as f:\n",
        "        ckpt_dict = np.load(f, allow_pickle=False)\n",
        "        keys, values = zip(*list(ckpt_dict.items()))\n",
        "    return keys, values\n",
        "\n",
        "\n",
        "def save_jax_to_pytorch(jax_path, save_path):\n",
        "    model_name = jax_path.split('/')[-1].split('.')[0]\n",
        "    keys, values = load_jax(jax_path)\n",
        "    state_dict = convert_jax_pytorch(keys, values)\n",
        "    checkpoint = {'state_dict': state_dict}\n",
        "    torch.save(checkpoint, os.path.join(save_path, model_name + '.pth'))\n",
        "\n",
        "\n",
        "def replace_names(names):\n",
        "    \"\"\" Replace jax model names with pytorch model names \"\"\"\n",
        "    new_names = []\n",
        "    for name in names:\n",
        "        if name == 'Transformer':\n",
        "            new_names.append('transformer')\n",
        "        elif name == 'encoder_norm':\n",
        "            new_names.append('norm')\n",
        "        elif 'encoderblock' in name:\n",
        "            num = name.split('_')[-1]\n",
        "            new_names.append('encoder_layers')\n",
        "            new_names.append(num)\n",
        "        elif 'LayerNorm' in name:\n",
        "            num = name.split('_')[-1]\n",
        "            if num == '0':\n",
        "                new_names.append('norm{}'.format(1))\n",
        "            elif num == '2':\n",
        "                new_names.append('norm{}'.format(2))\n",
        "        elif 'MlpBlock' in name:\n",
        "            new_names.append('mlp')\n",
        "        elif 'Dense' in name:\n",
        "            num = name.split('_')[-1]\n",
        "            new_names.append('fc{}'.format(int(num) + 1))\n",
        "        elif 'MultiHeadDotProductAttention' in name:\n",
        "            new_names.append('attn')\n",
        "        elif name == 'kernel' or name == 'scale':\n",
        "            new_names.append('weight')\n",
        "        elif name == 'bias':\n",
        "            new_names.append(name)\n",
        "        elif name == 'posembed_input':\n",
        "            new_names.append('pos_embedding')\n",
        "        elif name == 'pos_embedding':\n",
        "            new_names.append('pos_embedding')\n",
        "        elif name == 'embedding':\n",
        "            new_names.append('embedding')\n",
        "        elif name == 'head':\n",
        "            new_names.append('classifier')\n",
        "        elif name == 'cls':\n",
        "            new_names.append('cls_token')\n",
        "        else:\n",
        "            new_names.append(name)\n",
        "    return new_names\n",
        "\n",
        "\n",
        "def convert_jax_pytorch(keys, values):\n",
        "    \"\"\" Convert jax model parameters with pytorch model parameters \"\"\"\n",
        "    state_dict = {}\n",
        "    for key, value in zip(keys, values):\n",
        "\n",
        "        # convert name to torch names\n",
        "        names = key.split('/')\n",
        "        torch_names = replace_names(names)\n",
        "        torch_key = '.'.join(w for w in torch_names)\n",
        "\n",
        "        # convert values to tensor and check shapes\n",
        "        tensor_value = torch.tensor(value, dtype=torch.float)\n",
        "        # check shape\n",
        "        num_dim = len(tensor_value.shape)\n",
        "\n",
        "        if num_dim == 1:\n",
        "            tensor_value = tensor_value.squeeze()\n",
        "        elif num_dim == 2 and torch_names[-1] == 'weight':\n",
        "            # for normal weight, transpose it\n",
        "            tensor_value = tensor_value.T\n",
        "        elif num_dim == 3 and torch_names[-1] == 'weight' and torch_names[-2] in ['query', 'key', 'value']:\n",
        "            feat_dim, num_heads, head_dim = tensor_value.shape\n",
        "            # for multi head attention q/k/v weight\n",
        "            tensor_value = tensor_value\n",
        "        elif num_dim == 2 and torch_names[-1] == 'bias' and torch_names[-2] in ['query', 'key', 'value']:\n",
        "            # for multi head attention q/k/v bias\n",
        "            tensor_value = tensor_value\n",
        "        elif num_dim == 3 and torch_names[-1] == 'weight' and torch_names[-2] == 'out':\n",
        "            # for multi head attention out weight\n",
        "            tensor_value = tensor_value\n",
        "        elif num_dim == 4 and torch_names[-1] == 'weight':\n",
        "            tensor_value = tensor_value.permute(3, 2, 0, 1)\n",
        "\n",
        "        # print(\"{}: {}\".format(torch_key, tensor_value.shape))\n",
        "        state_dict[torch_key] = tensor_value\n",
        "    return state_dict\n",
        "\n",
        "\n",
        "#save_jax_to_pytorch('/Users/leon/Downloads/jax/imagenet21k+imagenet2012_ViT-B_16-224.npz', '/Users/leon/Downloads/pytorch')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYtXW4DLfhkp"
      },
      "source": [
        "class PositionEmbs(nn.Module):\n",
        "    def __init__(self, num_patches, emb_dim, dropout_rate=0.1):\n",
        "        super(PositionEmbs, self).__init__()\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))\n",
        "        if dropout_rate > 0:\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.pos_embedding\n",
        "\n",
        "        if self.dropout:\n",
        "            out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MlpBlock(nn.Module):\n",
        "    \"\"\" Transformer Feed-Forward Block \"\"\"\n",
        "    def __init__(self, in_dim, mlp_dim, out_dim, dropout_rate=0.1):\n",
        "        super(MlpBlock, self).__init__()\n",
        "\n",
        "        # init layers\n",
        "        self.fc1 = nn.Linear(in_dim, mlp_dim)\n",
        "        self.fc2 = nn.Linear(mlp_dim, out_dim)\n",
        "        self.act = nn.GELU()\n",
        "        if dropout_rate > 0.0:\n",
        "            self.dropout1 = nn.Dropout(dropout_rate)\n",
        "            self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        else:\n",
        "            self.dropout1 = None\n",
        "            self.dropout2 = None\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.fc1(x)\n",
        "        out = self.act(out)\n",
        "        if self.dropout1:\n",
        "            out = self.dropout1(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "        out = self.dropout2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LinearGeneral(nn.Module):\n",
        "    def __init__(self, in_dim=(768,), feat_dim=(12, 64)):\n",
        "        super(LinearGeneral, self).__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.randn(*in_dim, *feat_dim))\n",
        "        self.bias = nn.Parameter(torch.zeros(*feat_dim))\n",
        "\n",
        "    def forward(self, x, dims):\n",
        "        a = torch.tensordot(x, self.weight, dims=dims) + self.bias\n",
        "        return a\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_dim, heads=8, dropout_rate=0.1):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.head_dim = in_dim // heads\n",
        "        self.scale = self.head_dim ** 0.5\n",
        "\n",
        "        self.query = LinearGeneral((in_dim,), (self.heads, self.head_dim))\n",
        "        self.key = LinearGeneral((in_dim,), (self.heads, self.head_dim))\n",
        "        self.value = LinearGeneral((in_dim,), (self.heads, self.head_dim))\n",
        "        self.out = LinearGeneral((self.heads, self.head_dim), (in_dim,))\n",
        "\n",
        "        if dropout_rate > 0:\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        q = self.query(x, dims=([2], [0]))\n",
        "        k = self.key(x, dims=([2], [0]))\n",
        "        v = self.value(x, dims=([2], [0]))\n",
        "\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        out = torch.matmul(attn_weights, v)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "\n",
        "        out = self.out(out, dims=([2, 3], [0, 1]))\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_dim, mlp_dim, num_heads, dropout_rate=0.1, attn_dropout_rate=0.1):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(in_dim)\n",
        "        self.attn = SelfAttention(in_dim, heads=num_heads, dropout_rate=attn_dropout_rate)\n",
        "        if dropout_rate > 0:\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "        self.norm2 = nn.LayerNorm(in_dim)\n",
        "        self.mlp = MlpBlock(in_dim, mlp_dim, in_dim, dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.norm1(x)\n",
        "        out = self.attn(out)\n",
        "        if self.dropout:\n",
        "            out = self.dropout(out)\n",
        "        out += residual\n",
        "        residual = out\n",
        "\n",
        "        out = self.norm2(out)\n",
        "        out = self.mlp(out)\n",
        "        out += residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_patches, emb_dim, mlp_dim, num_layers=12, num_heads=12, dropout_rate=0.1, attn_dropout_rate=0.0):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # positional embedding\n",
        "        self.pos_embedding = PositionEmbs(num_patches, emb_dim, dropout_rate)\n",
        "\n",
        "        # encoder blocks\n",
        "        in_dim = emb_dim\n",
        "        self.encoder_layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            layer = EncoderBlock(in_dim, mlp_dim, num_heads, dropout_rate, attn_dropout_rate)\n",
        "            self.encoder_layers.append(layer)\n",
        "        self.norm = nn.LayerNorm(in_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.pos_embedding(x)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            out = layer(out)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self,\n",
        "                 image_size=(256, 256),\n",
        "                 patch_size=(16, 16),\n",
        "                 emb_dim=768,\n",
        "                 mlp_dim=3072,\n",
        "                 num_heads=12,\n",
        "                 num_layers=12,\n",
        "                 num_classes=1000,\n",
        "                 attn_dropout_rate=0.0,\n",
        "                 dropout_rate=0.1,\n",
        "                 feat_dim=None):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        h, w = image_size\n",
        "\n",
        "        # embedding layer\n",
        "        fh, fw = patch_size\n",
        "        gh, gw = h // fh, w // fw\n",
        "        num_patches = gh * gw\n",
        "        self.embedding = nn.Conv2d(3, emb_dim, kernel_size=(fh, fw), stride=(fh, fw))\n",
        "        # class token\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
        "\n",
        "        # transformer\n",
        "        self.transformer = Encoder(\n",
        "            num_patches=num_patches,\n",
        "            emb_dim=emb_dim,\n",
        "            mlp_dim=mlp_dim,\n",
        "            num_layers=num_layers,\n",
        "            num_heads=num_heads,\n",
        "            dropout_rate=dropout_rate,\n",
        "            attn_dropout_rate=attn_dropout_rate)\n",
        "\n",
        "        # classfier\n",
        "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)     # (n, c, gh, gw)\n",
        "        emb = emb.permute(0, 2, 3, 1)  # (n, gh, hw, c)\n",
        "        b, h, w, c = emb.shape\n",
        "        emb = emb.reshape(b, h * w, c)\n",
        "\n",
        "        # prepend class token\n",
        "        cls_token = self.cls_token.repeat(b, 1, 1)\n",
        "        emb = torch.cat([cls_token, emb], dim=1)\n",
        "\n",
        "        # transformer\n",
        "        feat = self.transformer(emb)\n",
        "\n",
        "        # classifier\n",
        "        logits = self.classifier(feat[:, 0])\n",
        "        return logits"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8ekG2_PgQG3"
      },
      "source": [
        "class CIFAR100DataLoader(DataLoader):\n",
        "    def __init__(self, data_dir, split='train', image_size=224, batch_size=16, num_workers=8):\n",
        "        if split == 'train':\n",
        "            train = True\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(image_size),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "            ])\n",
        "        else:\n",
        "            train = False\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(image_size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "            ])\n",
        "\n",
        "        self.dataset = CIFAR100(root=data_dir, train=train, transform=transform, download=True)\n",
        "\n",
        "        super(CIFAR100DataLoader, self).__init__(\n",
        "            dataset=self.dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False if not train else True,\n",
        "            num_workers=num_workers)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-po-eIxEaOK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4cc095-80c4-4948-cc4a-9084a75a712a"
      },
      "source": [
        "def train_epoch(epoch, model, data_loader, criterion, optimizer, lr_scheduler, metrics, device=torch.device('cpu')):\n",
        "    metrics.reset()\n",
        "\n",
        "    # training loop\n",
        "    for batch_idx, (batch_data, batch_target) in enumerate(data_loader):\n",
        "        batch_data = batch_data.to(device)\n",
        "        batch_target = batch_target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_pred = model(batch_data)\n",
        "        loss = criterion(batch_pred, batch_target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        acc1, acc5 = accuracy(batch_pred, batch_target, topk=(1, 5))\n",
        "\n",
        "        metrics.writer.set_step((epoch - 1) * len(data_loader) + batch_idx)\n",
        "        metrics.update('loss', loss.item())\n",
        "        metrics.update('acc1', acc1.item())\n",
        "        metrics.update('acc5', acc5.item())\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\"Train Epoch: {:03d} Batch: {:05d}/{:05d} Loss: {:.4f} Acc@1: {:.2f}, Acc@5: {:.2f}\"\n",
        "                    .format(epoch, batch_idx, len(data_loader), loss.item(), acc1.item(), acc5.item()))\n",
        "    return metrics.result()\n",
        "\n",
        "\n",
        "def valid_epoch(epoch, model, data_loader, criterion, metrics, device=torch.device('cpu')):\n",
        "    metrics.reset()\n",
        "    losses = []\n",
        "    acc1s = []\n",
        "    acc5s = []\n",
        "    # validation loop\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (batch_data, batch_target) in enumerate(data_loader):\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_target = batch_target.to(device)\n",
        "\n",
        "            batch_pred = model(batch_data)\n",
        "            loss = criterion(batch_pred, batch_target)\n",
        "            acc1, acc5 = accuracy(batch_pred, batch_target, topk=(1, 5))\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            acc1s.append(acc1.item())\n",
        "            acc5s.append(acc5.item())\n",
        "\n",
        "    loss = np.mean(losses)\n",
        "    acc1 = np.mean(acc1s)\n",
        "    acc5 = np.mean(acc5s)\n",
        "    metrics.writer.set_step(epoch, 'valid')\n",
        "    metrics.update('loss', loss)\n",
        "    metrics.update('acc1', acc1)\n",
        "    metrics.update('acc5', acc5)\n",
        "    return metrics.result()\n",
        "\n",
        "\n",
        "def save_model(save_dir, epoch, model, optimizer, lr_scheduler, device_ids, best=False):\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict() if len(device_ids) <= 1 else model.module.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'lr_scheduler': lr_scheduler.state_dict(),\n",
        "    }\n",
        "    filename = str(save_dir + 'current.pth')\n",
        "    torch.save(state, filename)\n",
        "\n",
        "    if best:\n",
        "        filename = str(save_dir + 'best.pth')\n",
        "        torch.save(state, filename)\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # device\n",
        "    device, device_ids = setup_device(n_gpu)\n",
        "\n",
        "    # tensorboard\n",
        "    writer = TensorboardWriter(summary_dir, tensorboard)\n",
        "\n",
        "    # metric tracker\n",
        "    metric_names = ['loss', 'acc1', 'acc5']\n",
        "    train_metrics = MetricTracker(*[metric for metric in metric_names], writer=writer)\n",
        "    valid_metrics = MetricTracker(*[metric for metric in metric_names], writer=writer)\n",
        "\n",
        "    # create model\n",
        "    print(\"create model\")\n",
        "    model = VisionTransformer(\n",
        "             image_size=(image_size, image_size),\n",
        "             patch_size=(patch_size, patch_size),\n",
        "             emb_dim=emb_dim,\n",
        "             mlp_dim=mlp_dim,\n",
        "             num_heads=num_heads,\n",
        "             num_layers=num_layers,\n",
        "             num_classes=num_classes,\n",
        "             attn_dropout_rate=attn_dropout_rate,\n",
        "             dropout_rate=dropout_rate)\n",
        "\n",
        "    # load checkpoint\n",
        "    if checkpoint_path:\n",
        "        state_dict = load_checkpoint(checkpoint_path)\n",
        "        if num_classes != state_dict['classifier.weight'].size(0):\n",
        "            del state_dict['classifier.weight']\n",
        "            del state_dict['classifier.bias']\n",
        "            print(\"re-initialize fc layer\")\n",
        "            model.load_state_dict(state_dict, strict=False)\n",
        "        else:\n",
        "            model.load_state_dict(state_dict)\n",
        "        print(\"Load pretrained weights from {}\".format(checkpoint_path))\n",
        "\n",
        "    # send model to device\n",
        "    model = model.to(device)\n",
        "    if len(device_ids) > 1:\n",
        "        model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
        "\n",
        "    # create dataloader\n",
        "    print(\"create dataloaders\")\n",
        "    train_dataloader = eval(\"{}DataLoader\".format(dataset))(\n",
        "                    data_dir=os.path.join(data_dir, dataset),\n",
        "                    image_size=image_size,\n",
        "                    batch_size=batch_size,\n",
        "                    num_workers=num_workers,\n",
        "                    split='train')\n",
        "    valid_dataloader = eval(\"{}DataLoader\".format(dataset))(\n",
        "                    data_dir=os.path.join(data_dir, dataset),\n",
        "                    image_size=image_size,\n",
        "                    batch_size=batch_size,\n",
        "                    num_workers=num_workers,\n",
        "                    split='val')\n",
        "\n",
        "    # training criterion\n",
        "    print(\"create criterion and optimizer\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create optimizers and learning rate scheduler\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params=model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=wd,\n",
        "        momentum=0.9)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer=optimizer,\n",
        "        max_lr=lr,\n",
        "        pct_start=warmup_steps / train_steps,\n",
        "        total_steps=train_steps)\n",
        "\n",
        "    # start training\n",
        "    print(\"start training\")\n",
        "    best_acc = 0.0\n",
        "    epochs = train_steps // len(train_dataloader)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        log = {'epoch': epoch}\n",
        "\n",
        "        # train the model\n",
        "        model.train()\n",
        "        result = train_epoch(epoch, model, train_dataloader, criterion, optimizer, lr_scheduler, train_metrics, device)\n",
        "        log.update(result)\n",
        "\n",
        "        # validate the model\n",
        "        model.eval()\n",
        "        result = valid_epoch(epoch, model, valid_dataloader, criterion, valid_metrics, device)\n",
        "        log.update(**{'val_' + k: v for k, v in result.items()})\n",
        "\n",
        "        # best acc\n",
        "        best = False\n",
        "        if log['val_acc1'] > best_acc:\n",
        "            best_acc = log['val_acc1']\n",
        "            best = True\n",
        "\n",
        "        # save model\n",
        "        save_model(checkpoint_dir, epoch, model, optimizer, lr_scheduler, device_ids, best)\n",
        "\n",
        "        # print logged informations to the screen\n",
        "        for key, value in log.items():\n",
        "            print('    {:15s}: {}'.format(str(key), value))\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: There's no GPU available on this machine, training will be performed on CPU.\n",
            "create model\n",
            "Load pretrained weights from imagenet21k+imagenet2012_ViT-B_16.pth\n",
            "create dataloaders\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "create criterion and optimizer\n",
            "start training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O_HnwCfaHdF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}